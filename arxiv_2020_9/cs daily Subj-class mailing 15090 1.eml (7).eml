Delivered-To: brucelu2013@gmail.com
Received: by 2002:ab4:a06d:0:0:0:0:0 with SMTP id cz13csp201444ecb;
        Thu, 27 Aug 2020 01:11:16 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJy590p5M73sa/fE6nFZNjhg6qEcfd/dIMu9n6jiqwi8VjTSoAd6G3neg7+7RSWqG0iYLqDU
X-Received: by 2002:ac8:1111:: with SMTP id c17mr6083669qtj.2.1598515876293;
        Thu, 27 Aug 2020 01:11:16 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1598515876; cv=none;
        d=google.com; s=arc-20160816;
        b=e3ysnvjpGYeyXiydprQ8+UdQ58Cx1pKvDT7syX7Bp0iuZeUHqVQ3T1XNUgJAlxt/Vv
         D8wcl8p/IXfB1Wxd0DqFCnEyj9/gtMM45LVEMZpTLeC6i7NH5rlzaKUWJrYHnolFsySK
         68H/vJdTBcemXWj5E1VxAKnWcbroWozKd8nzD6raXvsdV7XkibSC9WDiQePlKDuIrYva
         hDHkZYaZ/4bUR4qdvoMQ2rx7M9bwLffW3xZCpoW/Rxf0wNvWK2vSPWO+onKNnyRvRhH2
         6kI/OkEe5uuiwrW8T1H98BLagJh8rzu11/po9UEEX5EioXeyLdJ5j1QMSOdcIXbVA4o2
         nBfA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=xaW9URGzWOq07lRCIMlxEHPqj3bnvHE+6EN4AGCAXoE=;
        b=nUP6UQ+2KXFssAkSw4nv+gJRviQBIfFukumtFMbAioxPDdpLn6xddsNXzVSiOp4jOI
         9A8cCwVdQFiww8xwID/RzlZwALulfJBlBDRRigXaKnHQIcP1nscX83B4Vt8YlegvJW6S
         8cLo+wtHR1VjvQpOU3A6dV0f0bAkCEdJC2tYnsinX6B6fYYy74xa0+8vWyXhn8uNpoYG
         113rmpuVCxLXHd+58BLVSOOJe1fwzv0Y+SDqKMnXGSx6JvL/vhsMcz4yxiixHUJldCjg
         /HbCZkreS6BLSMHFrLfUwJJbXUuRyuVrzhlBx7OqvErROk89DViRLu0FUIbXQwkUYUx4
         PhQg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id 194si876016qkj.154.2020.08.27.01.11.15
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 27 Aug 2020 01:11:16 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07R8BFfL051616;
	Thu, 27 Aug 2020 04:11:15 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07R8BFdU062423;
	Thu, 27 Aug 2020 04:11:15 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 07R8BFYC062422;
	Thu, 27 Aug 2020 04:11:15 -0400
Date: Thu, 27 Aug 2020 04:11:15 -0400
Message-Id: <202008270811.07R8BFYC062422@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 15090 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Emerging Technologies
Graphics
 received from  Tue 25 Aug 20 18:00:00 GMT  to  Wed 26 Aug 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2008.11292
Date: Tue, 25 Aug 2020 22:16:18 GMT   (4279kb,D)

Title: Flip Paths Between Lattice Triangulations
Authors: William Sims and Meera Sitharam
Categories: cs.CG cs.DM
Comments: 30 pages, 12 figures
\\
  The problem of finding a diagonal flip path between two triangulations has
been studied for nearly a century in the combinatorial (topological) setting
and for decades in the geometric setting. In the geometric setting, finding a
diagonal flip path between two triangulations that minimizes the number of
diagonal flips over all such paths is NP-complete. However, when restricted to
lattice triangulations - i.e. triangulations of a finite subset of the integer
lattice, or an affine transformation of this lattice, bounded by a simple,
closed polygon with lattice points as vertices - the problem has a polynomial
time algorithm. Lattice triangulations have been studied for their uses in
discriminant theory, Hilbert's 16th problem, toric varieties, quantum spin
systems, and material science. Our first main result shows that there is a
polynomial-time computable, unique partially-ordered set of diagonal flips such
that there is a bijection between valid linear-orderings of this set and
minimum diagonal flip paths between two lattice triangulations. This provides
an alternative proof of the previously known result, as well as new structural
insights into these diagonal flip paths. Our second main result characterizes
pairs of triangulations, containing sets of edges $E$ and $E'$ respectively,
such that the minimum diagonal flip path between them contains the minimum
number of diagonal flips over all minimum diagonal flip paths between pairs of
triangulations, containing sets of edges $E$ and $E'$ respectively. Remarkably,
all of our results are derived from a simple relationship between edges in
triangulations and Farey sequences.
\\ ( https://arxiv.org/abs/2008.11292 ,  4279kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11230
Date: Tue, 25 Aug 2020 18:35:28 GMT   (40590kb,D)

Title: A Hidden Markov Tree Model for Flood Extent Mapping in Heavily Vegetated
  Areas based on High Resolution Aerial Imagery and DEM: A Case Study on
  Hurricane Matthew Floods
Authors: Zhe Jiang, Arpan Man Sainju
Categories: cs.CV cs.LG
\\
  Flood extent mapping plays a crucial role in disaster management and national
water forecasting. In recent years, high-resolution optical imagery becomes
increasingly available with the deployment of numerous small satellites and
drones. However, analyzing such imagery data to extract flood extent poses
unique challenges due to the rich noise and shadows, obstacles (e.g., tree
canopies, clouds), and spectral confusion between pixel classes (flood, dry)
due to spatial heterogeneity. Existing machine learning techniques often focus
on spectral and spatial features from raster images without fully incorporating
the geographic terrain within classification models. In contrast, we recently
proposed a novel machine learning model called geographical hidden Markov tree
that integrates spectral features of pixels and topographic constraints from
Digital Elevation Model (DEM) data (i.e., water flow directions) in a holistic
manner. This paper evaluates the model through case studies on high-resolution
aerial imagery from the National Oceanic and Atmospheric Administration (NOAA)
National Geodetic Survey together with DEM. Three scenes are selected in
heavily vegetated floodplains near the cities of Grimesland and Kinston in
North Carolina during Hurricane Matthew floods in 2016. Results show that the
proposed hidden Markov tree model outperforms several state of the art machine
learning algorithms (e.g., random forests, gradient boosted model) by an
improvement of F-score (the harmonic mean of the user's accuracy and producer's
accuracy) from around 70% to 80% to over 95% on our datasets.
\\ ( https://arxiv.org/abs/2008.11230 ,  40590kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11239
Date: Tue, 25 Aug 2020 19:05:38 GMT   (6152kb,D)

Title: HoloLens 2 Research Mode as a Tool for Computer Vision Research
Authors: Dorin Ungureanu, Federica Bogo, Silvano Galliani, Pooja Sama, Xin
  Duan, Casey Meekhof, Jan St\"uhmer, Thomas J. Cashman, Bugra Tekin, Johannes
  L. Sch\"onberger, Pawel Olszta, Marc Pollefeys
Categories: cs.CV
\\
  Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful
sensing devices with integrated compute capabilities, which makes it an ideal
platform for computer vision research. In this technical report, we present
HoloLens 2 Research Mode, an API and a set of tools enabling access to the raw
sensor streams. We provide an overview of the API and explain how it can be
used to build mixed reality applications based on processing sensor data. We
also show how to combine the Research Mode sensor data with the built-in eye
and hand tracking capabilities provided by HoloLens 2. By releasing the
Research Mode API and a set of open-source tools, we aim to foster further
research in the fields of computer vision as well as robotics and encourage
contributions from the research community.
\\ ( https://arxiv.org/abs/2008.11239 ,  6152kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11254
Date: Tue, 25 Aug 2020 20:12:59 GMT   (11282kb)

Title: Temporal Action Localization with Variance-Aware Networks
Authors: Ting-Ting Xie, Christos Tzelepis, Ioannis Patras
Categories: cs.CV
Comments: Journal paper; Under review
\\
  This work addresses the problem of temporal action localization with
Variance-Aware Networks (VAN), i.e., DNNs that use second-order statistics in
the input and/or the output of regression tasks. We first propose a network
(VANp) that when presented with the second-order statistics of the input, i.e.,
each sample has a mean and a variance, it propagates the mean and the variance
throughout the network to deliver outputs with second order statistics. In this
framework, both the input and the output could be interpreted as Gaussians. To
do so, we derive differentiable analytic solutions, or reasonable
approximations, to propagate across commonly used NN layers. To train the
network, we define a differentiable loss based on the KL-divergence between the
predicted Gaussian and a Gaussian around the ground truth action borders, and
use standard back-propagation. Importantly, the variances propagation in VANp
does not require any additional parameters, and during testing, does not
require any additional computations either. In action localization, the means
and the variances of the input are computed at pooling operations, that are
typically used to bring arbitrarily long videos to a vector with fixed
dimensions. Second, we propose two alternative formulations that augment the
first (respectively, the last) layer of a regression network with additional
parameters so as to take in the input (respectively, predict in the output)
both means and variances. Results in the action localization problem show that
the incorporation of second order statistics improves over the baseline
network, and that VANp surpasses the accuracy of virtually all other two-stage
networks without involving any additional parameters.
\\ ( https://arxiv.org/abs/2008.11254 ,  11282kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11269
Date: Tue, 25 Aug 2020 20:54:27 GMT   (8702kb,D)

Title: Deep Neural Network for 3D Surface Segmentation based on Contour Tree
  Hierarchy
Authors: Wenchong He, Arpan Man Sainju, Zhe Jiang and Da Yan
Categories: cs.CV cs.LG
\\
  Given a 3D surface defined by an elevation function on a 2D grid as well as
non-spatial features observed at each pixel, the problem of surface
segmentation aims to classify pixels into contiguous classes based on both
non-spatial features and surface topology. The problem has important
applications in hydrology, planetary science, and biochemistry but is uniquely
challenging for several reasons. First, the spatial extent of class segments
follows surface contours in the topological space, regardless of their spatial
shapes and directions. Second, the topological structure exists in multiple
spatial scales based on different surface resolutions. Existing widely
successful deep learning models for image segmentation are often not applicable
due to their reliance on convolution and pooling operations to learn regular
structural patterns on a grid. In contrast, we propose to represent surface
topological structure by a contour tree skeleton, which is a polytree capturing
the evolution of surface contours at different elevation levels. We further
design a graph neural network based on the contour tree hierarchy to model
surface topological structure at different spatial scales. Experimental
evaluations based on real-world hydrological datasets show that our model
outperforms several baseline methods in classification accuracy.
\\ ( https://arxiv.org/abs/2008.11269 ,  8702kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11289
Date: Tue, 25 Aug 2020 22:07:41 GMT   (1031kb,D)

Title: Multi-Face: Self-supervised Multiview Adaptation for Robust Face
  Clustering in Videos
Authors: Krishna Somandepalli, Rajat Hebbar, Shrikanth Narayanan
Categories: cs.CV eess.IV
\\
  Robust face clustering is a key step towards computational understanding of
visual character portrayals in media. Face clustering for long-form content
such as movies is challenging because of variations in appearance and lack of
large-scale labeled video resources. However, local face tracking in videos can
be used to mine samples belonging to same/different persons by examining the
faces co-occurring in a video frame. In this work, we use this idea of
self-supervision to harvest large amounts of weakly labeled face tracks in
movies. We propose a nearest-neighbor search in the embedding space to mine
hard examples from the face tracks followed by domain adaptation using
multiview shared subspace learning. Our benchmarking on movie datasets
demonstrate the robustness of multiview adaptation for face verification and
clustering. We hope that the large-scale data resources developed in this work
can further advance automatic character labeling in videos.
\\ ( https://arxiv.org/abs/2008.11289 ,  1031kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11297
Date: Tue, 25 Aug 2020 22:38:41 GMT   (394kb,D)

Title: Transductive Information Maximization For Few-Shot Learning
Authors: Malik Boudiaf, Ziko Imtiaz Masud, J\'er\^ome Rony, Jos\'e Dolz, Pablo
  Piantanida, Ismail Ben Ayed
Categories: cs.CV
Comments: 16 pages, 6 tables, 1 figure
\\
  We introduce Transductive Infomation Maximization (TIM) for few-shot
learning. Our method maximizes the mutual information between the query
features and predictions of a few-shot task, subject to supervision constraints
from the support set. Furthermore, we propose a new alternating direction
solver for our mutual-information loss, which substantially speeds up
transductive-inference convergence over gradient-based optimization, while
demonstrating similar accuracy performance. Following standard few-shot
settings, our comprehensive experiments demonstrate that TIM outperforms
state-of-the-art methods significantly across all datasets and networks, while
using simple cross-entropy training on the base classes, without resorting to
complex meta-learning schemes. It consistently brings between 2% to 5%
improvement in accuracy over the best performing methods not only on all the
well-established few-shot benchmarks, but also on more challenging scenarios,
with domain shifts and larger number of classes.
\\ ( https://arxiv.org/abs/2008.11297 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11300
Date: Tue, 25 Aug 2020 22:51:51 GMT   (3728kb,D)

Title: Likelihood Landscapes: A Unifying Principle Behind Many Adversarial
  Defenses
Authors: Fu Lin, Rohit Mittapalli, Prithvijit Chattopadhyay, Daniel Bolya, Judy
  Hoffman
Categories: cs.CV cs.LG
Comments: ECCV 2020 Workshop on Adversarial Robustness in the Real World
\\
  Convolutional Neural Networks have been shown to be vulnerable to adversarial
examples, which are known to locate in subspaces close to where normal data
lies but are not naturally occurring and of low probability. In this work, we
investigate the potential effect defense techniques have on the geometry of the
likelihood landscape - likelihood of the input images under the trained model.
We first propose a way to visualize the likelihood landscape leveraging an
energy-based model interpretation of discriminative classifiers. Then we
introduce a measure to quantify the flatness of the likelihood landscape. We
observe that a subset of adversarial defense techniques results in a similar
effect of flattening the likelihood landscape. We further explore directly
regularizing towards a flat landscape for adversarial robustness.
\\ ( https://arxiv.org/abs/2008.11300 ,  3728kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11331
Date: Wed, 26 Aug 2020 01:34:19 GMT   (1230kb,D)

Title: Synthetic Sample Selection via Reinforcement Learning
Authors: Jiarong Ye, Yuan Xue, L. Rodney Long, Sameer Antani, Zhiyun Xue, Keith
  Cheng, Xiaolei Huang
Categories: cs.CV cs.LG
Comments: MICCAI2020
\\
  Synthesizing realistic medical images provides a feasible solution to the
shortage of training data in deep learning based medical image recognition
systems. However, the quality control of synthetic images for data augmentation
purposes is under-investigated, and some of the generated images are not
realistic and may contain misleading features that distort data distribution
when mixed with real images. Thus, the effectiveness of those synthetic images
in medical image recognition systems cannot be guaranteed when they are being
added randomly without quality assurance. In this work, we propose a
reinforcement learning (RL) based synthetic sample selection method that learns
to choose synthetic images containing reliable and informative features. A
transformer based controller is trained via proximal policy optimization (PPO)
using the validation classification accuracy as the reward. The selected images
are mixed with the original training data for improved training of image
recognition systems. To validate our method, we take the pathology image
recognition as an example and conduct extensive experiments on two
histopathology image datasets. In experiments on a cervical dataset and a lymph
node dataset, the image classification performance is improved by 8.1% and
2.3%, respectively, when utilizing high-quality synthetic images selected by
our RL framework. Our proposed synthetic sample selection method is general and
has great potential to boost the performance of various medical image
recognition systems given limited annotation.
\\ ( https://arxiv.org/abs/2008.11331 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11351
Date: Wed, 26 Aug 2020 02:43:25 GMT   (18350kb,D)

Title: SNE-RoadSeg: Incorporating Surface Normal Information into Semantic
  Segmentation for Accurate Freespace Detection
Authors: Rui Fan, Hengli Wang, Peide Cai, Ming Liu
Categories: cs.CV cs.RO eess.IV
Comments: ECCV 2020
DOI: 10.1007/978-3-030-58577-8_21
\\
  Freespace detection is an essential component of visual perception for
self-driving cars. The recent efforts made in data-fusion convolutional neural
networks (CNNs) have significantly improved semantic driving scene
segmentation. Freespace can be hypothesized as a ground plane, on which the
points have similar surface normals. Hence, in this paper, we first introduce a
novel module, named surface normal estimator (SNE), which can infer surface
normal information from dense depth/disparity images with high accuracy and
efficiency. Furthermore, we propose a data-fusion CNN architecture, referred to
as RoadSeg, which can extract and fuse features from both RGB images and the
inferred surface normal information for accurate freespace detection. For
research purposes, we publish a large-scale synthetic freespace detection
dataset, named Ready-to-Drive (R2D) road dataset, collected under different
illumination and weather conditions. The experimental results demonstrate that
our proposed SNE module can benefit all the state-of-the-art CNNs for freespace
detection, and our SNE-RoadSeg achieves the best overall performance among
different datasets.
\\ ( https://arxiv.org/abs/2008.11351 ,  18350kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11353
Date: Wed, 26 Aug 2020 02:49:32 GMT   (290kb,D)

Title: Detection of Genuine and Posed Facial Expressions of Emotion: A Review
Authors: Shan Jia, Shuo Wang, Chuanbo Hu, Paula Webster, Xin Li
Categories: cs.CV
\\
  Facial expressions of emotion play an important role in human social
interactions. However, posed acting is not always the same as genuine feeling.
Therefore, the credibility assessment of facial expressions, namely, the
discrimination of genuine (spontaneous) expressions from
posed(deliberate/volitional/deceptive) ones, is a crucial yet challenging task
in facial expression understanding. Rapid progress has been made in recent
years for automatic detection of genuine and posed facial expressions. This
paper presents a general review of the relevant research, including several
spontaneous vs. posed (SVP) facial expression databases and various computer
vision based detection methods. In addition, a variety of factors that will
influence the performance of SVP detection methods are discussed along with
open issues and technical challenges.
\\ ( https://arxiv.org/abs/2008.11353 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11354
Date: Wed, 26 Aug 2020 02:52:48 GMT   (6628kb,D)

Title: Generating Handwriting via Decouple Style Descriptors
Authors: Atsunobu Kotani, Stefanie Tellex, James Tompkin
Categories: cs.CV
Comments: European Conference on Computer Vision (ECCV) 2020
\\
  Representing a space of handwriting stroke styles includes the challenge of
representing both the style of each character and the overall style of the
human writer. Existing VRNN approaches to representing handwriting often do not
distinguish between these different style components, which can reduce model
capability. Instead, we introduce the Decoupled Style Descriptor (DSD) model
for handwriting, which factors both character- and writer-level styles and
allows our model to represent an overall greater space of styles. This approach
also increases flexibility: given a few examples, we can generate handwriting
in new writer styles, and also now generate handwriting of new characters
across writer styles. In experiments, our generated results were preferred over
a state of the art baseline method 88% of the time, and in a writer
identification task on 20 held-out writers, our DSDs achieved 89.38% accuracy
from a single sample word. Overall, DSDs allows us to improve both the quality
and flexibility over existing handwriting stroke generation approaches.
\\ ( https://arxiv.org/abs/2008.11354 ,  6628kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11360
Date: Wed, 26 Aug 2020 03:18:53 GMT   (10982kb,D)

Title: Discriminative Cross-Domain Feature Learning for Partial Domain
  Adaptation
Authors: Taotao Jing, Ming Shao, Zhengming Ding
Categories: cs.CV
\\
  Partial domain adaptation aims to adapt knowledge from a larger and more
diverse source domain to a smaller target domain with less number of classes,
which has attracted appealing attention. Recent practice on domain adaptation
manages to extract effective features by incorporating the pseudo labels for
the target domain to better fight off the cross-domain distribution
divergences. However, it is essential to align target data with only a small
set of source data. In this paper, we develop a novel Discriminative
Cross-Domain Feature Learning (DCDF) framework to iteratively optimize target
labels with a cross-domain graph in a weighted scheme. Specifically, a weighted
cross-domain center loss and weighted cross-domain graph propagation are
proposed to couple unlabeled target data to related source samples for
discriminative cross-domain feature learning, where irrelevant source centers
will be ignored, to alleviate the marginal and conditional disparities
simultaneously. Experimental evaluations on several popular benchmarks
demonstrate the effectiveness of our proposed approach on facilitating the
recognition for the unlabeled target domain, through comparing it to the
state-of-the-art partial domain adaptation approaches.
\\ ( https://arxiv.org/abs/2008.11360 ,  10982kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11363
Date: Wed, 26 Aug 2020 03:35:47 GMT   (3039kb,D)

Title: How Do the Hearts of Deep Fakes Beat? Deep Fake Source Detection via
  Interpreting Residuals with Biological Signals
Authors: Umur Aybars Ciftci and Ilke Demir and Lijun Yin
Categories: cs.CV cs.LG
Comments: To be published in the proceedings of 2020 IEEE/IAPR International
  Joint Conference on Biometrics (IJCB)
\\
  Fake portrait video generation techniques have been posing a new threat to
the society with photorealistic deep fakes for political propaganda, celebrity
imitation, forged evidences, and other identity related manipulations.
Following these generation techniques, some detection approaches have also been
proved useful due to their high classification accuracy. Nevertheless, almost
no effort was spent to track down the source of deep fakes. We propose an
approach not only to separate deep fakes from real videos, but also to discover
the specific generative model behind a deep fake. Some pure deep learning based
approaches try to classify deep fakes using CNNs where they actually learn the
residuals of the generator. We believe that these residuals contain more
information and we can reveal these manipulation artifacts by disentangling
them with biological signals. Our key observation yields that the
spatiotemporal patterns in biological signals can be conceived as a
representative projection of residuals. To justify this observation, we extract
PPG cells from real and fake videos and feed these to a state-of-the-art
classification network for detecting the generative model per video. Our
results indicate that our approach can detect fake videos with 97.29% accuracy,
and the source model with 93.39% accuracy.
\\ ( https://arxiv.org/abs/2008.11363 ,  3039kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11368
Date: Wed, 26 Aug 2020 03:56:37 GMT   (9490kb,D)

Title: Keypoint-Aligned Embeddings for Image Retrieval and Re-identification
Authors: Olga Moskvyak, Frederic Maire, Feras Dayoub and Mahsa Baktashmotlagh
Categories: cs.CV
Comments: 8 pages, 7 figures, accepted to WACV 2021
\\
  Learning embeddings that are invariant to the pose of the object is crucial
in visual image retrieval and re-identification. The existing approaches for
person, vehicle, or animal re-identification tasks suffer from high intra-class
variance due to deformable shapes and different camera viewpoints. To overcome
this limitation, we propose to align the image embedding with a predefined
order of the keypoints. The proposed keypoint aligned embeddings model
(KAE-Net) learns part-level features via multi-task learning which is guided by
keypoint locations. More specifically, KAE-Net extracts channels from a feature
map activated by a specific keypoint through learning the auxiliary task of
heatmap reconstruction for this keypoint. The KAE-Net is compact, generic and
conceptually simple. It achieves state of the art performance on the benchmark
datasets of CUB-200-2011, Cars196 and VeRi-776 for retrieval and
re-identification tasks.
\\ ( https://arxiv.org/abs/2008.11368 ,  9490kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11378
Date: Wed, 26 Aug 2020 05:19:04 GMT   (1803kb,D)

Title: Effective Action Recognition with Embedded Key Point Shifts
Authors: Haozhi Cao, Yuecong Xu, Jianfei Yang, Kezhi Mao, Jianxiong Yin and
  Simon See
Categories: cs.CV
Comments: 35 pages, 10 figures
\\
  Temporal feature extraction is an essential technique in video-based action
recognition. Key points have been utilized in skeleton-based action recognition
methods but they require costly key point annotation. In this paper, we propose
a novel temporal feature extraction module, named Key Point Shifts Embedding
Module ($KPSEM$), to adaptively extract channel-wise key point shifts across
video frames without key point annotation for temporal feature extraction. Key
points are adaptively extracted as feature points with maximum feature values
at split regions, while key point shifts are the spatial displacements of
corresponding key points. The key point shifts are encoded as the overall
temporal features via linear embedding layers in a multi-set manner. Our method
achieves competitive performance through embedding key point shifts with
trivial computational cost, achieving the state-of-the-art performance of
82.05% on Mini-Kinetics and competitive performance on UCF101,
Something-Something-v1, and HMDB51 datasets.
\\ ( https://arxiv.org/abs/2008.11378 ,  1803kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11383
Date: Wed, 26 Aug 2020 05:44:07 GMT   (993kb,D)

Title: Applying Surface Normal Information in Drivable Area and Road Anomaly
  Detection for Ground Mobile Robots
Authors: Hengli Wang, Rui Fan, Yuxiang Sun, Ming Liu
Categories: cs.CV cs.RO
Comments: 6 pages, 6 figures and 1 table. This paper is accepted by IROS 2020
\\
  The joint detection of drivable areas and road anomalies is a crucial task
for ground mobile robots. In recent years, many impressive semantic
segmentation networks, which can be used for pixel-level drivable area and road
anomaly detection, have been developed. However, the detection accuracy still
needs improvement. Therefore, we develop a novel module named the Normal
Inference Module (NIM), which can generate surface normal information from
dense depth images with high accuracy and efficiency. Our NIM can be deployed
in existing convolutional neural networks (CNNs) to refine the segmentation
performance. To evaluate the effectiveness and robustness of our NIM, we embed
it in twelve state-of-the-art CNNs. The experimental results illustrate that
our NIM can greatly improve the performance of the CNNs for drivable area and
road anomaly detection. Furthermore, our proposed NIM-RTFNet ranks 8th on the
KITTI road benchmark and exhibits a real-time inference speed.
\\ ( https://arxiv.org/abs/2008.11383 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11401
Date: Wed, 26 Aug 2020 06:39:24 GMT   (17618kb,D)

Title: Point Adversarial Self Mining: A Simple Method for Facial Expression
  Recognition in the Wild
Authors: Ping Liu, Yuewei Lin, Zibo Meng, Weihong Deng, Joey Tianyi Zhou, and
  Yi Yang
Categories: cs.CV
\\
  In this paper, the Point Adversarial Self Mining (PASM) approach, a simple
yet effective way to progressively mine knowledge from training samples, is
proposed to produce training data for CNNs to improve the performance and
network generality in Facial Expression Recognition (FER) task. In order to
achieve a high prediction accuracy under real-world scenarios, most of the
existing works choose to manipulate the network architectures and design
sophisticated loss terms. Although demonstrated to be effective in real
scenarios, those aforementioned methods require extra efforts in network
design. Inspired by random erasing and adversarial erasing, we propose PASM for
data augmentation, simulating the data distribution in the wild. Specifically,
given a sample and a pre-trained network, our proposed approach locates the
informative region in the sample generated by point adversarial attack policy.
The informative region is highly structured and sparse. Comparing to the
regions produced by random erasing which selects the region in a purely random
way and adversarial erasing which operates by attention maps, the located
informative regions obtained by PASM are more adaptive and better aligned with
the previous findings: not all but only a few facial regions contribute to the
accurate prediction. Then, the located informative regions are masked out from
the original samples to generate augmented images, which would force the
network to explore additional information from other less informative regions.
The augmented images are used to finetune the network to enhance its
generality. In the refinement process, we take advantage of knowledge
distillation, utilizing the pre-trained network to provide guidance and retain
knowledge from old samples to train a new network with the same structural
configuration.
\\ ( https://arxiv.org/abs/2008.11401 ,  17618kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11423
Date: Wed, 26 Aug 2020 07:33:09 GMT   (27725kb,D)

Title: Orientation-aware Vehicle Re-identification with Semantics-guided Part
  Attention Network
Authors: Tsai-Shien Chen, Chih-Ting Liu, Chih-Wei Wu, Shao-Yi Chien
Categories: cs.CV
Comments: ECCV 2020 (Oral). Paper Website:
  http://media.ee.ntu.edu.tw/research/SPAN/
\\
  Vehicle re-identification (re-ID) focuses on matching images of the same
vehicle across different cameras. It is fundamentally challenging because
differences between vehicles are sometimes subtle. While several studies
incorporate spatial-attention mechanisms to help vehicle re-ID, they often
require expensive keypoint labels or suffer from noisy attention mask if not
trained with expensive labels. In this work, we propose a dedicated
Semantics-guided Part Attention Network (SPAN) to robustly predict part
attention masks for different views of vehicles given only image-level semantic
labels during training. With the help of part attention masks, we can extract
discriminative features in each part separately. Then we introduce
Co-occurrence Part-attentive Distance Metric (CPDM) which places greater
emphasis on co-occurrence vehicle parts when evaluating the feature distance of
two images. Extensive experiments validate the effectiveness of the proposed
method and show that our framework outperforms the state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2008.11423 ,  27725kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11440
Date: Wed, 26 Aug 2020 08:25:34 GMT   (2996kb,D)

Title: Fusion of Global-Local Features for Image Quality Inspection of Shipping
  Label
Authors: Sungho Suh, Paul Lukowicz and Yong Oh Lee
Categories: cs.CV cs.LG eess.IV
Comments: Accepted at ICPR 2020
\\
  The demands of automated shipping address recognition and verification have
increased to handle a large number of packages and to save costs associated
with misdelivery. A previous study proposed a deep learning system where the
shipping address is recognized and verified based on a camera image capturing
the shipping address and barcode area. Because the system performance depends
on the input image quality, inspection of input image quality is necessary for
image preprocessing. In this paper, we propose an input image quality
verification method combining global and local features. Object detection and
scale-invariant feature transform in different feature spaces are developed to
extract global and local features from several independent convolutional neural
networks. The conditions of shipping label images are classified by fully
connected fusion layers with concatenated global and local features. The
experimental results regarding real captured and generated images show that the
proposed method achieves better performance than other methods. These results
are expected to improve the shipping address recognition and verification
system by applying different image preprocessing steps based on the classified
conditions.
\\ ( https://arxiv.org/abs/2008.11440 ,  2996kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11451
Date: Wed, 26 Aug 2020 09:06:11 GMT   (2303kb,D)

Title: Determinantal Point Process as an alternative to NMS
Authors: Samik Some, Mithun Das Gupta, Vinay P. Namboodiri
Categories: cs.CV
\\
  We present a determinantal point process (DPP) inspired alternative to
non-maximum suppression (NMS) which has become an integral step in all
state-of-the-art object detection frameworks. DPPs have been shown to encourage
diversity in subset selection problems. We pose NMS as a subset selection
problem and posit that directly incorporating DPP like framework can improve
the overall performance of the object detection system. We propose an
optimization problem which takes the same inputs as NMS, but introduces a novel
sub-modularity based diverse subset selection functional. Our results strongly
indicate that the modifications proposed in this paper can provide consistent
improvements to state-of-the-art object detection pipelines.
\\ ( https://arxiv.org/abs/2008.11451 ,  2303kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11459
Date: Wed, 26 Aug 2020 09:27:26 GMT   (14413kb,D)

Title: Semantic Graph Based Place Recognition for 3D Point Clouds
Authors: Xin Kong, Xuemeng Yang, Guangyao Zhai, Xiangrui Zhao, Xianfang Zeng,
  Mengmeng Wang, Yong Liu, Wanlong Li, Feng Wen
Categories: cs.CV cs.RO
Comments: 8 pages. Accpeted by IROS-2020
\\
  Due to the difficulty in generating the effective descriptors which are
robust to occlusion and viewpoint changes, place recognition for 3D point cloud
remains an open issue. Unlike most of the existing methods that focus on
extracting local, global, and statistical features of raw point clouds, our
method aims at the semantic level that can be superior in terms of robustness
to environmental changes. Inspired by the perspective of humans, who recognize
scenes through identifying semantic objects and capturing their relations, this
paper presents a novel semantic graph based approach for place recognition.
First, we propose a novel semantic graph representation for the point cloud
scenes by reserving the semantic and topological information of the raw point
cloud. Thus, place recognition is modeled as a graph matching problem. Then we
design a fast and effective graph similarity network to compute the similarity.
Exhaustive evaluations on the KITTI dataset show that our approach is robust to
the occlusion as well as viewpoint changes and outperforms the state-of-the-art
methods with a large margin. Our code is available at:
\url{https://github.com/kxhit/SG_PR}.
\\ ( https://arxiv.org/abs/2008.11459 ,  14413kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11469
Date: Wed, 26 Aug 2020 09:56:07 GMT   (3273kb,D)

Title: SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation
Authors: Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao,
  Xiaowei Zhou
Categories: cs.CV
Comments: ECCV 2020, project page: https://zju3dv.github.io/SMAP/
\\
  Recovering multi-person 3D poses with absolute scales from a single RGB image
is a challenging problem due to the inherent depth and scale ambiguity from a
single view. Addressing this ambiguity requires to aggregate various cues over
the entire image, such as body sizes, scene layouts, and inter-person
relationships. However, most previous methods adopt a top-down scheme that
first performs 2D pose detection and then regresses the 3D pose and scale for
each detected person individually, ignoring global contextual cues. In this
paper, we propose a novel system that first regresses a set of 2.5D
representations of body parts and then reconstructs the 3D absolute poses based
on these 2.5D representations with a depth-aware part association algorithm.
Such a single-shot bottom-up scheme allows the system to better learn and
reason about the inter-person depth relationship, improving both 3D and 2D pose
estimation. The experiments demonstrate that the proposed approach achieves the
state-of-the-art performance on the CMU Panoptic and MuPoTS-3D datasets and is
applicable to in-the-wild videos.
\\ ( https://arxiv.org/abs/2008.11469 ,  3273kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11476
Date: Wed, 26 Aug 2020 10:30:18 GMT   (2196kb,D)

Title: HipaccVX: Wedding of OpenVX and DSL-based Code Generation
Authors: M. Akif \"Ozkan, Burak Ok, Bo Qiao, J\"urgen Teich, Frank Hannig
Categories: cs.CV cs.DC cs.PL
\\
  Writing programs for heterogeneous platforms optimized for high performance
is hard since this requires the code to be tuned at a low level with
architecture-specific optimizations that are most times based on fundamentally
differing programming paradigms and languages. OpenVX promises to solve this
issue for computer vision applications with a royalty-free industry standard
that is based on a graph-execution model. Yet, the OpenVX' algorithm space is
constrained to a small set of vision functions. This hinders accelerating
computations that are not included in the standard.
  In this paper, we analyze OpenVX vision functions to find an orthogonal set
of computational abstractions. Based on these abstractions, we couple an
existing Domain-Specific Language (DSL) back end to the OpenVX environment and
provide language constructs to the programmer for the definition of
user-defined nodes. In this way, we enable optimizations that are not possible
to detect with OpenVX graph implementations using the standard computer vision
functions. These optimizations can double the throughput on an Nvidia GTX GPU
and decrease the resource usage of a Xilinx Zynq FPGA by 50% for our
benchmarks. Finally, we show that our proposed compiler framework, called
HipaccVX, can achieve better results than the state-of-the-art approaches
Nvidia VisionWorks and Halide-HLS.
\\ ( https://arxiv.org/abs/2008.11476 ,  2196kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11479
Date: Wed, 26 Aug 2020 10:34:46 GMT   (5660kb,D)

Title: Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image
  Translation
Authors: Koya Tango, Marie Katsurai, Hayato Maki, Ryosuke Goto
Categories: cs.CV eess.IV
Comments: 19 pages
\\
  Cosplay has grown from its origins at fan conventions into a billion-dollar
global dress phenomenon. To facilitate imagination and reinterpretation from
animated images to real garments, this paper presents an automatic costume
image generation method based on image-to-image translation. Cosplay items can
be significantly diverse in their styles and shapes, and conventional methods
cannot be directly applied to the wide variation in clothing images that are
the focus of this study. To solve this problem, our method starts by collecting
and preprocessing web images to prepare a cleaned, paired dataset of the anime
and real domains. Then, we present a novel architecture for generative
adversarial networks (GANs) to facilitate high-quality cosplay image
generation. Our GAN consists of several effective techniques to fill the gap
between the two domains and improve both the global and local consistency of
generated images. Experiments demonstrated that, with two types of evaluation
metrics, the proposed GAN achieves better performance than existing methods. We
also showed that the images generated by the proposed method are more realistic
than those generated by the conventional methods. Our codes and pretrained
model are available on the web.
\\ ( https://arxiv.org/abs/2008.11479 ,  5660kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11493
Date: Wed, 26 Aug 2020 11:15:49 GMT   (1657kb)

Title: Vehicle Trajectory Prediction in Crowded Highway Scenarios Using Bird
  Eye View Representations and CNNs
Authors: R. Izquierdo, A. Quintanar, I. Parra, D. Fernandez-Llorca, and M. A.
  Sotelo
Categories: cs.CV
Comments: This work has been accepted for publication at IEEE Intelligent
  Transportation Systems Conference 2020
\\
  This paper describes a novel approach to perform vehicle trajectory
predictions employing graphic representations. The vehicles are represented
using Gaussian distributions into a Bird Eye View. Then the U-net model is used
to perform sequence to sequence predictions. This deep learning-based
methodology has been trained using the HighD dataset, which contains vehicles'
detection in a highway scenario from aerial imagery. The problem is faced as an
image to image regression problem training the network to learn the underlying
relations between the traffic participants. This approach generates an
estimation of the future appearance of the input scene, not trajectories or
numeric positions. An extra step is conducted to extract the positions from the
predicted representation with subpixel resolution. Different network
configurations have been tested, and prediction error up to three seconds ahead
is in the order of the representation resolution. The model has been tested in
highway scenarios with more than 30 vehicles simultaneously in two opposite
traffic flow streams showing good qualitative and quantitative results.
\\ ( https://arxiv.org/abs/2008.11493 ,  1657kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11497
Date: Wed, 26 Aug 2020 11:28:50 GMT   (517kb)

Title: Gesture Recognition from Skeleton Data for Intuitive Human-Machine
  Interaction
Authors: Andr\'e Br\'as, Miguel Sim\~ao, Pedro Neto
Categories: cs.CV cs.RO
Comments: Transdisciplinary Engineering Methods for Social Innovation of
  Industry 4.0
DOI: 10.3233/978-1-61499-898-3-271
\\
  Human gesture recognition has assumed a capital role in industrial
applications, such as Human-Machine Interaction. We propose an approach for
segmentation and classification of dynamic gestures based on a set of
handcrafted features, which are drawn from the skeleton data provided by the
Kinect sensor. The module for gesture detection relies on a feedforward neural
network which performs framewise binary classification. The method for gesture
recognition applies a sliding window, which extracts information from both the
spatial and temporal dimensions. Then we combine windows of varying durations
to get a multi-temporal scale approach and an additional gain in performance.
Encouraged by the recent success of Recurrent Neural Networks for time series
domains, we also propose a method for simultaneous gesture segmentation and
classification based on the bidirectional Long Short-Term Memory cells, which
have shown ability for learning the temporal relationships on long temporal
scales. We evaluate all the different approaches on the dataset published for
the ChaLearn Looking at People Challenge 2014. The most effective method
achieves a Jaccard index of 0.75, which suggests a performance almost on pair
with that presented by the state-of-the-art techniques. At the end, the
recognized gestures are used to interact with a collaborative robot.
\\ ( https://arxiv.org/abs/2008.11497 ,  517kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11505
Date: Wed, 26 Aug 2020 12:02:44 GMT   (3931kb)

Title: Cross-regional oil palm tree counting and detection via multi-level
  attention domain adaptation network
Authors: Juepeng Zheng, Haohuan Fu, Weijia Li, Wenzhao Wu, Yi Zhao, Runmin Dong
  and Le Yu
Categories: cs.CV
Comments: 39 pages, 13 figures, accepted by ISPRS PG&RS
Journal-ref: ISPRS Journal of Photogrammetry and Remote Sensing 2020
DOI: 10.1016/j.isprsjprs.2020.07.002
\\
  Providing an accurate evaluation of palm tree plantation in a large region
can bring meaningful impacts in both economic and ecological aspects. However,
the enormous spatial scale and the variety of geological features across
regions has made it a grand challenge with limited solutions based on manual
human monitoring efforts. Although deep learning based algorithms have
demonstrated potential in forming an automated approach in recent years, the
labelling efforts needed for covering different features in different regions
largely constrain its effectiveness in large-scale problems. In this paper, we
propose a novel domain adaptive oil palm tree detection method, i.e., a
Multi-level Attention Domain Adaptation Network (MADAN) to reap cross-regional
oil palm tree counting and detection. MADAN consists of 4 procedures: First, we
adopted a batch-instance normalization network (BIN) based feature extractor
for improving the generalization ability of the model, integrating batch
normalization and instance normalization. Second, we embedded a multi-level
attention mechanism (MLA) into our architecture for enhancing the
transferability, including a feature level attention and an entropy level
attention. Then we designed a minimum entropy regularization (MER) to increase
the confidence of the classifier predictions through assigning the entropy
level attention value to the entropy penalty. Finally, we employed a sliding
window-based prediction and an IOU based post-processing approach to attain the
final detection results. We conducted comprehensive ablation experiments using
three different satellite images of large-scale oil palm plantation area with
six transfer tasks. MADAN improves the detection accuracy by 14.98% in terms of
average F1-score compared with the Baseline method (without DA), and performs
3.55%-14.49% better than existing domain adaptation methods.
\\ ( https://arxiv.org/abs/2008.11505 ,  3931kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11516
Date: Wed, 26 Aug 2020 12:24:23 GMT   (11088kb,D)

Title: Making a Case for 3D Convolutions for Object Segmentation in Videos
Authors: Sabarinath Mahadevan, Ali Athar, Aljo\v{s}a O\v{s}ep, Sebastian
  Hennen, Laura Leal-Taix\'e, Bastian Leibe
Categories: cs.CV
Comments: BMVC '20
\\
  The task of object segmentation in videos is usually accomplished by
processing appearance and motion information separately using standard 2D
convolutional networks, followed by a learned fusion of the two sources of
information. On the other hand, 3D convolutional networks have been
successfully applied for video classification tasks, but have not been
leveraged as effectively to problems involving dense per-pixel interpretation
of videos compared to their 2D convolutional counterparts and lag behind the
aforementioned networks in terms of performance. In this work, we show that 3D
CNNs can be effectively applied to dense video prediction tasks such as salient
object segmentation. We propose a simple yet effective encoder-decoder network
architecture consisting entirely of 3D convolutions that can be trained
end-to-end using a standard cross-entropy loss. To this end, we leverage an
efficient 3D encoder, and propose a 3D decoder architecture, that comprises
novel 3D Global Convolution layers and 3D Refinement modules. Our approach
outperforms existing state-of-the-arts by a large margin on the DAVIS'16
Unsupervised, FBMS and ViSal dataset benchmarks in addition to being faster,
thus showing that our architecture can efficiently learn expressive
spatio-temporal features and produce high quality video segmentation masks. Our
code and models will be made publicly available.
\\ ( https://arxiv.org/abs/2008.11516 ,  11088kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11560
Date: Wed, 26 Aug 2020 13:41:20 GMT   (8336kb,D)

Title: Performance Optimization for Federated Person Re-identification via
  Benchmark Analysis
Authors: Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin,
  Dongzhan Zhou, Shuai Zhang, Shuai Yi
Categories: cs.CV cs.DC cs.LG
Comments: ACMMM'20
DOI: 10.1145/3394171.3413814
\\
  Federated learning is a privacy-preserving machine learning technique that
learns a shared model across decentralized clients. It can alleviate privacy
concerns of personal re-identification, an important computer vision task. In
this work, we implement federated learning to person re-identification
(FedReID) and optimize its performance affected by statistical heterogeneity in
the real-world scenario. We first construct a new benchmark to investigate the
performance of FedReID. This benchmark consists of (1) nine datasets with
different volumes sourced from different domains to simulate the heterogeneous
situation in reality, (2) two federated scenarios, and (3) an enhanced
federated algorithm for FedReID. The benchmark analysis shows that the
client-edge-cloud architecture, represented by the federated-by-dataset
scenario, has better performance than client-server architecture in FedReID. It
also reveals the bottlenecks of FedReID under the real-world scenario,
including poor performance of large datasets caused by unbalanced weights in
model aggregation and challenges in convergence. Then we propose two
optimization methods: (1) To address the unbalanced weight problem, we propose
a new method to dynamically change the weights according to the scale of model
changes in clients in each training round; (2) To facilitate convergence, we
adopt knowledge distillation to refine the server model with knowledge
generated from client models on a public dataset. Experiment results
demonstrate that our strategies can achieve much better convergence with
superior performance on all datasets. We believe that our work will inspire the
community to further explore the implementation of federated learning on more
computer vision tasks in real-world scenarios.
\\ ( https://arxiv.org/abs/2008.11560 ,  8336kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11586
Date: Tue, 25 Aug 2020 05:43:48 GMT   (3442kb,D)

Title: Weakly Supervised Learning with Side Information for Noisy Labeled
  Images
Authors: Lele Cheng, Xiangzeng Zhou, Liming Zhao, Dangwei Li, Hong Shang, Yun
  Zheng, Pan Pan, Yinghui Xu
Categories: cs.CV
Comments: Accepted by ECCV2020. arXiv admin note: text overlap with
  arXiv:1808.01097 by other authors
\\
  In many real-world datasets, like WebVision, the performance of DNN based
classifier is often limited by the noisy labeled data. To tackle this problem,
some image related side information, such as captions and tags, often reveal
underlying relationships across images. In this paper, we present an efficient
weakly supervised learning by using a Side Information Network (SINet), which
aims to effectively carry out a large scale classification with severely noisy
labels. The proposed SINet consists of a visual prototype module and a noise
weighting module. The visual prototype module is designed to generate a compact
representation for each category by introducing the side information. The noise
weighting module aims to estimate the correctness of each noisy image and
produce a confidence score for image ranking during the training procedure. The
propsed SINet can largely alleviate the negative impact of noisy image labels,
and is beneficial to train a high performance CNN based classifier. Besides, we
released a fine-grained product dataset called AliProducts, which contains more
than 2.5 million noisy web images crawled from the internet by using queries
generated from 50,000 fine-grained semantic classes. Extensive experiments on
several popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our
proposed AliProducts achieve state-of-the-art performance. The SINet has won
the first place in the classification task on WebVision Challenge 2019, and
outperformed other competitors by a large margin.
\\ ( https://arxiv.org/abs/2008.11586 ,  3442kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11588
Date: Wed, 26 Aug 2020 14:44:45 GMT   (512kb,D)

Title: A Prospective Study on Sequence-Driven Temporal Sampling and Ego-Motion
  Compensation for Action Recognition in the EPIC-Kitchens Dataset
Authors: Alejandro L\'opez-Cifuentes, Marcos Escudero-Vi\~nolo, Jes\'us
  Besc\'os
Categories: cs.CV
Comments: Paper accepted at CVPR 2020 EPIC Kitchens Workshop
\\
  Action recognition is currently one of the top-challenging research fields in
computer vision. Convolutional Neural Networks (CNNs) have significantly
boosted its performance but rely on fixed-size spatio-temporal windows of
analysis, reducing CNNs temporal receptive fields. Among action recognition
datasets, egocentric recorded sequences have become of important relevance
while entailing an additional challenge: ego-motion is unavoidably transferred
to these sequences. The proposed method aims to cope with it by estimating this
ego-motion or camera motion. The estimation is used to temporally partition
video sequences into motion-compensated temporal \textit{chunks} showing the
action under stable backgrounds and allowing for a content-driven temporal
sampling. A CNN trained in an end-to-end fashion is used to extract temporal
features from each \textit{chunk}, which are late fused. This process leads to
the extraction of features from the whole temporal range of an action,
increasing the temporal receptive field of the network.
\\ ( https://arxiv.org/abs/2008.11588 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11598
Date: Tue, 25 Aug 2020 16:54:46 GMT   (67kb,D)

Title: End-to-End 3D Multi-Object Tracking and Trajectory Forecasting
Authors: Xinshuo Weng, Ye Yuan, Kris Kitani
Categories: cs.CV cs.LG cs.MA cs.RO
Comments: Extended abstract. The first two authors contributed equally. Project
  website: http://www.xinshuoweng.com/projects/GNNTrkForecast. arXiv admin
  note: substantial text overlap with arXiv:2003.07847
\\
  3D multi-object tracking (MOT) and trajectory forecasting are two critical
components in modern 3D perception systems. We hypothesize that it is
beneficial to unify both tasks under one framework to learn a shared feature
representation of agent interaction. To evaluate this hypothesis, we propose a
unified solution for 3D MOT and trajectory forecasting which also incorporates
two additional novel computational units. First, we employ a feature
interaction technique by introducing Graph Neural Networks (GNNs) to capture
the way in which multiple agents interact with one another. The GNN is able to
model complex hierarchical interactions, improve the discriminative feature
learning for MOT association, and provide socially-aware context for trajectory
forecasting. Second, we use a diversity sampling function to improve the
quality and diversity of our forecasted trajectories. The learned sampling
function is trained to efficiently extract a variety of outcomes from a
generative trajectory distribution and helps avoid the problem of generating
many duplicate trajectory samples. We show that our method achieves
state-of-the-art performance on the KITTI dataset. Our project website is at
http://www.xinshuoweng.com/projects/GNNTrkForecast.
\\ ( https://arxiv.org/abs/2008.11598 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11600
Date: Wed, 26 Aug 2020 14:53:24 GMT   (1101kb,D)

Title: Estimating Example Difficulty using Variance of Gradients
Authors: Chirag Agarwal, Sara Hooker
Categories: cs.CV cs.LG
Comments: Accepted to Workshop on Human Interpretability in Machine Learning
  (WHI), ICML, 2020
\\
  In machine learning, a question of great interest is understanding what
examples are challenging for a model to classify. Identifying atypical examples
helps inform safe deployment of models, isolates examples that require further
human inspection, and provides interpretability into model behavior. In this
work, we propose Variance of Gradients (VOG) as a proxy metric for detecting
outliers in the data distribution. We provide quantitative and qualitative
support that VOG is a meaningful way to rank data by difficulty and to surface
a tractable subset of the most challenging examples for human-in-the-loop
auditing. Data points with high VOG scores are more difficult for the model to
classify and over-index on examples that require memorization.
\\ ( https://arxiv.org/abs/2008.11600 ,  1101kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11603
Date: Wed, 26 Aug 2020 14:57:47 GMT   (2851kb,D)

Title: An End-to-End Attack on Text-based CAPTCHAs Based on Cycle-Consistent
  Generative Adversarial Network
Authors: Chunhui Li, Xingshu Chen, Haizhou Wang, Yu Zhang, Peiming Wang
Categories: cs.CV
\\
  As a widely deployed security scheme, text-based CAPTCHAs have become more
and more difficult to resist machine learning-based attacks. So far, many
researchers have conducted attacking research on text-based CAPTCHAs deployed
by different companies (such as Microsoft, Amazon, and Apple) and achieved
certain results.However, most of these attacks have some shortcomings, such as
poor portability of attack methods, requiring a series of data preprocessing
steps, and relying on large amounts of labeled CAPTCHAs. In this paper, we
propose an efficient and simple end-to-end attack method based on
cycle-consistent generative adversarial networks. Compared with previous
studies, our method greatly reduces the cost of data labeling. In addition,
this method has high portability. It can attack common text-based CAPTCHA
schemes only by modifying a few configuration parameters, which makes the
attack easier. Firstly, we train CAPTCHA synthesizers based on the cycle-GAN to
generate some fake samples. Basic recognizers based on the convolutional
recurrent neural network are trained with the fake data. Subsequently, an
active transfer learning method is employed to optimize the basic recognizer
utilizing tiny amounts of labeled real-world CAPTCHA samples. Our approach
efficiently cracked the CAPTCHA schemes deployed by 10 popular websites,
indicating that our attack is likely very general. Additionally, we analyzed
the current most popular anti-recognition mechanisms. The results show that the
combination of more anti-recognition mechanisms can improve the security of
CAPTCHA, but the improvement is limited. Conversely, generating more complex
CAPTCHAs may cost more resources and reduce the availability of CAPTCHAs.
\\ ( https://arxiv.org/abs/2008.11603 ,  2851kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11604
Date: Wed, 26 Aug 2020 15:02:04 GMT   (24661kb,D)

Title: Cross-Spectral Periocular Recognition with Conditional Adversarial
  Networks
Authors: Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Josef Bigun
Categories: cs.CV
Comments: Accepted for publication at 2020 International Joint Conference on
  Biometrics (IJCB 2020)
\\
  This work addresses the challenge of comparing periocular images captured in
different spectra, which is known to produce significant drops in performance
in comparison to operating in the same spectrum. We propose the use of
Conditional Generative Adversarial Networks, trained to con-vert periocular
images between visible and near-infrared spectra, so that biometric
verification is carried out in the same spectrum. The proposed setup allows the
use of existing feature methods typically optimized to operate in a single
spectrum. Recognition experiments are done using a number of off-the-shelf
periocular comparators based both on hand-crafted features and CNN descriptors.
Using the Hong Kong Polytechnic University Cross-Spectral Iris Images Database
(PolyU) as benchmark dataset, our experiments show that cross-spectral
performance is substantially improved if both images are converted to the same
spectrum, in comparison to matching features extracted from images in different
spectra. In addition to this, we fine-tune a CNN based on the ResNet50
architecture, obtaining a cross-spectral periocular performance of EER=1%, and
GAR>99% @ FAR=1%, which is comparable to the state-of-the-art with the PolyU
database.
\\ ( https://arxiv.org/abs/2008.11604 ,  24661kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11638
Date: Wed, 26 Aug 2020 16:01:00 GMT   (7245kb,D)

Title: Buy Me That Look: An Approach for Recommending Similar Fashion Products
Authors: Abhinav Ravi, Sandeep Repakula, Ujjal Kr Dutta, Maulik Parmar
Categories: cs.CV cs.LG
Comments: Preprint
\\
  The recent proliferation of numerous fashion e-commerce platforms has led to
a surge in online shopping of fashion products. Fashion being the dominant
aspect in online retail sales, demands for efficient and effective fashion
products recommendation systems that could boost revenue, improve customer
experience and engagement. In this paper, we focus on the problem of similar
fashion item recommendation for multiple fashion items. Given a Product Display
Page for a fashion item in an online e-commerce platform, we identify the
images with a full-shot look, i.e., the one with a full human model wearing the
fashion item. While the majority of existing works in this domain focus on
retrieving similar products corresponding to a single item present in a query,
we focus on the retrieval of multiple fashion items at once. This is an
important problem because while a user might have searched for a particular
primary article type (e.g., men's shorts), the human model in the full-shot
look image would usually be wearing secondary fashion items as well (e.g.,
t-shirts, shoes etc). Upon looking at the full-shot look image in the PDP, the
user might also be interested in viewing similar items for the secondary
article types. To address this need, we use human keypoint detection to first
identify the fullshot images, from which we subsequently select the front
facing ones. An article detection and localisation module pretrained on a
large-dataset is then used to identify different articles in the image. The
detected articles and the catalog database images are then represented in a
common embedding space, for the purpose of similarity based retrieval. We make
use of a triplet-based neural network to obtain the embeddings. Our embedding
network by virtue of an active-learning component achieves further improvements
in the retrieval performance.
\\ ( https://arxiv.org/abs/2008.11638 ,  7245kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11646
Date: Wed, 26 Aug 2020 16:06:11 GMT   (5719kb,D)

Title: Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization
Authors: Tingyu Wang, Zhedong Zheng, Chenggang Yan, and Yi Yang
Categories: cs.CV cs.LG
\\
  Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geo-localization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on two prevailing benchmarks, i.e.,
University-1652 and CVUSA. Besides, we also show the proposed LPN can be easily
embedded into other frameworks to further boost performance.
\\ ( https://arxiv.org/abs/2008.11646 ,  5719kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11647
Date: Wed, 26 Aug 2020 16:06:24 GMT   (2670kb,D)

Title: RNN-based Pedestrian Crossing Prediction using Activity and Pose-related
  Features
Authors: Javier Lorenzo, Ignacio Parra, Florian Wirth, Christoph Stiller, David
  Fernandez Llorca and Miguel Angel Sotelo
Categories: cs.CV cs.LG
Comments: 6 pages, 5 figures. This work has been accepted for publication at
  IEEE Intelligent Vehicle Symposium 2020
\\
  Pedestrian crossing prediction is a crucial task for autonomous driving.
Numerous studies show that an early estimation of the pedestrian's intention
can decrease or even avoid a high percentage of accidents. In this paper,
different variations of a deep learning system are proposed to attempt to solve
this problem. The proposed models are composed of two parts: a CNN-based
feature extractor and an RNN module. All the models were trained and tested on
the JAAD dataset. The results obtained indicate that the choice of the features
extraction method, the inclusion of additional variables such as pedestrian
gaze direction and discrete orientation, and the chosen RNN type have a
significant impact on the final performance.
\\ ( https://arxiv.org/abs/2008.11647 ,  2670kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11662
Date: Wed, 26 Aug 2020 16:42:21 GMT   (9890kb,D)

Title: Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles
  via Apparel Attributes
Authors: Rajdeep Hazra Banerjee, Abhinav Ravi, Ujjal Kr Dutta
Categories: cs.CV
Comments: Preprint
\\
  Popular fashion e-commerce platforms mostly provide details about low-level
attributes of an apparel (for example, neck type, dress length, collar type,
print etc) on their product detail pages. However, customers usually prefer to
buy apparels based on their style information, or simply put, occasion (for
example, party wear, sports wear, casual wear etc). Application of a supervised
image-captioning model to generate style-based image captions is limited
because obtaining ground-truth annotations in the form of style-based captions
is difficult. This is because annotating style-based captions requires a
certain amount of fashion domain expertise, and also adds to the costs and
manual effort. On the contrary, low-level attribute based annotations are much
more easily available. To address this issue, we propose a transfer-learning
based image captioning model that is trained on a source dataset with
sufficient attribute-based ground-truth captions, and used to predict
style-based captions on a target dataset. The target dataset has only a limited
amount of images with style-based ground-truth captions. The main motivation of
our approach comes from the fact that most often there are correlations among
the low-level attributes and the higher-level styles for an apparel. We
leverage this fact and train our model in an encoder-decoder based framework
using attention mechanism. In particular, the encoder of the model is first
trained on the source dataset to obtain latent representations capturing the
low-level attributes. The trained model is fine-tuned to generate style-based
captions for the target dataset. To highlight the effectiveness of our method,
we qualitatively demonstrate that the captions generated by our approach are
close to the actual style information for the evaluated apparels.
\\ ( https://arxiv.org/abs/2008.11662 ,  9890kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11672
Date: Wed, 26 Aug 2020 16:56:57 GMT   (9879kb,D)

Title: DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment
  in COVID-19 Pandemic
Authors: Mahdi Rezaei, Mohsen Azarmi
Categories: cs.CV cs.LG eess.IV physics.med-ph
\\
  Social distancing is a recommended solution by the World Health Organisation
(WHO) to minimise the spread of COVID-19 in public places. The majority of
governments and national health authorities have set the 2-meter physical
distancing as a mandatory safety measure in shopping centres, schools and other
covered areas. In this research, we develop a generic Deep Neural Network-Based
model for automated people detection, tracking, and inter-people distances
estimation in the crowd, using common CCTV security cameras. The proposed model
includes a YOLOv4-based framework and inverse perspective mapping for accurate
people detection and social distancing monitoring in challenging conditions,
including people occlusion, partial visibility, and lighting variations. We
also provide an online risk assessment scheme by statistical analysis of the
Spatio-temporal data from the moving trajectories and the rate of social
distancing violations. We identify high-risk zones with the highest possibility
of virus spread and infections. This may help authorities to redesign the
layout of a public place or to take precaution actions to mitigate high-risk
zones. The efficiency of the proposed methodology is evaluated on the Oxford
Town Centre dataset, with superior performance in terms of accuracy and speed
compared to three state-of-the-art methods.
\\ ( https://arxiv.org/abs/2008.11672 ,  9879kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11689
Date: Wed, 26 Aug 2020 17:27:52 GMT   (2175kb)

Title: 5G Utility Pole Planner Using Google Street View and Mask R-CNN
Authors: Yanyu Zhang, Osama Alshaykh
Categories: cs.CV cs.LG eess.IV
Comments: 4 pages, 7 figures
\\
  With the advances of fifth-generation (5G) cellular networks technology, many
studies and work have been carried out on how to build 5G networks for smart
cities. In the previous research, street lighting poles and smart light poles
are capable of being a 5G access point. In order to determine the position of
the points, this paper discusses a new way to identify poles based on Mask
R-CNN, which extends Fast R-CNNs by making it employ recursive Bayesian
filtering and perform proposal propagation and reuse. The dataset contains
3,000 high-resolution images from google map. To make training faster, we used
a very efficient GPU implementation of the convolution operation. We achieved a
train error rate of 7.86% and a test error rate of 32.03%. At last, we used the
immune algorithm to set 5G poles in the smart cities.
\\ ( https://arxiv.org/abs/2008.11689 ,  2175kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11702
Date: Wed, 26 Aug 2020 17:44:23 GMT   (2442kb,D)

Title: Delving into Inter-Image Invariance for Unsupervised Visual
  Representations
Authors: Jiahao Xie, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, Chen Change Loy
Categories: cs.CV cs.LG
\\
  Contrastive learning has recently shown immense potential in unsupervised
visual representation learning. Existing studies in this track mainly focus on
intra-image invariance learning. The learning typically uses rich intra-image
transformations to construct positive pairs and then maximizes agreement using
a contrastive loss. The merits of inter-image invariance, conversely, remain
much less explored. One major obstacle to exploit inter-image invariance is
that it is unclear how to reliably construct inter-image positive pairs, and
further derive effective supervision from them since there are no pair
annotations available. In this work, we present a rigorous and comprehensive
study on inter-image invariance learning from three main constituting
components: pseudo-label maintenance, sampling strategy, and decision boundary
design. Through carefully-designed comparisons and analysis, we propose a
unified framework that supports the integration of unsupervised intra- and
inter-image invariance learning. With all the obtained recipes, our final
model, namely InterCLR, achieves state-of-the-art performance on standard
benchmarks. Code and models will be available at
https://github.com/open-mmlab/OpenSelfSup.
\\ ( https://arxiv.org/abs/2008.11702 ,  2442kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11713
Date: Wed, 26 Aug 2020 17:59:36 GMT   (4305kb,D)

Title: NAS-DIP: Learning Deep Image Prior with Neural Architecture Search
Authors: Yun-Chun Chen, Chen Gao, Esther Robb, Jia-Bin Huang
Categories: cs.CV
Comments: ECCV 2020. Project: https://yunchunchen.github.io/NAS-DIP/ Code:
  https://github.com/YunChunChen/NAS-DIP-pytorch The first two authors
  contributed equally to this work
\\
  Recent work has shown that the structure of deep convolutional neural
networks can be used as a structured image prior for solving various inverse
image restoration tasks. Instead of using hand-designed architectures, we
propose to search for neural architectures that capture stronger image priors.
Building upon a generic U-Net architecture, our core contribution lies in
designing new search spaces for (1) an upsampling cell and (2) a pattern of
cross-scale residual connections. We search for an improved network by
leveraging an existing neural architecture search algorithm (using
reinforcement learning with a recurrent neural network controller). We validate
the effectiveness of our method via a wide variety of applications, including
image restoration, dehazing, image-to-image translation, and matrix
factorization. Extensive experimental results show that our algorithm performs
favorably against state-of-the-art learning-free approaches and reaches
competitive performance with existing learning-based methods in some cases.
\\ ( https://arxiv.org/abs/2008.11713 ,  4305kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11714
Date: Wed, 26 Aug 2020 17:59:40 GMT   (23200kb,D)

Title: DRG: Dual Relation Graph for Human-Object Interaction Detection
Authors: Chen Gao, Jiarui Xu, Yuliang Zou, Jia-Bin Huang
Categories: cs.CV
Comments: ECCV 2020. Project: http://chengao.vision/DRG/ Code:
  https://github.com/vt-vl-lab/DRG
\\
  We tackle the challenging problem of human-object interaction (HOI)
detection. Existing methods either recognize the interaction of each
human-object pair in isolation or perform joint inference based on complex
appearance-based features. In this paper, we leverage an abstract
spatial-semantic representation to describe each human-object pair and
aggregate the contextual information of the scene via a dual relation graph
(one human-centric and one object-centric). Our proposed dual relation graph
effectively captures discriminative cues from the scene to resolve ambiguity
from local predictions. Our model is conceptually simple and leads to favorable
results compared to the state-of-the-art HOI detection algorithms on two
large-scale benchmark datasets.
\\ ( https://arxiv.org/abs/2008.11714 ,  23200kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11298
Date: Tue, 25 Aug 2020 22:45:34 GMT   (1022kb,D)

Title: Rethinking Non-idealities in Memristive Crossbars for Adversarial
  Robustness in Neural Networks
Authors: Abhiroop Bhattacharjee and Priyadarshini Panda
Categories: cs.ET cs.NE
Comments: 11 pages, 13 figures
\\
  \textit{Deep Neural Networks} (DNNs) have been shown to be prone to
adversarial attacks. With a growing need to enable intelligence in embedded
devices in this \textit{Internet of Things} (IoT) era, secure hardware
implementation of DNNs has become imperative. Memristive crossbars, being able
to perform \textit{Matrix-Vector-Multiplications} (MVMs) efficiently, are used
to realize DNNs on hardware. However, crossbar non-idealities have always been
devalued since they cause errors in performing MVMs, leading to degradation in
the accuracy of the DNNs. Several software-based adversarial defenses have been
proposed in the past to make DNNs adversarially robust. However, no previous
work has demonstrated the advantage conferred by the non-idealities present in
analog crossbars in terms of adversarial robustness. In this work, we show that
the intrinsic hardware variations manifested through crossbar non-idealities
yield adversarial robustness to the mapped DNNs without any additional
optimization. We evaluate resilience of state-of-the-art DNNs (VGG8 \& VGG16
networks) using benchmark datasets (CIFAR-10 \& CIFAR-100) across various
crossbar sizes towards both hardware and software adversarial attacks. We find
that crossbar non-idealities unleash greater adversarial robustness
($>10-20\%$) in DNNs than baseline software DNNs. We further assess the
performance of our approach with other state-of-the-art efficiency-driven
adversarial defenses and find that our approach performs significantly well in
terms of reducing adversarial losses.
\\ ( https://arxiv.org/abs/2008.11298 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11657
Date: Wed, 26 Aug 2020 16:33:42 GMT   (43754kb,D)

Title: Test Scene Design for Physically Based Rendering
Authors: Elias Brugger, Christian Freude, Michael Wimmer
Categories: cs.GR
\\
  Physically based rendering is a discipline in computer graphics which aims at
reproducing certain light and material appearances that occur in the real
world. Complex scenes can be difficult to compute for rendering algorithms.
This paper introduces a new comprehensive test database of scenes that treat
different light setups in conjunction with diverse materials and discusses its
design principles. A lot of research is focused on the development of new
algorithms that can deal with difficult light conditions and materials
efficiently. This database delivers a comprehensive foundation for evaluating
existing and newly developed rendering techniques. A final evaluation compares
different results of different rendering algorithms for all scenes.
\\ ( https://arxiv.org/abs/2008.11657 ,  43754kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2008.11434 (*cross-listing*)
Date: Wed, 26 Aug 2020 08:10:48 GMT   (7119kb,D)

Title: Better Than Reference In Low Light Image Enhancement: Conditional
  Re-Enhancement Networks
Authors: Yu Zhang, Xiaoguang Di, Bin Zhang, Ruihang Ji, and Chunhui Wang
Categories: eess.IV cs.CV
Comments: 10 pages, 8 figures
\\
  Low light images suffer from severe noise, low brightness, low contrast, etc.
In previous researches, many image enhancement methods have been proposed, but
few methods can deal with these problems simultaneously. In this paper, to
solve these problems simultaneously, we propose a low light image enhancement
method that can combined with supervised learning and previous HSV (Hue,
Saturation, Value) or Retinex model based image enhancement methods. First, we
analyse the relationship between the HSV color space and the Retinex theory,
and show that the V channel (V channel in HSV color space, equals the maximum
channel in RGB color space) of the enhanced image can well represent the
contrast and brightness enhancement process. Then, a data-driven conditional
re-enhancement network (denoted as CRENet) is proposed. The network takes low
light images as input and the enhanced V channel as condition, then it can
re-enhance the contrast and brightness of the low light image and at the same
time reduce noise and color distortion. It should be noted that during the
training process, any paired images with different exposure time can be used
for training, and there is no need to carefully select the supervised images
which will save a lot. In addition, it takes less than 20 ms to process a color
image with the resolution 400*600 on a 2080Ti GPU. Finally, some comparative
experiments are implemented to prove the effectiveness of the method. The
results show that the method proposed in this paper can significantly improve
the quality of the enhanced image, and by combining with other image contrast
enhancement methods, the final enhancement result can even be better than the
reference image in contrast and brightness. (Code will be available at
https://github.com/hitzhangyu/image-enhancement-with-denoise)
\\ ( https://arxiv.org/abs/2008.11434 ,  7119kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11449 (*cross-listing*)
Date: Wed, 26 Aug 2020 09:05:07 GMT   (15851kb,D)

Title: Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution
  using Dynamic Filters
Authors: Qingyan Sun, Shuo Zhang, Song Chang, Lixi Zhu and Youfang Lin
Categories: eess.IV cs.CV
\\
  Light field cameras have been proved to be powerful tools for 3D
reconstruction and virtual reality applications. However, the limited
resolution of light field images brings a lot of difficulties for further
information display and extraction. In this paper, we introduce a novel
learning-based framework to improve the spatial resolution of light fields.
First, features from different dimensions are parallelly extracted and fused
together in our multi-dimension fusion architecture. These features are then
used to generate dynamic filters, which extract subpixel information from
micro-lens images and also implicitly consider the disparity information.
Finally, more high-frequency details learned in the residual branch are added
to the upsampled images and the final super-resolved light fields are obtained.
Experimental results show that the proposed method uses fewer parameters but
achieves better performances than other state-of-the-art methods in various
kinds of datasets. Our reconstructed images also show sharp details and
distinct lines in both sub-aperture images and epipolar plane images.
\\ ( https://arxiv.org/abs/2008.11449 ,  15851kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11478 (*cross-listing*)
Date: Wed, 26 Aug 2020 10:34:45 GMT   (2315kb)

Title: DRR4Covid: Learning Automated COVID-19 Infection Segmentation from
  Digitally Reconstructed Radiographs
Authors: Pengyi Zhang, Yunxin Zhong, Yulin Deng, Xiaoying Tang, Xiaoqiong Li
Categories: eess.IV cs.CV cs.LG
\\
  Automated infection measurement and COVID-19 diagnosis based on Chest X-ray
(CXR) imaging is important for faster examination. We propose a novel approach,
called DRR4Covid, to learn automated COVID-19 diagnosis and infection
segmentation on CXRs from digitally reconstructed radiographs (DRRs). DRR4Covid
comprises of an infection-aware DRR generator, a classification and/or
segmentation network, and a domain adaptation module. The infection-aware DRR
generator is able to produce DRRs with adjustable strength of radiological
signs of COVID-19 infection, and generate pixel-level infection annotations
that match the DRRs precisely. The domain adaptation module is introduced to
reduce the domain discrepancy between DRRs and CXRs by training networks on
unlabeled real CXRs and labeled DRRs together.We provide a simple but effective
implementation of DRR4Covid by using a domain adaptation module based on
Maximum Mean Discrepancy (MMD), and a FCN-based network with a classification
header and a segmentation header. Extensive experiment results have confirmed
the efficacy of our method; specifically, quantifying the performance by
accuracy, AUC and F1-score, our network without using any annotations from CXRs
has achieved a classification score of (0.954, 0.989, 0.953) and a segmentation
score of (0.957, 0.981, 0.956) on a test set with 794 normal cases and 794
positive cases. Besides, we estimate the sensitive of X-ray images in detecting
COVID-19 infection by adjusting the strength of radiological signs of COVID-19
infection in synthetic DRRs. The estimated detection limit of the proportion of
infected voxels in the lungs is 19.43%, and the estimated lower bound of the
contribution rate of infected voxels is 20.0% for significant radiological
signs of COVID-19 infection. Our codes will be made publicly available at
https://github.com/PengyiZhang/DRR4Covid.
\\ ( https://arxiv.org/abs/2008.11478 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11491 (*cross-listing*)
Date: Wed, 26 Aug 2020 11:07:50 GMT   (1198kb,D)

Title: Selective Particle Attention: Visual Feature-Based Attention in Deep
  Reinforcement Learning
Authors: Sam Blakeman, Denis Mareschal
Categories: q-bio.NC cs.CV cs.LG cs.NE
\\
  The human brain uses selective attention to filter perceptual input so that
only the components that are useful for behaviour are processed using its
limited computational resources. We focus on one particular form of visual
attention known as feature-based attention, which is concerned with identifying
features of the visual input that are important for the current task regardless
of their spatial location. Visual feature-based attention has been proposed to
improve the efficiency of Reinforcement Learning (RL) by reducing the
dimensionality of state representations and guiding learning towards relevant
features. Despite achieving human level performance in complex perceptual-motor
tasks, Deep RL algorithms have been consistently criticised for their poor
efficiency and lack of flexibility. Visual feature-based attention therefore
represents one option for addressing these criticisms. Nevertheless, it is
still an open question how the brain is able to learn which features to attend
to during RL. To help answer this question we propose a novel algorithm, termed
Selective Particle Attention (SPA), which imbues a Deep RL agent with the
ability to perform selective feature-based attention. SPA learns which
combinations of features to attend to based on their bottom-up saliency and how
accurately they predict future reward. We evaluate SPA on a multiple choice
task and a 2D video game that both involve raw pixel input and dynamic changes
to the task structure. We show various benefits of SPA over approaches that
naively attend to either all or random subsets of features. Our results
demonstrate (1) how visual feature-based attention in Deep RL models can
improve their learning efficiency and ability to deal with sudden changes in
task structure and (2) that particle filters may represent a viable
computational account of how visual feature-based attention occurs in the
brain.
\\ ( https://arxiv.org/abs/2008.11491 ,  1198kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11508 (*cross-listing*)
Date: Tue, 25 Aug 2020 16:51:12 GMT   (5808kb)

Title: Detection of Retinal Blood Vessels by using Gabor filter with Entropic
  threshold
Authors: Mohamed. I. Waly, Ahmed El-Hossiny
Categories: eess.IV cs.CV
Journal-ref: Vol. 4, issue 2, November 2016 - Safar 1438
\\
  Diabetic retinopathy is the basic reason for visual deficiency. This paper
introduces a programmed strategy to identify and dispense with the blood
vessels. The location of the blood vessels is the fundamental stride in the
discovery of diabetic retinopathy because the blood vessels are the typical
elements of the retinal picture. The location of the blood vessels can help the
ophthalmologists to recognize the sicknesses prior and quicker. The blood
vessels recognized and wiped out by utilizing Gobar filter on two freely
accessible retinal databases which are STARE and DRIVE. The exactness of
segmentation calculation is assessed quantitatively by contrasting the
physically sectioned pictures and the comparing yield pictures, the Gabor
filter with Entropic threshold vessel pixel segmentation by Entropic
thresholding is better vessels with less false positive portion rate.
\\ ( https://arxiv.org/abs/2008.11508 ,  5808kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11514 (*cross-listing*)
Date: Wed, 26 Aug 2020 12:20:09 GMT   (615kb,D)

Title: Disentangled Representations for Domain-generalized Cardiac Segmentation
Authors: Xiao Liu, Spyridon Thermos, Agisilaos Chartsias, Alison O'Neil and
  Sotirios A. Tsaftaris
Categories: eess.IV cs.CV
Comments: Accepted by STACOM 2020
\\
  Robust cardiac image segmentation is still an open challenge due to the
inability of the existing methods to achieve satisfactory performance on unseen
data of different domains. Since the acquisition and annotation of medical data
are costly and time-consuming, recent work focuses on domain adaptation and
generalization to bridge the gap between data from different populations and
scanners. In this paper, we propose two data augmentation methods that focus on
improving the domain adaptation and generalization abilities of
state-to-the-art cardiac segmentation models. In particular, our "Resolution
Augmentation" method generates more diverse data by rescaling images to
different resolutions within a range spanning different scanner protocols.
Subsequently, our "Factor-based Augmentation" method generates more diverse
data by projecting the original samples onto disentangled latent spaces, and
combining the learned anatomy and modality factors from different domains. Our
extensive experiments demonstrate the importance of efficient adaptation
between seen and unseen domains, as well as model generalization ability, to
robust cardiac image segmentation.
\\ ( https://arxiv.org/abs/2008.11514 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11546 (*cross-listing*)
Date: Tue, 25 Aug 2020 02:52:18 GMT   (26425kb,D)

Title: Towards Structured Prediction in Bioinformatics with Deep Learning
Authors: Yu Li
Categories: q-bio.QM cs.CV cs.LG
Comments: PhD dissertatation
\\
  Using machine learning, especially deep learning, to facilitate biological
research is a fascinating research direction. However, in addition to the
standard classification or regression problems, in bioinformatics, we often
need to predict more complex structured targets, such as 2D images and 3D
molecular structures. The above complex prediction tasks are referred to as
structured prediction. Structured prediction is more complicated than the
traditional classification but has much broader applications, considering that
most of the original bioinformatics problems have complex output objects. Due
to the properties of those structured prediction problems, such as having
problem-specific constraints and dependency within the labeling space, the
straightforward application of existing deep learning models can lead to
unsatisfactory results. Here, we argue that the following ideas can help
resolve structured prediction problems in bioinformatics. Firstly, we can
combine deep learning with other classic algorithms, such as probabilistic
graphical models, which model the problem structure explicitly. Secondly, we
can design the problem-specific deep learning architectures or methods by
considering the structured labeling space and problem constraints, either
explicitly or implicitly. We demonstrate our ideas with six projects from four
bioinformatics subfields, including sequencing analysis, structure prediction,
function annotation, and network analysis. The structured outputs cover 1D
signals, 2D images, 3D structures, hierarchical labeling, and heterogeneous
networks. With the help of the above ideas, all of our methods can achieve SOTA
performance on the corresponding problems. The success of these projects
motivates us to extend our work towards other more challenging but important
problems, such as health-care problems, which can directly benefit people's
health and wellness.
\\ ( https://arxiv.org/abs/2008.11546 ,  26425kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11572 (*cross-listing*)
Date: Wed, 26 Aug 2020 14:16:01 GMT   (209kb,D)

Title: On the Composition and Limitations of Publicly Available COVID-19 X-Ray
  Imaging Datasets
Authors: Beatriz Garcia Santa Cruz, Jan S\"olter, Matias Nicolas Bossa and
  Andreas Dominik Husch
Categories: eess.IV cs.CV cs.LG
Comments: 12 pages, 3 figures
\\
  Machine learning based methods for diagnosis and progression prediction of
COVID-19 from imaging data have gained significant attention in the last
months, in particular by the use of deep learning models. In this context
hundreds of models where proposed with the majority of them trained on public
datasets. Data scarcity, mismatch between training and target population, group
imbalance, and lack of documentation are important sources of bias, hindering
the applicability of these models to real-world clinical practice. Considering
that datasets are an essential part of model building and evaluation, a deeper
understanding of the current landscape is needed. This paper presents an
overview of the currently public available COVID-19 chest X-ray datasets. Each
dataset is briefly described and potential strength, limitations and
interactions between datasets are identified. In particular, some key
properties of current datasets that could be potential sources of bias,
impairing models trained on them are pointed out. These descriptions are useful
for model building on those datasets, to choose the best dataset according the
model goal, to take into account the specific limitations to avoid reporting
overconfident benchmark results, and to discuss their impact on the
generalisation capabilities in a specific clinical setting
\\ ( https://arxiv.org/abs/2008.11572 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11576 (*cross-listing*)
Date: Tue, 25 Aug 2020 04:32:29 GMT   (440kb,D)

Title: 3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction
Authors: Rupal Agravat, Mehul S Raval
Categories: eess.IV cs.CV cs.LG
Comments: 11 pages, 3 figures, BRaTS 2020. arXiv admin note: text overlap with
  arXiv:1909.09399
\\
  Glioma, the malignant brain tumor, requires immediate treatment to improve
the survival of patients. Gliomas heterogeneous nature makes the segmentation
difficult, especially for sub-regions like necrosis, enhancing tumor,
non-enhancing tumor, and Edema. Deep neural networks like full convolution
neural networks and ensemble of fully convolution neural networks are
successful for Glioma segmentation. The paper demonstrates the use of a 3D
fully convolution neural network with a three layer encoder decoder approach
for layer arrangement. The encoder blocks include the dense modules, and
decoder blocks include convolution modules. The input to the network is 3D
patches. The loss function combines dice loss and focal loss functions. The
validation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing
tumor, whole tumor, and tumor core, respectively. The Random Forest Regressor
uses shape, volumetric, and age features extracted from ground truth for
overall survival prediction. The regressor achieves an accuracy of 44.8% on the
validation set.
\\ ( https://arxiv.org/abs/2008.11576 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11639 (*cross-listing*)
Date: Tue, 25 Aug 2020 10:51:54 GMT   (1230kb,D)

Title: Analysis of deep machine learning algorithms in COVID-19 disease
  diagnosis
Authors: Samir S. Yadav, Mininath R. Bendre, Pratap S. Vikhe, Shivajirao M.
  Jadhav
Categories: cs.LG cs.CV eess.IV
Comments: 17 pages, 10 figures, 11 tables
\\
  The aim of the work is to use deep neural network models for solving the
problem of image recognition. These days, every human being is threatened by a
harmful coronavirus disease, also called COVID-19 disease. The spread of
coronavirus affects the economy of many countries in the world. To find
COVID-19 patients early is very essential to avoid the spread and harm to
society. Pathological tests and Chromatography(CT) scans are helpful for the
diagnosis of COVID-19. However, these tests are having drawbacks such as a
large number of false positives, and cost of these tests are so expensive.
Hence, it requires finding an easy, accurate, and less expensive way for the
detection of the harmful COVID-19 disease. Chest-x-ray can be useful for the
detection of this disease. Therefore, in this work chest, x-ray images are used
for the diagnosis of suspected COVID-19 patients using modern machine learning
techniques. The analysis of the results is carried out and conclusions are made
about the effectiveness of deep machine learning algorithms in image
recognition problems.
\\ ( https://arxiv.org/abs/2008.11639 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11673 (*cross-listing*)
Date: Wed, 26 Aug 2020 16:57:45 GMT   (748kb,D)

Title: Orientation-Disentangled Unsupervised Representation Learning for
  Computational Pathology
Authors: Maxime W. Lafarge, Josien P.W. Pluim and Mitko Veta
Categories: eess.IV cs.CV
\\
  Unsupervised learning enables modeling complex images without the need for
annotations. The representation learned by such models can facilitate any
subsequent analysis of large image datasets.
  However, some generative factors that cause irrelevant variations in images
can potentially get entangled in such a learned representation causing the risk
of negatively affecting any subsequent use. The orientation of imaged objects,
for instance, is often arbitrary/irrelevant, thus it can be desired to learn a
representation in which the orientation information is disentangled from all
other factors.
  Here, we propose to extend the Variational Auto-Encoder framework by
leveraging the group structure of rotation-equivariant convolutional networks
to learn orientation-wise disentangled generative factors of histopathology
images. This way, we enforce a novel partitioning of the latent space, such
that oriented and isotropic components get separated.
  We evaluated this structured representation on a dataset that consists of
tissue regions for which nuclear pleomorphism and mitotic activity was assessed
by expert pathologists. We show that the trained models efficiently disentangle
the inherent orientation information of single-cell images. In comparison to
classical approaches, the resulting aggregated representation of
sub-populations of cells produces higher performances in subsequent tasks.
\\ ( https://arxiv.org/abs/2008.11673 ,  748kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1802.06897
replaced with revised version Tue, 25 Aug 2020 22:42:17 GMT   (1277kb,D)

Title: Machine Learning Methods for Data Association in Multi-Object Tracking
Authors: Patrick Emami, Panos M. Pardalos, Lily Elefteriadou, Sanjay Ranka
Categories: cs.CV
Comments: Accepted for publication in ACM Computing Surveys
Journal-ref: Volume 53, Issue 4 (August 2020)
DOI: 10.1145/3394659
\\ ( https://arxiv.org/abs/1802.06897 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:1909.01643
replaced with revised version Wed, 26 Aug 2020 09:42:09 GMT   (8011kb,D)

Title: PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud
Authors: Xin Kong, Guangyao Zhai, Baoquan Zhong, Yong Liu
Categories: cs.CV cs.RO
Comments: This paper has been accepted by IROS-2019
DOI: 10.1109/IROS40897.2019.8968296
\\ ( https://arxiv.org/abs/1909.01643 ,  8011kb)
------------------------------------------------------------------------------
\\
arXiv:1909.06121
replaced with revised version Wed, 26 Aug 2020 04:52:15 GMT   (3447kb,D)

Title: Dual Graph Convolutional Network for Semantic Segmentation
Authors: Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip
  H.S. Torr
Categories: cs.CV
Comments: BMVC 2019. Code is available at
  \url{https://github.com/lxtGH/GALD-DGCNet}
\\ ( https://arxiv.org/abs/1909.06121 ,  3447kb)
------------------------------------------------------------------------------
\\
arXiv:1912.04260
replaced with revised version Wed, 26 Aug 2020 13:03:16 GMT   (2899kb,D)

Title: Side-Aware Boundary Localization for More Precise Object Detection
Authors: Jiaqi Wang, Wenwei Zhang, Yuhang Cao, Kai Chen, Jiangmiao Pang, Tao
  Gong, Jianping Shi, Chen Change Loy, Dahua Lin
Categories: cs.CV
Comments: ECCV 2020 Spotlight
\\ ( https://arxiv.org/abs/1912.04260 ,  2899kb)
------------------------------------------------------------------------------
\\
arXiv:1912.05317
replaced with revised version Wed, 26 Aug 2020 09:50:48 GMT   (317kb,D)

Title: A Variational-Sequential Graph Autoencoder for Neural Architecture
  Performance Prediction
Authors: David Friede, Jovita Lukasik, Heiner Stuckenschmidt, Margret Keuper
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/1912.05317 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2004.08222
replaced with revised version Wed, 26 Aug 2020 03:49:13 GMT   (1720kb,D)

Title: Learning to Predict Context-adaptive Convolution for Semantic
  Segmentation
Authors: Jianbo Liu, Junjun He, Jimmy S. Ren, Yu Qiao, Hongsheng Li
Categories: cs.CV
Comments: Accepted in ECCV 2020
\\ ( https://arxiv.org/abs/2004.08222 ,  1720kb)
------------------------------------------------------------------------------
\\
arXiv:2005.06514
replaced with revised version Wed, 26 Aug 2020 02:40:45 GMT   (10338kb,D)

Title: 3D Face Anti-spoofing with Factorized Bilinear Coding
Authors: Shan Jia, Xin Li, Chuanbo Hu, Guodong Guo, Zhengquan Xu
Categories: cs.CV eess.IV
Comments: arXiv admin note: text overlap with arXiv:1910.05457
\\ ( https://arxiv.org/abs/2005.06514 ,  10338kb)
------------------------------------------------------------------------------
\\
arXiv:2006.07634
replaced with revised version Wed, 26 Aug 2020 06:45:10 GMT   (5397kb,D)

Title: DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms
Authors: Hua Qi and Qing Guo and Felix Juefei-Xu and Xiaofei Xie and Lei Ma and
  Wei Feng and Yang Liu and Jianjun Zhao
Categories: cs.CV
Comments: 11 pages, 7 figures; This paper has been accepted to ACM-MM 2020
\\ ( https://arxiv.org/abs/2006.07634 ,  5397kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13457
replaced with revised version Wed, 26 Aug 2020 14:08:45 GMT   (3005kb,D)

Title: Learning Semantically Enhanced Feature for Fine-Grained Image
  Classification
Authors: Wei Luo and Hengmin Zhang and Jun Li and Xiu-Shen Wei
Categories: cs.CV
Comments: Accepted by IEEE Signal Processing Letters. 5 pages, 4 figures, 4
  tables
\\ ( https://arxiv.org/abs/2006.13457 ,  3005kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01476
replaced with revised version Wed, 26 Aug 2020 07:30:36 GMT   (777kb,D)

Title: Interactive Knowledge Distillation
Authors: Shipeng Fu, Zhen Li, Jun Xu, Ming-Ming Cheng, Zitao Liu, Xiaomin Yang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2007.01476 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2007.04538
replaced with revised version Wed, 26 Aug 2020 07:10:04 GMT   (4353kb,D)

Title: EPI-based Oriented Relation Networks for Light Field Depth Estimation
Authors: Kunyuan Li, Jun Zhang, Rui Sun, Xudong Zhang, Jun Gao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2007.04538 ,  4353kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09883
replaced with revised version Wed, 26 Aug 2020 01:51:02 GMT   (2574kb,D)

Title: Complementary Boundary Generator with Scale-Invariant Relation Modeling
  for Temporal Action Localization: Submission to ActivityNet Challenge 2020
Authors: Haisheng Su, Jinyuan Feng, Hao Shao, Zhenyu Jiang, Manyuan Zhang, Wei
  Wu, Yu Liu, Hongsheng Li, Junjie Yan
Categories: cs.CV
Comments: Submitted to CVPR workshop of ActivityNet Challenge 2020
\\ ( https://arxiv.org/abs/2007.09883 ,  2574kb)
------------------------------------------------------------------------------
\\
arXiv:2008.09694
replaced with revised version Wed, 26 Aug 2020 17:26:13 GMT   (68372kb,D)

Title: Many-shot from Low-shot: Learning to Annotate using Mixed Supervision
  for Object Detection
Authors: Carlo Biffi, Steven McDonagh, Philip Torr, Ales Leonardis, Sarah
  Parisot
Categories: cs.CV eess.IV
Comments: Accepted at ECCV 2020. Camera-ready version and Appendices
\\ ( https://arxiv.org/abs/2008.09694 ,  68372kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10960
replaced with revised version Wed, 26 Aug 2020 09:51:12 GMT   (15968kb,D)

Title: AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with
  Spatially-Aware Conditional GANs
Authors: Julien Despois, Frederic Flament, Matthieu Perrot
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://despoisj.github.io/AgingMapGAN/
\\ ( https://arxiv.org/abs/2008.10960 ,  15968kb)
------------------------------------------------------------------------------
\\
arXiv:2008.09405
replaced with revised version Tue, 25 Aug 2020 19:07:33 GMT   (4497kb,D)

Title: A Tipping Point for the Planarity of Small and Medium Sized Graphs
Authors: Emanuele Balloni, Giuseppe Di Battista, Maurizio Patrignani
Categories: cs.DM math.CO
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\ ( https://arxiv.org/abs/2008.09405 ,  4497kb)
------------------------------------------------------------------------------
\\
arXiv:1908.09572
replaced with revised version Wed, 26 Aug 2020 09:28:55 GMT   (377kb,D)

Title: Neuromorphic Electronic Systems for Reservoir Computing
Authors: Fatemeh Hadaeghi
Categories: cs.ET cs.LG
Comments: This chapter is a contribution to a Springer book project titled
  Reservoir Computing: Theory, Physical Implementations and Applications. This
  pre-print is an updated version of the one submitted in 2019
\\ ( https://arxiv.org/abs/1908.09572 ,  377kb)
------------------------------------------------------------------------------
\\
arXiv:1910.09734
replaced with revised version Wed, 26 Aug 2020 00:03:40 GMT   (1152kb,D)

Title: Single Versus Union: Non-parallel Support Vector Machine Frameworks
Authors: Chun-Na Li, Yuan-Hai Shao, Huajun Wang, Yu-Ting Zhao, Ling-Wei Huang,
  Naihua Xiu and Nai-Yang Deng
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/1910.09734 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11524
replaced with revised version Tue, 25 Aug 2020 23:30:57 GMT   (2514kb,D)

Title: Neuro-Symbolic Visual Reasoning: Disentangling "Visual" from "Reasoning"
Authors: Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang,
  Kazuhito Koishida
Categories: cs.LG cs.AI cs.CV cs.NE cs.SC stat.ML
Comments: Published in Proceedings of the 37th International Conference on
  Machine Learning (ICML), Online, PMLR 119, 2020
\\ ( https://arxiv.org/abs/2006.11524 ,  2514kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11702
replaced with revised version Wed, 26 Aug 2020 06:30:08 GMT   (13716kb,D)

Title: A Universal Representation Transformer Layer for Few-Shot Image
  Classification
Authors: Lu Liu, William Hamilton, Guodong Long, Jing Jiang, Hugo Larochelle
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/2006.11702 ,  13716kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13311 (*cross-listing*)
replaced with revised version Wed, 26 Aug 2020 12:34:24 GMT   (1287kb,D)

Title: 70 years of machine learning in geoscience in review
Authors: Jesper S\"oren Dramsch
Categories: physics.geo-ph cs.CV cs.LG eess.IV
Comments: 36 pages, 17 figures, book chapter
\\ ( https://arxiv.org/abs/2006.13311 ,  1287kb)
------------------------------------------------------------------------------
\\
arXiv:2007.04589
replaced with revised version Wed, 26 Aug 2020 08:15:42 GMT   (9919kb,D)

Title: InfoMax-GAN: Improved Adversarial Image Generation via Information
  Maximization and Contrastive Learning
Authors: Kwot Sin Lee, Ngoc-Trung Tran, Ngai-Man Cheung
Categories: cs.LG cs.CV stat.ML
Comments: Initial version was presented at NeurIPS 2019 Workshop on Information
  Theory and Machine Learning
\\ ( https://arxiv.org/abs/2007.04589 ,  9919kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04488 (*cross-listing*)
replaced with revised version Wed, 26 Aug 2020 00:42:32 GMT   (658kb)

Title: ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images
Authors: Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Weixiong Zhang, Baozhou
  Sun
Categories: eess.IV cs.CV
Comments: 16 pages, 7 figures; under review as a journal article at Medical
  Physics; abstract presented at AAPM 2020
MSC-class: 68T07(Primary), 68T45(Secondary)
\\ ( https://arxiv.org/abs/2008.04488 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03984 (*cross-listing*)
replaced with revised version Wed, 26 Aug 2020 15:02:23 GMT   (29kb)

Title: Asymptotics of the number of 2-threshold functions
Authors: Elena Zamaraeva and Jovisa Zunic
Categories: math.CO cs.DM
Comments: 26 pages
MSC-class: 03B50, 05A18, 52C05, 11P21
ACM-class: G.2.1
\\ ( https://arxiv.org/abs/2007.03984 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03986 (*cross-listing*)
replaced with revised version Wed, 26 Aug 2020 15:00:56 GMT   (29kb)

Title: A characterization of 2-threshold functions via pairs of prime segments
Authors: Elena Zamaraeva and Jovisa Zunic
Categories: math.CO cs.DM
Comments: 21 pages
MSC-class: 05A16, 03B50, 05A18, 52C05, 11P21
ACM-class: G.2.1
\\ ( https://arxiv.org/abs/2007.03986 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:1906.07994 (*cross-listing*)
replaced with revised version Wed, 26 Aug 2020 00:11:06 GMT   (532kb,D)

Title: OpenSurgery for Topological Assemblies
Authors: Alexandru Paler, Austin G. Fowler
Categories: quant-ph cs.ET
Comments: 4 pages including more details
\\ ( https://arxiv.org/abs/1906.07994 ,  532kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
