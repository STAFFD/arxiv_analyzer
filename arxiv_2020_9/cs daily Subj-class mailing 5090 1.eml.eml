Delivered-To: brucelu2013@gmail.com
Received: by 2002:a67:e3b6:0:0:0:0:0 with SMTP id j22csp378467vsm;
        Wed, 24 Jun 2020 02:40:54 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJzGucRz7U5AsARcpOIx1wcMJ/PZ82F49QkeKTmn8/1wmRkR56qxn4LfEf4klP+3gKRAgrcE
X-Received: by 2002:ac8:23e3:: with SMTP id r32mr26195719qtr.268.1592991653732;
        Wed, 24 Jun 2020 02:40:53 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1592991653; cv=none;
        d=google.com; s=arc-20160816;
        b=MnhcIHwSs2C17TfOGbooPpHP17zQ2TuaeDVet+yXlS0ODYktuqqdgJWgf+9jeWem/t
         UXB61v6dTFAwMSc3fQkl89fTR7f8X5EaKQ2LnzJzMLTTdRXRQseb1ZSjK7WktEXb3Jqz
         8jd7v9sn5boetw1N5pcMi0oQ5DVoK1EPQIaPX7tUwdd50OvBOfd+okXPn2R/pNsGHdlw
         ntHH8CcmuQMTA7/be0mL/+La74smi0b2Qk/9j9CUY3VujaQOwWRBLSbnTzidJlR8R2VF
         YTpplqjxIyNthafHwiosm7s1LCmMwn/mfn3004qOxBUh0NCCHbJe5fcZBcbzNXLdmvD6
         xc2g==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=XOgLSG9dEWFKOi9b6/A3i+m4SJA9Bxj8v5l3MiJ3Lac=;
        b=gfpLReg6m2Gl2B54e6G1DHt75/xnmmT53TfToINsjAf5kmTCKll5HsEdo3IVVQ8Xj1
         Odn+2u2kVq0luZQLMmHtTHBBEfPGzheHFt58JOSmYqzEjJZUu2FJT3orAh/nWFFSSRzQ
         ixlnelVR2DVyH0Ae1pUTtZuQh/2IBG9tQvISZpGYuflb2quQbUc2rHdtZtUrGLNAjdzR
         YOjPOmxXneh/Qf/jJwQ4ydO0uYpfjUUVCm/Uyq/oW/yj8aBrK/JY1kCQzxX+YQGf7oDG
         zvp1JAd+Vt8v4/JT7gc/Haze/O/VY3rnFSzzvjfyz0Q8wKgHBvOrwgNXVhDpMbtoi99N
         9qYQ==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id w126si11451007qka.63.2020.06.24.02.40.53
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 24 Jun 2020 02:40:53 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 05O9eqgK063814;
	Wed, 24 Jun 2020 05:40:52 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 05O9eqVK016471;
	Wed, 24 Jun 2020 05:40:52 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 05O9eqQA016469;
	Wed, 24 Jun 2020 05:40:52 -0400
Date: Wed, 24 Jun 2020 05:40:52 -0400
Message-Id: <202006240940.05O9eqQA016469@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 5090 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Graphics
 received from  Mon 22 Jun 20 18:00:00 GMT  to  Tue 23 Jun 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2006.12556
Date: Thu, 18 Jun 2020 10:51:04 GMT   (529kb)

Title: Frost filtered scale-invariant feature extraction and multilayer
  perceptron for hyperspectral image classification
Authors: G.Kalaiarasi, S.Maheswari
Categories: cs.CV eess.IV
\\
  Hyperspectral image (HSI) classification plays a significant in the field of
remote sensing due to its ability to provide spatial and spectral information.
Due to the rapid development and increasing of hyperspectral remote sensing
technology, many methods have been developed for HSI classification but still a
lack of achieving the better performance. A Frost Filtered Scale-Invariant
Feature Transformation based MultiLayer Perceptron Classification (FFSIFT-MLPC)
technique is introduced for classifying the hyperspectral image with higher
accuracy and minimum time consumption. The FFSIFT-MLPC technique performs three
major processes, namely preprocessing, feature extraction and classification
using multiple layers. Initially, the hyperspectral image is divided into
number of spectral bands. These bands are given as input in the input layer of
perceptron. Then the Frost filter is used in FFSIFT-MLPC technique for
preprocessing the input bands which helps to remove the noise from
hyper-spectral image at the first hidden layer. After preprocessing task,
texture, color and object features of hyper-spectral image are extracted at
second hidden layer using Gaussian distributive scale-invariant feature
transform. At the third hidden layer, Euclidean distance is measured between
the extracted features and testing features. Finally, feature matching is
carried out at the output layer for hyper-spectral image classification. The
classified outputs are resulted in terms of spectral bands (i.e., different
colors). Experimental analysis is performed with PSNR, classification accuracy,
false positive rate and classification time with number of spectral bands. The
results evident that presented FFSIFT-MLPC technique improves the hyperspectral
image classification accuracy, PSNR and minimizes false positive rate as well
as classification time than the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2006.12556 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12567
Date: Mon, 22 Jun 2020 19:01:21 GMT   (2547kb,D)

Title: A Survey on Deep Learning for Localization and Mapping: Towards the Age
  of Spatial Machine Intelligence
Authors: Changhao Chen, Bing Wang, Chris Xiaoxuan Lu, Niki Trigoni, Andrew
  Markham
Categories: cs.CV cs.LG cs.RO eess.IV
Comments: 26 pages, 10 figures. Project website:
  https://github.com/changhao-chen/deep-learning-localization-mapping
\\
  Deep learning based localization and mapping has recently attracted great
attentions. Instead of crating hand-designed algorithms via exploiting physical
models or geometry theory, deep learning based solutions provide an alternative
to solve the problem in a data-driven way. Benefited from the ever-increasing
amount of data and computational power, these methods are fast evolving into a
new area that offers accurate and robust systems to track motion and estimate
scene structure for real-world applications. In this work, we provide a
comprehensive survey, and propose a new taxonomy on the existing approaches on
localization and mapping using deep learning. We also discuss the limitations
of current models, and indicate possible future directions. A wide range of
topics are covered, from learning odometry estimation, mapping, to global
localization and simultaneous localization and mapping (SLAM). We revisit the
problem of perceiving self-motion and scene with on-board sensors, and show how
to solve it by integrating these modules into a prospective spatial machine
intelligence system (SMIS). It is our hope that this work can connect the
emerging works from robotics, computer vision and machine learning communities,
and serve as a guide for future researchers to know about the possible ways
that apply deep learning to tackle the localization and mapping problems.
\\ ( https://arxiv.org/abs/2006.12567 ,  2547kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12575
Date: Mon, 22 Jun 2020 19:20:35 GMT   (3646kb,D)

Title: LAMP: Large Deep Nets with Automated Model Parallelism for Image
  Segmentation
Authors: Wentao Zhu, Can Zhao, Wenqi Li, Holger Roth, Ziyue Xu, Daguang Xu
Categories: cs.CV cs.AI cs.DC cs.LG cs.NE eess.IV
Comments: MICCAI 2020 Early Accepted paper. Code is
  available\footnote{https://github.com/wentaozhu/lamp-automated-model-parallelism}
\\
  Deep Learning (DL) models are becoming larger, because the increase in model
size might offer significant accuracy gain. To enable the training of large
deep networks, data parallelism and model parallelism are two well-known
approaches for parallel training. However, data parallelism does not help
reduce memory footprint per device. In this work, we introduce Large deep 3D
ConvNets with Automated Model Parallelism (LAMP) and investigate the impact of
both input's and deep 3D ConvNets' size on segmentation accuracy. Through
automated model parallelism, it is feasible to train large deep 3D ConvNets
with a large input patch, even the whole image. Extensive experiments
demonstrate that, facilitated by the automated model parallelism, the
segmentation accuracy can be improved through increasing model size and input
context size, and large input yields significant inference speedup compared
with sliding window of small patches in the inference. Code is
available\footnote{https://github.com/wentaozhu/lamp-automated-model-parallelism}.
\\ ( https://arxiv.org/abs/2006.12575 ,  3646kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12582
Date: Mon, 22 Jun 2020 19:46:41 GMT   (216kb)

Title: Laplacian Mixture Model Point Based Registration
Authors: Mohammad Sadegh Majdi, Emad Fatemizadeh
Categories: cs.CV
Journal-ref: 2015 9th Iranian Conference on Machine Vision and Image Processing
  (MVIP), Tehran, 2015, pp. 57-60
DOI: 10.1109/IranianMVIP.2015.7397504
\\
  Point base registration is an important part in many machine VISIOn
applications, medical diagnostics, agricultural studies etc. The goal of point
set registration is to find correspondences between different data sets and
estimate the appropriate transformation that can map one set to another. Here
we introduce a novel method for matching of different data sets based on
Laplacian distribution. We consider the alignment of two point sets as
probability density estimation problem. By using maximum likelihood methods we
try to fit the Laplacian mixture model (LMM) centroids (source point set) to
the data point set.
\\ ( https://arxiv.org/abs/2006.12582 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12586
Date: Mon, 22 Jun 2020 19:54:53 GMT   (1483kb,D)

Title: Drive-Net: Convolutional Network for Driver Distraction Detection
Authors: Mohammed S. Majdi, Sundaresh Ram, Jonathan T. Gill, Jeffery J.
  Rodriguez
Categories: cs.CV
Journal-ref: 2018 IEEE Southwest Symposium on Image Analysis and Interpretation
  (SSIAI), Las Vegas, NV, 2018, pp. 1-4,
DOI: 10.1109/SSIAI.2018.8470309
\\
  To help prevent motor vehicle accidents, there has been significant interest
in finding an automated method to recognize signs of driver distraction, such
as talking to passengers, fixing hair and makeup, eating and drinking, and
using a mobile phone. In this paper, we present an automated supervised
learning method called Drive-Net for driver distraction detection. Drive-Net
uses a combination of a convolutional neural network (CNN) and a random
decision forest for classifying images of a driver. We compare the performance
of our proposed Drive-Net to two other popular machine-learning approaches: a
recurrent neural network (RNN), and a multi-layer perceptron (MLP). We test the
methods on a publicly available database of images acquired under a controlled
environment containing about 22425 images manually annotated by an expert.
Results show that Drive-Net achieves a detection accuracy of 95%, which is 2%
more than the best results obtained on the same database using other methods
\\ ( https://arxiv.org/abs/2006.12586 ,  1483kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12634
Date: Mon, 22 Jun 2020 21:39:56 GMT   (5606kb,D)

Title: RP2K: A Large-Scale Retail Product Dataset forFine-Grained Image
  Classification
Authors: Jingtian Peng, Chang Xiao, Xun Wei, Yifan Li
Categories: cs.CV
\\
  We introduce RP2K, a new large-scale retail product dataset for fine-grained
image classification. Unlike previous datasets focusing on relatively few
products, we collect more than 500,000 images of retail products on shelves
belonging to 2000 different products. Our dataset aims to advance the research
in retail object recognition, which has massive applications such as automatic
shelf auditing and image-based product information retrieval. Our dataset
enjoys following properties: (1) It is by far the largest scale dataset in
terms of product categories. (2) All images are captured manually in physical
retail stores with natural lightings, matching the scenario of real
applications. (3) We provide rich annotations to each object, including the
sizes, shapes and flavors/scents. We believe our dataset could benefit both
computer vision research and retail industry.
\\ ( https://arxiv.org/abs/2006.12634 ,  5606kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12671
Date: Tue, 23 Jun 2020 00:15:07 GMT   (9195kb,D)

Title: AFDet: Anchor Free One Stage 3D Object Detection
Authors: Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li
  Huang, Yuan Li
Categories: cs.CV
Comments: Accepted on May 6th, 2020 by CVPRW 2020, published on June 7th, 2020;
  Baseline detector for the 1st place solutions of Waymo Open Dataset
  Challenges 2020
\\
  High-efficiency point cloud 3D object detection operated on embedded systems
is important for many robotics applications including autonomous driving. Most
previous works try to solve it using anchor-based detection methods which come
with two drawbacks: post-processing is relatively complex and computationally
expensive; tuning anchor parameters is tricky. We are the first to address
these drawbacks with an anchor free and Non-Maximum Suppression free one stage
detector called AFDet. The entire AFDet can be processed efficiently on a CNN
accelerator or a GPU with the simplified post-processing. Without bells and
whistles, our proposed AFDet performs competitively with other one stage
anchor-based methods on KITTI validation set and Waymo Open Dataset validation
set.
\\ ( https://arxiv.org/abs/2006.12671 ,  9195kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12681
Date: Tue, 23 Jun 2020 00:49:05 GMT   (8807kb,D)

Title: Contrastive Generative Adversarial Networks
Authors: Minguk Kang and Jaesik Park
Categories: cs.CV cs.LG
Comments: 23 pages, 13 figures
\\
  Conditional image synthesis is the task to generate high-fidelity diverse
images using class label information. Although many studies have shown
realistic results, there is room for improvement if the number of classes
increases. In this paper, we propose a novel conditional contrastive loss to
maximize a lower bound on mutual information between samples from the same
class. Our framework, called Contrastive Generative Adversarial Networks
(ContraGAN), learns to synthesize images using class information and
data-to-data relations of training examples. The discriminator in ContraGAN
discriminates the authenticity of given samples and maximizes the mutual
information between embeddings of real images from the same class.
Simultaneously, the generator attempts to synthesize images to fool the
discriminator and to maximize the mutual information of fake images from the
same class prior. The experimental results show that ContraGAN is robust to
network architecture selection and outperforms state-of-the-art-models by 3.7%
and 11.2% on CIFAR10 and Tiny ImageNet datasets, respectively, without any data
augmentation. For the fair comparison, we re-implement the nine
state-of-the-art-approaches to test various methods under the same condition.
The software package that can re-produce all experiments is available at
https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.
\\ ( https://arxiv.org/abs/2006.12681 ,  8807kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12706
Date: Tue, 23 Jun 2020 02:54:55 GMT   (40602kb,D)

Title: Deep Learning of Unified Region, Edge, and Contour Models for Automated
  Image Segmentation
Authors: Ali Hatamizadeh
Categories: cs.CV
Comments: PhD dissertation, UCLA, 2020
\\
  Image segmentation is a fundamental and challenging problem in computer
vision with applications spanning multiple areas, such as medical imaging,
remote sensing, and autonomous vehicles. Recently, convolutional neural
networks (CNNs) have gained traction in the design of automated segmentation
pipelines. Although CNN-based models are adept at learning abstract features
from raw image data, their performance is dependent on the availability and
size of suitable training datasets. Additionally, these models are often unable
to capture the details of object boundaries and generalize poorly to unseen
classes. In this thesis, we devise novel methodologies that address these
issues and establish robust representation learning frameworks for
fully-automatic semantic segmentation in medical imaging and mainstream
computer vision. In particular, our contributions include (1) state-of-the-art
2D and 3D image segmentation networks for computer vision and medical image
analysis, (2) an end-to-end trainable image segmentation framework that unifies
CNNs and active contour models with learnable parameters for fast and robust
object delineation, (3) a novel approach for disentangling edge and texture
processing in segmentation networks, and (4) a novel few-shot learning model in
both supervised settings and semi-supervised settings where synergies between
latent and image spaces are leveraged to learn to segment images given limited
training data.
\\ ( https://arxiv.org/abs/2006.12706 ,  40602kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12708
Date: Tue, 23 Jun 2020 02:57:29 GMT   (893kb,D)

Title: iffDetector: Inference-aware Feature Filtering for Object Detection
Authors: Mingyuan Mao, Yuxin Tian, Baochang Zhang, Qixiang Ye, Wanquan Liu,
  Guodong Guo, David Doermann
Categories: cs.CV
Comments: 14 pages, 6 figures
\\
  Modern CNN-based object detectors focus on feature configuration during
training but often ignore feature optimization during inference. In this paper,
we propose a new feature optimization approach to enhance features and suppress
background noise in both the training and inference stages. We introduce a
generic Inference-aware Feature Filtering (IFF) module that can easily be
combined with modern detectors, resulting in our iffDetector. Unlike
conventional open-loop feature calculation approaches without feedback, the IFF
module performs closed-loop optimization by leveraging high-level semantics to
enhance the convolutional features. By applying Fourier transform analysis, we
demonstrate that the IFF module acts as a negative feedback that theoretically
guarantees the stability of feature learning. IFF can be fused with CNN-based
object detectors in a plug-and-play manner with negligible computational cost
overhead. Experiments on the PASCAL VOC and MS COCO datasets demonstrate that
our iffDetector consistently outperforms state-of-the-art methods by
significant margins\footnote{The test code and model are anonymously available
in https://github.com/anonymous2020new/iffDetector }.
\\ ( https://arxiv.org/abs/2006.12708 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12709
Date: Tue, 23 Jun 2020 02:59:11 GMT   (9357kb,D)

Title: CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks
Authors: Mahmoud Afifi, Abdelrahman Abdelhamed, Abdullah Abuolaim, Abhijith
  Punnappurath, and Michael S. Brown
Categories: cs.CV eess.IV
\\
  Cameras currently allow access to two image states: (i) a minimally processed
linear raw-RGB image state (i.e., raw sensor data) or (ii) a highly-processed
nonlinear image state (e.g., sRGB). There are many computer vision tasks that
work best with a linear image state, such as image deblurring and image
dehazing. Unfortunately, the vast majority of images are saved in the nonlinear
image state. Because of this, a number of methods have been proposed to
"unprocess" nonlinear images back to a raw-RGB state. However, existing
unprocessing methods have a drawback because raw-RGB images are
sensor-specific. As a result, it is necessary to know which camera produced the
sRGB output and use a method or network tailored for that sensor to properly
unprocess it. This paper addresses this limitation by exploiting another camera
image state that is not available as an output, but it is available inside the
camera pipeline. In particular, cameras apply a colorimetric conversion step to
convert the raw-RGB image to a device-independent space based on the CIE XYZ
color space before they apply the nonlinear photo-finishing. Leveraging this
canonical image state, we propose a deep learning framework, CIE XYZ Net, that
can unprocess a nonlinear image back to the canonical CIE XYZ image. This image
can then be processed by any low-level computer vision operator and re-rendered
back to the nonlinear image. We demonstrate the usefulness of the CIE XYZ Net
on several low-level vision tasks and show significant gains that can be
obtained by this processing framework. Code and dataset are publicly available
at https://github.com/mahmoudnafifi/CIE_XYZ_NET.
\\ ( https://arxiv.org/abs/2006.12709 ,  9357kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12712
Date: Tue, 23 Jun 2020 03:15:04 GMT   (3312kb,D)

Title: PoseGAN: A Pose-to-Image Translation Framework for Camera Localization
Authors: Kanglin Liu and Qing Li and Guoping Qiu
Categories: cs.CV cs.RO
\\
  Camera localization is a fundamental requirement in robotics and computer
vision. This paper introduces a pose-to-image translation framework to tackle
the camera localization problem. We present PoseGANs, a conditional generative
adversarial networks (cGANs) based framework for the implementation of
pose-to-image translation. PoseGANs feature a number of innovations including a
distance metric based conditional discriminator to conduct camera localization
and a pose estimation technique for generated camera images as a stronger
constraint to improve camera localization performance. Compared with
learning-based regression methods such as PoseNet, PoseGANs can achieve better
performance with model sizes that are 70% smaller. In addition, PoseGANs
introduce the view synthesis technique to establish the correspondence between
the 2D images and the scene, \textit{i.e.}, given a pose, PoseGANs are able to
synthesize its corresponding camera images. Furthermore, we demonstrate that
PoseGANs differ in principle from structure-based localization and
learning-based regressions for camera localization, and show that PoseGANs
exploit the geometric structures to accomplish the camera localization task,
and is therefore more stable than and superior to learning-based regressions
which rely on local texture features instead. In addition to camera
localization and view synthesis, we also demonstrate that PoseGANs can be
successfully used for other interesting applications such as moving object
elimination and frame interpolation in video sequences.
\\ ( https://arxiv.org/abs/2006.12712 ,  3312kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12761
Date: Tue, 23 Jun 2020 05:02:11 GMT   (5724kb,AD)

Title: Benchmarking features from different radiomics toolkits / toolboxes
  using Image Biomarkers Standardization Initiative
Authors: Mingxi Lei, Bino Varghese, Darryl Hwang, Steven Cen, Xiaomeng Lei,
  Afshin Azadikhah, Bhushan Desai, Assad Oberai, Vinay Duddalwar
Categories: cs.CV eess.IV
Comments: 21 pages, 8 figures
\\
  There is no consensus regarding the radiomic feature terminology, the
underlying mathematics, or their implementation. This creates a scenario where
features extracted using different toolboxes could not be used to build or
validate the same model leading to a non-generalization of radiomic results. In
this study, the image biomarker standardization initiative (IBSI) established
phantom and benchmark values were used to compare the variation of the radiomic
features while using 6 publicly available software programs and 1 in-house
radiomics pipeline. All IBSI-standardized features (11 classes, 173 in total)
were extracted. The relative differences between the extracted feature values
from the different software and the IBSI benchmark values were calculated to
measure the inter-software agreement. To better understand the variations,
features are further grouped into 3 categories according to their properties:
1) morphology, 2) statistic/histogram and 3)texture features. While a good
agreement was observed for a majority of radiomics features across the various
programs, relatively poor agreement was observed for morphology features.
Significant differences were also found in programs that use different gray
level discretization approaches. Since these programs do not include all IBSI
features, the level of quantitative assessment for each category was analyzed
using Venn and the UpSet diagrams and also quantified using two ad hoc metrics.
Morphology features earns lowest scores for both metrics, indicating that
morphological features are not consistently evaluated among software programs.
We conclude that radiomic features calculated using different software programs
may not be identical and reliable. Further studies are needed to standardize
the workflow of radiomic feature extraction.
\\ ( https://arxiv.org/abs/2006.12761 ,  5724kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12770
Date: Tue, 23 Jun 2020 05:33:54 GMT   (4130kb,D)

Title: Discriminative Feature Alignment: ImprovingTransferability of
  Unsupervised DomainAdaptation by Gaussian-guided LatentAlignment
Authors: Jing Wang, Jiahong Chen, Jianzhe Lin, Leonid Sigal, and Clarence W. de
  Silva
Categories: cs.CV
Comments: 12 pages, 12 figures
\\
  In this study, we focus on the unsupervised domain adaptation problem where
an approximate inference model is to be learned from a labeled data domain and
expected to generalize well to an unlabeled data domain. The success of
unsupervised domain adaptation largely relies on the cross-domain feature
alignment. Previous work has attempted to directly align latent features by the
classifier-induced discrepancies. Nevertheless, a common feature space cannot
always be learned via this direct feature alignment especially when a large
domain gap exists. To solve this problem, we introduce a Gaussian-guided latent
alignment approach to align the latent feature distributions of the two domains
under the guidance of the prior distribution. In such an indirect way, the
distributions over the samples from the two domains will be constructed on a
common feature space, i.e., the space of the prior, which promotes better
feature alignment. To effectively align the target latent distribution with
this prior distribution, we also propose a novel unpaired L1-distance by taking
advantage of the formulation of the encoder-decoder. The extensive evaluations
on eight benchmark datasets validate the superior knowledge transferability
through outperforming state-of-the-art methods and the versatility of the
proposed method by improving the existing work significantly.
\\ ( https://arxiv.org/abs/2006.12770 ,  4130kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12774
Date: Tue, 23 Jun 2020 05:38:47 GMT   (6955kb,D)

Title: Surpassing Real-World Source Training Data: Random 3D Characters for
  Generalizable Person Re-Identification
Authors: Yanan Wang, Shengcai Liao, Ling Shao
Categories: cs.CV
Comments: https://github.com/VideoObjectSearch/RandPerson
\\
  Person re-identification has seen significant advancement in recent years.
However, the ability of learned models to generalize to unknown target domains
still remains limited. One possible reason for this is the lack of large-scale
and diverse source training data, since manually labeling such a dataset is
very expensive. To address this, we propose to automatically synthesize a
large-scale person re-identification dataset following a set-up similar to real
surveillance but with virtual environments, and then use the synthesized person
images to train a generalizable person re-identification model. Specifically,
we design a method to generate a large number of random UV texture maps and use
them to create different 3D clothing models. Then, an automatic code is
developed to randomly generate various different 3D characters with diverse
clothes, races and attributes. Next, we simulate a number of different virtual
environments using Unity3D, with customized camera networks similar to real
surveillance systems, and import multiple 3D characters at the same time, with
various movements and interactions along different paths through the camera
networks. As a result, we obtain a virtual dataset, called RandPerson, with
1,756,759 person images of 8,000 identities. By training person
re-identification models on these synthesized person images, we demonstrate,
for the first time, that models trained on virtual data can generalize well to
unseen target images, surpassing the models trained on various real-world
datasets, including CUHK03, Market-1501, DukeMTMC-reID, and MSMT17. The
RandPerson dataset will be released at
https://github.com/VideoObjectSearch/RandPerson.
\\ ( https://arxiv.org/abs/2006.12774 ,  6955kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12791
Date: Tue, 23 Jun 2020 07:01:32 GMT   (6113kb,D)

Title: Increased-Range Unsupervised Monocular Depth Estimation
Authors: Saad Imran, Muhammad Umar Karim Khan, Sikander Bin Mukarram, Chong-Min
  Kyung
Categories: cs.CV
\\
  Unsupervised deep learning methods have shown promising performance for
single-image depth estimation. Since most of these methods use binocular stereo
pairs for self-supervision, the depth range is generally limited.
Small-baseline stereo pairs provide small depth range but handle occlusions
well. On the other hand, stereo images acquired with a wide-baseline rig cause
occlusions-related errors in the near range but estimate depth well in the far
range. In this work, we propose to integrate the advantages of the small and
wide baselines. By training the network using three horizontally aligned views,
we obtain accurate depth predictions for both close and far ranges. Our
strategy allows to infer multi-baseline depth from a single image. This is
unlike previous multi-baseline systems which employ more than two cameras. The
qualitative and quantitative results show the superior performance of
multi-baseline approach over previous stereo-based monocular methods. For 0.1
to 80 meters depth range, our approach decreases the absolute relative error of
depth by 24% compared to Monodepth2. Our approach provides 21 frames per second
on a single Nvidia1080 GPU, making it useful for practical applications.
\\ ( https://arxiv.org/abs/2006.12791 ,  6113kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12797
Date: Tue, 23 Jun 2020 07:12:00 GMT   (4754kb,D)

Title: MSMD-Net: Deep Stereo Matching with Multi-scale and Multi-dimension Cost
  Volume
Authors: Zhelun Shen, Yuchao Dai, Zhibo Rao
Categories: cs.CV
\\
  Deep end-to-end learning based stereo matching methods have achieved great
success as witnessed by the leaderboards across different benchmarking datasets
(KITTI, Middlebury, ETH3D, etc), where the cost volume representation is an
indispensable step to the success. However, most existing work only employs a
single cost volume, which cannot fully exploit the multi-scale cues in stereo
matching and provide guidance for disparity refinement. What's more, the single
cost volume representation also limits the disparity range and the resolution
of the disparity estimation. In this paper, we propose MSMD-Net (Multi-Scale
and Multi-Dimension) to construct multi-scale and multi-dimension cost volume.
At the multi-scale level, we generate four 4D combination volumes at different
scales and integrate them in 3D cost aggregation to predict an initial
disparity estimation. At the multi-dimension level, we construct a 3D warped
correlation volume and use it to refine the initial disparity map with residual
learning. These two dimensional cost volumes are complementary to each other
and can boost the performance of disparity estimation. Additionally, we propose
a switch training strategy to further improve the accuracy of disparity
estimation, where we switch two kinds of different activation functions to
alleviate the overfitting issue in the pre-training process. Our proposed
method was evaluated on several benchmark datasets and ranked first on KITTI
2012 leaderboard and second on KITTI 2015 leaderboard as of June 23.The code of
MSMD-Net is available at https://github.com/gallenszl/MSMD-Net.
\\ ( https://arxiv.org/abs/2006.12797 ,  4754kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12813
Date: Tue, 23 Jun 2020 08:14:02 GMT   (3618kb,D)

Title: NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep
  Neural Networks
Authors: Eugene Lee and Chen-Yi Lee
Categories: cs.CV cs.LG cs.NE
Comments: 17 pages, 11 figures, accepted by CVPR as oral paper
Journal-ref: In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (pp. 1478-1487) 2020
\\
  Deciding the amount of neurons during the design of a deep neural network to
maximize performance is not intuitive. In this work, we attempt to search for
the neuron (filter) configuration of a fixed network architecture that
maximizes accuracy. Using iterative pruning methods as a proxy, we parameterize
the change of the neuron (filter) number of each layer with respect to the
change in parameters, allowing us to efficiently scale an architecture across
arbitrary sizes. We also introduce architecture descent which iteratively
refines the parameterized function used for model scaling. The combination of
both proposed methods is coined as NeuralScale. To prove the efficiency of
NeuralScale in terms of parameters, we show empirical simulations on VGG11,
MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark
datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41%
for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet
respectively under a parameter-constrained setting (output neurons (filters) of
default configuration with scaling factor of 0.25).
\\ ( https://arxiv.org/abs/2006.12813 ,  3618kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12864
Date: Tue, 23 Jun 2020 09:55:40 GMT   (188kb)

Title: Object recognition through pose and shape estimation
Authors: Anitta D, Annis Fathima A
Categories: cs.CV
Report-no: 20182959
Journal-ref: Journal of advanced research in dynamic and control systems 2018
\\
  Computer vision helps machines or computer to see like humans. Computer Takes
information from the images and then understands of useful information from
images. Gesture recognition and movement recognition are the current area of
research in computer vision. For both gesture and movement recognition finding
pose of an object is of great importance. The purpose of this paper is to
review many state of art which is already available for finding the pose of
object based on shape, based on appearance, based on feature and comparison for
its accuracy, complexity and performance
\\ ( https://arxiv.org/abs/2006.12864 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12874
Date: Tue, 23 Jun 2020 10:12:08 GMT   (3520kb)

Title: Non-parametric spatially constrained local prior for scene parsing on
  real-world data
Authors: Ligang Zhang
Categories: cs.CV cs.LG eess.IV
Comments: 10 pages, journal
MSC-class: 68U10
Journal-ref: Engineering Applications of Artificial Intelligence,Volume
  93,2020,103708
\\
  Scene parsing aims to recognize the object category of every pixel in scene
images, and it plays a central role in image content understanding and computer
vision applications. However, accurate scene parsing from unconstrained
real-world data is still a challenging task. In this paper, we present the
non-parametric Spatially Constrained Local Prior (SCLP) for scene parsing on
realistic data. For a given query image, the non-parametric SCLP is learnt by
first retrieving a subset of most similar training images to the query image
and then collecting prior information about object co-occurrence statistics
between spatial image blocks and between adjacent superpixels from the
retrieved subset. The SCLP is powerful in capturing both long- and short-range
context about inter-object correlations in the query image and can be
effectively integrated with traditional visual features to refine the
classification results. Our experiments on the SIFT Flow and PASCAL-Context
benchmark datasets show that the non-parametric SCLP used in conjunction with
superpixel-level visual features achieves one of the top performance compared
with state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2006.12874 ,  3520kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12884
Date: Tue, 23 Jun 2020 10:24:13 GMT   (8202kb,D)

Title: SLV: Spatial Likelihood Voting for Weakly Supervised Object Detection
Authors: Ze Chen, Zhihang Fu, Rongxin Jiang, Yaowu Chen, Xian-sheng Hua
Categories: cs.CV
Comments: Accepted by CVPR2020
\\
  Based on the framework of multiple instance learning (MIL), tremendous works
have promoted the advances of weakly supervised object detection (WSOD).
However, most MIL-based methods tend to localize instances to their
discriminative parts instead of the whole content. In this paper, we propose a
spatial likelihood voting (SLV) module to converge the proposal localizing
process without any bounding box annotations. Specifically, all region
proposals in a given image play the role of voters every iteration during
training, voting for the likelihood of each category in spatial dimensions.
After dilating alignment on the area with large likelihood values, the voting
results are regularized as bounding boxes, being used for the final
classification and localization. Based on SLV, we further propose an end-to-end
training framework for multi-task learning. The classification and localization
tasks promote each other, which further improves the detection performance.
Extensive experiments on the PASCAL VOC 2007 and 2012 datasets demonstrate the
superior performance of SLV.
\\ ( https://arxiv.org/abs/2006.12884 ,  8202kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12890
Date: Tue, 23 Jun 2020 10:51:26 GMT   (1299kb,D)

Title: Scribble2Label: Scribble-Supervised Cell Segmentation via
  Self-Generating Pseudo-Labels with Consistency
Authors: Hyeonsoo Lee, Won-Ki Jeong
Categories: cs.CV
Comments: MICCAI 2020 accepted
\\
  Segmentation is a fundamental process in microscopic cell image analysis.
With the advent of recent advances in deep learning, more accurate and
high-throughput cell segmentation has become feasible. However, most existing
deep learning-based cell segmentation algorithms require fully annotated
ground-truth cell labels, which are time-consuming and labor-intensive to
generate. In this paper, we introduce Scribble2Label, a novel weakly-supervised
cell segmentation framework that exploits only a handful of scribble
annotations without full segmentation labels. The core idea is to combine
pseudo-labeling and label filtering to generate reliable labels from weak
supervision. For this, we leverage the consistency of predictions by
iteratively averaging the predictions to improve pseudo labels. We demonstrate
the performance of Scribble2Label by comparing it to several state-of-the-art
cell segmentation methods with various cell image modalities, including
bright-field, fluorescence, and electron microscopy. We also show that our
method performs robustly across different levels of scribble details, which
confirms that only a few scribble annotations are required in real-use cases.
\\ ( https://arxiv.org/abs/2006.12890 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12906
Date: Tue, 23 Jun 2020 11:25:16 GMT   (4827kb,D)

Title: Probabilistic Crowd GAN: Multimodal Pedestrian Trajectory Prediction
  using a Graph Vehicle-Pedestrian Attention Network
Authors: Stuart Eiffert, Kunming Li, Mao Shan, Stewart Worrall, Salah Sukkarieh
  and Eduardo Nebot
Categories: cs.CV cs.RO
Comments: Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L) Copyright may be transferred without notice, after which this version
  may no longer be accessible
DOI: 10.1109/LRA.2020.3004324
\\
  Understanding and predicting the intention of pedestrians is essential to
enable autonomous vehicles and mobile robots to navigate crowds. This problem
becomes increasingly complex when we consider the uncertainty and multimodality
of pedestrian motion, as well as the implicit interactions between members of a
crowd, including any response to a vehicle. Our approach, Probabilistic Crowd
GAN, extends recent work in trajectory prediction, combining Recurrent Neural
Networks (RNNs) with Mixture Density Networks (MDNs) to output probabilistic
multimodal predictions, from which likely modal paths are found and used for
adversarial training. We also propose the use of Graph Vehicle-Pedestrian
Attention Network (GVAT), which models social interactions and allows input of
a shared vehicle feature, showing that inclusion of this module leads to
improved trajectory prediction both with and without the presence of a vehicle.
Through evaluation on various datasets, we demonstrate improvements on the
existing state of the art methods for trajectory prediction and illustrate how
the true multimodal and uncertain nature of crowd interactions can be directly
modelled.
\\ ( https://arxiv.org/abs/2006.12906 ,  4827kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12963
Date: Tue, 23 Jun 2020 13:03:21 GMT   (3082kb,D)

Title: PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural
  Networks Acceleration
Authors: Jianrong Xu, Chao Li, Bifeng Cui, Kang Yang, Yongjun Xu
Categories: cs.CV
\\
  The existence of a lot of redundant information in convolutional neural
networks leads to the slow deployment of its equipment on the edge. To solve
this issue, we proposed a novel deep learning model compression acceleration
method based on data distribution characteristics, namely Pruning Filter via
Gaussian Distribution Feature(PFGDF) which was to found the smaller interval of
the convolution layer of a certain layer to describe the original on the
grounds of distribution characteristics . Compared with revious advanced
methods, PFGDF compressed the model by filters with insignificance in
distribution regardless of the contribution and sensitivity information of the
convolution filter. The pruning process of the model was automated, and always
ensured that the compressed model could restore the performance of original
model. Notably, on CIFAR-10, PFGDF compressed the convolution filter on VGG-16
by 66:62%, the parameter reducing more than 90%, and FLOPs achieved 70:27%. On
ResNet-32, PFGDF reduced the convolution filter by 21:92%. The parameter was
reduced to 54:64%, and the FLOPs exceeded 42%
\\ ( https://arxiv.org/abs/2006.12963 ,  3082kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12986
Date: Sun, 21 Jun 2020 10:03:34 GMT   (5456kb,D)

Title: FNA++: Fast Network Adaptation via Parameter Remapping and Architecture
  Search
Authors: Jiemin Fang, Yuzhu Sun, Qian Zhang, Kangjian Peng, Yuan Li, Wenyu Liu,
  Xinggang Wang
Categories: cs.CV
Comments: arXiv admin note: substantial text overlap with arXiv:2001.02525
\\
  Deep neural networks achieve remarkable performance in many computer vision
tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection
approaches reuse neural network architectures designed for image classification
as the backbone, commonly pre-trained on ImageNet. However, performance gains
can be achieved by designing network architectures specifically for detection
and segmentation, as shown by recent neural architecture search (NAS) research
for detection and segmentation. One major challenge though is that ImageNet
pre-training of the search space representation (a.k.a. super network) or the
searched networks incurs huge computational cost. In this paper, we propose a
Fast Network Adaptation (FNA++) method, which can adapt both the architecture
and parameters of a seed network (e.g. an ImageNet pre-trained network) to
become a network with different depths, widths, or kernel sizes via a parameter
remapping technique, making it possible to use NAS for segmentation/detection
tasks a lot more efficiently. In our experiments, we conduct FNA++ on
MobileNetV2 to obtain new networks for semantic segmentation, object detection,
and human pose estimation that clearly outperform existing networks designed
both manually and by NAS. We also implement FNA++ on ResNets and NAS networks,
which demonstrates a great generalization ability. The total computation cost
of FNA++ is significantly less than SOTA segmentation/detection NAS approaches:
1737x less than DPC, 6.8x less than Auto-DeepLab, and 8.0x less than DetNAS.
The code will be released at https://github.com/JaminFong/FNA.
\\ ( https://arxiv.org/abs/2006.12986 ,  5456kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12987
Date: Sat, 20 Jun 2020 01:47:53 GMT   (1503kb)

Title: Exemplar Loss for Siamese Network in Visual Tracking
Authors: Shuo Chang, YiFan Zhang, Sai Huang, Yuanyuan Yao and Zhiyong Feng
Categories: cs.CV
\\
  Visual tracking plays an important role in perception system, which is a
crucial part of intelligent transportation. Recently, Siamese network is a hot
topic for visual tracking to estimate moving targets' trajectory, due to its
superior accuracy and simple framework. In general, Siamese tracking
algorithms, supervised by logistic loss and triplet loss, increase the value of
inner product between exemplar template and positive sample while reduce the
value of inner product with background sample. However, the distractors from
different exemplars are not considered by mentioned loss functions, which limit
the feature models' discrimination. In this paper, a new exemplar loss
integrated with logistic loss is proposed to enhance the feature model's
discrimination by reducing inner products among exemplars. Without the bells
and whistles, the proposed algorithm outperforms the methods supervised by
logistic loss or triplet loss. Numerical results suggest that the newly
developed algorithm achieves comparable performance in public benchmarks.
\\ ( https://arxiv.org/abs/2006.12987 ,  1503kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13017
Date: Sun, 21 Jun 2020 07:35:41 GMT   (6524kb)

Title: Motion Representation Using Residual Frames with 3D CNN
Authors: Li Tao, Xueting Wang, Toshihiko Yamasaki
Categories: cs.CV
Comments: Accepted in IEEE ICIP 2020. arXiv admin note: substantial text
  overlap with arXiv:2001.05661
\\
  Recently, 3D convolutional networks (3D ConvNets) yield good performance in
action recognition. However, optical flow stream is still needed to ensure
better performance, the cost of which is very high. In this paper, we propose a
fast but effective way to extract motion features from videos utilizing
residual frames as the input data in 3D ConvNets. By replacing traditional
stacked RGB frames with residual ones, 35.6% and 26.6% points improvements over
top-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18
models are trained from scratch. And we achieved the state-of-the-art results
in this training mode. Analysis shows that better motion features can be
extracted using residual frames compared to RGB counterpart. By combining with
a simple appearance path, our proposal can be even better than some methods
using optical flow streams.
\\ ( https://arxiv.org/abs/2006.13017 ,  6524kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13046
Date: Sun, 21 Jun 2020 21:09:31 GMT   (959kb,D)

Title: Rotation Invariant Deep CBIR
Authors: Subhadip Maji and Smarajit Bose
Categories: cs.CV cs.LG eess.IV
Comments: arXiv admin note: text overlap with arXiv:2002.07877
\\
  Introduction of Convolutional Neural Networks has improved results on almost
every image-based problem and Content-Based Image Retrieval is not an
exception. But the CNN features, being rotation invariant, creates problems to
build a rotation-invariant CBIR system. Though rotation-invariant features can
be hand-engineered, the retrieval accuracy is very low because by hand
engineering only low-level features can be created, unlike deep learning models
that create high-level features along with low-level features. This paper shows
a novel method to build a rotational invariant CBIR system by introducing a
deep learning orientation angle detection model along with the CBIR feature
extraction model. This paper also highlights that this rotation invariant deep
CBIR can retrieve images from a large dataset in real-time.
\\ ( https://arxiv.org/abs/2006.13046 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13065
Date: Tue, 23 Jun 2020 14:38:34 GMT   (3095kb,D)

Title: DCNNs: A Transfer Learning comparison of Full Weapon Family threat
  detection forDual-Energy X-Ray Baggage Imagery
Authors: A. Williamson (1), P. Dickinson (2), T. Lambrou (2), J. C. Murray (1)
  ((1) University of Hull, (2) University of Lincoln)
Categories: cs.CV
Comments: Submitted to BMVC 2019 Workshop on "Object Detection and Recognition
  for Security Screening"
\\
  Recent advancements in Convolutional Neural Networks have yielded super-human
levels of performance in image recognition tasks [13, 25]; however, with
increasing volumes of parcels crossing UK borders each year, classification of
threats becomes integral to the smooth operation of UK borders. In this work we
propose the first pipeline to effectively process Dual-Energy X-Ray scanner
output, and perform classification capable of distinguishing between firearm
families (Assault Rifle, Revolver, Self-Loading Pistol,Shotgun, and Sub-Machine
Gun) from this output. With this pipeline we compare re-cent Convolutional
Neural Network architectures against the X-Ray baggage domain via Transfer
Learning and show ResNet50 to be most suitable to classification - outlining a
number of considerations for operational success within the domain.
\\ ( https://arxiv.org/abs/2006.13065 ,  3095kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13084
Date: Tue, 23 Jun 2020 15:10:19 GMT   (2541kb,D)

Title: Single-Shot 3D Detection of Vehicles from Monocular RGB Images via
  Geometry Constrained Keypoints in Real-Time
Authors: Nils G\"ahlert and Jun-Jun Wan and Nicolas Jourdan and Jan Finkbeiner
  and Uwe Franke and Joachim Denzler
Categories: cs.CV cs.LG cs.RO eess.IV
Comments: 2020 IEEE IV Symposium
\\
  In this paper we propose a novel 3D single-shot object detection method for
detecting vehicles in monocular RGB images. Our approach lifts 2D detections to
3D space by predicting additional regression and classification parameters and
hence keeping the runtime close to pure 2D object detection. The additional
parameters are transformed to 3D bounding box keypoints within the network
under geometric constraints. Our proposed method features a full 3D description
including all three angles of rotation without supervision by any labeled
ground truth data for the object's orientation, as it focuses on certain
keypoints within the image plane. While our approach can be combined with any
modern object detection framework with only little computational overhead, we
exemplify the extension of SSD for the prediction of 3D bounding boxes. We test
our approach on different datasets for autonomous driving and evaluate it using
the challenging KITTI 3D Object Detection as well as the novel nuScenes Object
Detection benchmarks. While we achieve competitive results on both benchmarks
we outperform current state-of-the-art methods in terms of speed with more than
20 FPS for all tested datasets and image resolutions.
\\ ( https://arxiv.org/abs/2006.13084 ,  2541kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13108
Date: Tue, 23 Jun 2020 15:58:22 GMT   (1298kb,D)

Title: Distilling Object Detectors with Task Adaptive Regularization
Authors: Ruoyu Sun, Fuhui Tang, Xiaopeng Zhang, Hongkai Xiong, Qi Tian
Categories: cs.CV
\\
  Current state-of-the-art object detectors are at the expense of high
computational costs and are hard to deploy to low-end devices. Knowledge
distillation, which aims at training a smaller student network by transferring
knowledge from a larger teacher model, is one of the promising solutions for
model miniaturization. In this paper, we investigate each module of a typical
detector in depth, and propose a general distillation framework that adaptively
transfers knowledge from teacher to student according to the task specific
priors. The intuition is that simply distilling all information from teacher to
student is not advisable, instead we should only borrow priors from the teacher
model where the student cannot perform well. Towards this goal, we propose a
region proposal sharing mechanism to interflow region responses between the
teacher and student models. Based on this, we adaptively transfer knowledge at
three levels, \emph{i.e.}, feature backbone, classification head, and bounding
box regression head, according to which model performs more reasonably.
Furthermore, considering that it would introduce optimization dilemma when
minimizing distillation loss and detection loss simultaneously, we propose a
distillation decay strategy to help improve model generalization via gradually
reducing the distillation penalty. Experiments on widely used detection
benchmarks demonstrate the effectiveness of our method. In particular, using
Faster R-CNN with FPN as an instantiation, we achieve an accuracy of $39.0\%$
with Resnet-50 on COCO dataset, which surpasses the baseline $36.3\%$ by
$2.7\%$ points, and even better than the teacher model with $38.5\%$ mAP.
\\ ( https://arxiv.org/abs/2006.13108 ,  1298kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13144
Date: Tue, 23 Jun 2020 16:39:59 GMT   (8324kb,D)

Title: Calibrated Adversarial Refinement for Multimodal Semantic Segmentation
Authors: Elias Kassapis, Georgi Dikov, Deepak K. Gupta, Cedric Nugteren
Categories: cs.CV
\\
  Ambiguities in images or unsystematic annotation can lead to multiple valid
solutions in semantic segmentation. To learn a distribution over predictions,
recent work has explored the use of probabilistic networks. However, these do
not necessarily capture the empirical distribution accurately. In this work, we
aim to learn a calibrated multimodal predictive distribution, where the
empirical frequency of the sampled predictions closely reflects that of the
corresponding labels in the training set. To this end, we propose a novel
two-stage, cascaded strategy for calibrated adversarial refinement. In the
first stage, we explicitly model the data with a categorical likelihood. In the
second, we train an adversarial network to sample from it an arbitrary number
of coherent predictions. The model can be used independently or integrated into
any black-box segmentation framework to enable the synthesis of diverse
predictions. We demonstrate the utility and versatility of the approach by
achieving competitive results on the multigrader LIDC dataset and a modified
Cityscapes dataset. In addition, we use a toy regression dataset to show that
our framework is not confined to semantic segmentation, and the core design can
be adapted to other tasks requiring learning a calibrated predictive
distribution.
\\ ( https://arxiv.org/abs/2006.13144 ,  8324kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13164
Date: Tue, 23 Jun 2020 17:07:00 GMT   (7444kb,D)

Title: Joint Detection and Multi-Object Tracking with Graph Neural Networks
Authors: Yongxin Wang and Xinshuo Weng and Kris Kitani
Categories: cs.CV cs.LG cs.MA cs.RO
\\
  Object detection and data association are critical components in multi-object
tracking (MOT) systems. Despite the fact that these two components are highly
dependent on each other, one popular trend in MOT is to perform detection and
data association as separate modules, processed in a cascaded order. Due to
this cascaded process, the resulting MOT system can only perform forward
inference and cannot back-propagate error through the entire pipeline and
correct them. This leads to sub-optimal performance over the total pipeline. To
address this issue, recent work jointly optimizes detection and data
association and forms an integrated MOT approach, which has been shown to
improve performance in both detection and tracking. In this work, we propose a
new approach for joint MOT based on Graph Neural Networks (GNNs). The key idea
of our approach is that GNNs can explicitly model complex interactions between
multiple objects in both the spatial and temporal domains, which is essential
for learning discriminative features for detection and data association. We
also leverage the fact that motion features are useful for MOT when used
together with appearance features. So our proposed joint MOT approach also
incorporates appearance and motion features within our graph-based feature
learning framework, leading to better feature learning for MOT. Through
extensive experiments on the MOT challenge dataset, we show that our proposed
method achieves state-of-the-art performance on both object detection and MOT.
\\ ( https://arxiv.org/abs/2006.13164 ,  7444kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13171
Date: Tue, 23 Jun 2020 17:18:54 GMT   (5517kb,D)

Title: ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to
  Objects
Authors: Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets,
  Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans
Categories: cs.CV cs.RO
\\
  We revisit the problem of Object-Goal Navigation (ObjectNav). In its simplest
form, ObjectNav is defined as the task of navigating to an object, specified by
its label, in an unexplored environment. In particular, the agent is
initialized at a random location and pose in an environment and asked to find
an instance of an object category, e.g., find a chair, by navigating to it.
  As the community begins to show increased interest in semantic goal
specification for navigation tasks, a number of different often-inconsistent
interpretations of this task are emerging. This document summarizes the
consensus recommendations of this working group on ObjectNav. In particular, we
make recommendations on subtle but important details of evaluation criteria
(for measuring success when navigating towards a target object), the agent's
embodiment parameters, and the characteristics of the environments within which
the task is carried out. Finally, we provide a detailed description of the
instantiation of these recommendations in challenges organized at the Embodied
AI workshop at CVPR 2020 \url{http://embodied-ai.org} .
\\ ( https://arxiv.org/abs/2006.13171 ,  5517kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13176
Date: Tue, 23 Jun 2020 17:24:09 GMT   (464kb)

Title: Boundary Regularized Building Footprint Extraction From Satellite Images
  Using Deep Neural Network
Authors: Kang Zhao, Muhammad Kamran, Gunho Sohn
Categories: cs.CV
\\
  In recent years, an ever-increasing number of remote satellites are orbiting
the Earth which streams vast amount of visual data to support a wide range of
civil, public and military applications. One of the key information obtained
from satellite imagery is to produce and update spatial maps of built
environment due to its wide coverage with high resolution data. However,
reconstructing spatial maps from satellite imagery is not a trivial vision task
as it requires reconstructing a scene or object with high-level representation
such as primitives. For the last decade, significant advancement in object
detection and representation using visual data has been achieved, but the
primitive-based object representation still remains as a challenging vision
task. Thus, a high-quality spatial map is mainly produced through complex
labour-intensive processes. In this paper, we propose a novel deep neural
network, which enables to jointly detect building instance and regularize noisy
building boundary shapes from a single satellite imagery. The proposed deep
learning method consists of a two-stage object detection network to produce
region of interest (RoI) features and a building boundary extraction network
using graph models to learn geometric information of the polygon shapes.
Extensive experiments show that our model can accomplish multi-tasks of object
localization, recognition, semantic labelling and geometric shape extraction
simultaneously. In terms of building extraction accuracy, computation
efficiency and boundary regularization performance, our model outperforms the
state-of-the-art baseline models.
\\ ( https://arxiv.org/abs/2006.13176 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13188
Date: Tue, 23 Jun 2020 17:41:10 GMT   (7246kb,D)

Title: Efficient Spatially Adaptive Convolution and Correlation
Authors: Thomas W. Mitchel, Benedict Brown, David Koller, Tim Weyrich, Szymon
  Rusinkiewicz, Michael Kazhdan
Categories: cs.CV cs.GR
\\
  Fast methods for convolution and correlation underlie a variety of
applications in computer vision and graphics, including efficient filtering,
analysis, and simulation. However, standard convolution and correlation are
inherently limited to fixed filters: spatial adaptation is impossible without
sacrificing efficient computation. In early work, Freeman and Adelson have
shown how steerable filters can address this limitation, providing a way for
rotating the filter as it is passed over the signal. In this work, we provide a
general, representation-theoretic, framework that allows for spatially varying
linear transformations to be applied to the filter. This framework allows for
efficient implementation of extended convolution and correlation for
transformation groups such as rotation (in 2D and 3D) and scale, and provides a
new interpretation for previous methods including steerable filters and the
generalized Hough transform. We present applications to pattern matching, image
feature description, vector field visualization, and adaptive image filtering.
\\ ( https://arxiv.org/abs/2006.13188 ,  7246kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13190
Date: Tue, 23 Jun 2020 17:44:05 GMT   (3430kb,D)

Title: Facing the Hard Problems in FGVC
Authors: Connor Anderson, Matt Gwilliam, Adam Teuscher, Andrew Merrill, Ryan
  Farrell
Categories: cs.CV
Comments: 17 pages, 6 figures, 2 tables
\\
  In fine-grained visual categorization (FGVC), there is a near-singular focus
in pursuit of attaining state-of-the-art (SOTA) accuracy. This work carefully
analyzes the performance of recent SOTA methods, quantitatively, but more
importantly, qualitatively. We show that these models universally struggle with
certain "hard" images, while also making complementary mistakes. We underscore
the importance of such analysis, and demonstrate that combining complementary
models can improve accuracy on the popular CUB-200 dataset by over 5%. In
addition to detailed analysis and characterization of the errors made by these
SOTA methods, we provide a clear set of recommended directions for future FGVC
researchers.
\\ ( https://arxiv.org/abs/2006.13190 ,  3430kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13192
Date: Tue, 23 Jun 2020 17:46:16 GMT   (2365kb,D)

Title: Towards Robust Sensor Fusion in Visual Perception
Authors: Shaojie Wang, Tong Wu, Yevgeniy Vorobeychik
Categories: cs.CV
\\
  We study the problem of robust sensor fusion in visual perception, especially
under the autonomous driving settings. We evaluate the robustness of RGB camera
and LiDAR sensor fusion for binary classification and object detection. In this
work, we are interested in the behavior of different fusion methods under
adversarial attacks on different sensors. We first train both classification
and detection models with early fusion and late fusion, then apply different
combinations of adversarial attacks on both sensor inputs for evaluation. We
also study the effectiveness of adversarial attacks with varying budgets.
Experiment results show that while sensor fusion models are generally
vulnerable to adversarial attacks, late fusion method is more robust than early
fusion. The results also provide insights on further obtaining robust sensor
fusion models.
\\ ( https://arxiv.org/abs/2006.13192 ,  2365kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13194
Date: Tue, 23 Jun 2020 17:48:29 GMT   (7871kb,D)

Title: Instant 3D Object Tracking with Applications in Augmented Reality
Authors: Adel Ahmadyan, Tingbo Hou, Jianing Wei, Liangkai Zhang, Artsiom
  Ablavatski, Matthias Grundmann
Categories: cs.CV
Comments: 4 pages, five figures, CVPR Fourth Workshop on Computer Vision for
  AR/VR
\\
  Tracking object poses in 3D is a crucial building block for Augmented Reality
applications. We propose an instant motion tracking system that tracks an
object's pose in space (represented by its 3D bounding box) in real-time on
mobile devices. Our system does not require any prior sensory calibration or
initialization to function. We employ a deep neural network to detect objects
and estimate their initial 3D pose. Then the estimated pose is tracked using a
robust planar tracker. Our tracker is capable of performing relative-scale
9-DoF tracking in real-time on mobile devices. By combining use of CPU and GPU
efficiently, we achieve 26-FPS+ performance on mobile devices.
\\ ( https://arxiv.org/abs/2006.13194 ,  7871kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12661
Date: Mon, 22 Jun 2020 23:02:10 GMT   (5052kb,D)

Title: SN-Engine, a Scale-free Geometric Modelling Environment
Authors: T.A. Zhukov
Categories: cs.GR
Comments: 16 pages, 3 figures
\\
  We present a new scale-free geometric modelling environment designed by the
author of the paper. It allows one to consistently treat geometric objects of
arbitrary size and offers extensive analytic and computational support for
visualization of both real and artificial sceneries.
\\ ( https://arxiv.org/abs/2006.12661 ,  5052kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2006.12838 (*cross-listing*)
Date: Tue, 23 Jun 2020 08:54:36 GMT   (2134kb,D)

Title: Analytic Solution to the Piecewise Linear Interface Construction Problem
  and its Application in Curvature Calculation for Volume-of-Fluid Simulation
  Codes
Authors: Moritz Lehmann and Stephan Gekle
Categories: physics.comp-ph cs.CG physics.flu-dyn
Comments: 18 pages, 5 figures
\\
  The plane-cube intersection problem has been around in literature since 1984
and iterative solutions to it have been used as part of piecewise linear
interface construction (PLIC) in computational fluid dynamics simulation codes
ever since. In many cases, PLIC is the bottleneck of these simulations
regarding compute time, so a faster, analytic solution to the plane-cube
intersection would greatly reduce compute time for such simulations. We derive
an analytic solution for all intersection cases and compare it to the one
previous solution from Scardovelli and Zaleski (Ruben Scardovelli and Stephane
Zaleski. "Analytical relations connecting linear interfaces and volume
fractions in rectangular grids". In: Journal of Computational Physics 164.1
(2000), pp. 228-237.), which we further improve to include edge cases and
micro-optimize to reduce arithmetic operations and branching. We then extend
our comparison regarding compute time and accuracy to include two different
iterative solutions as well. We find that the best choice depends on the
employed hardware platform: on the CPU, Newton-Raphson is fastest with
vectorization while analytic solutions perform better without. The reason for
this is that vectorization instruction sets do not include trigonometric
functions as used in the analytic solutions. On the GPU, the fastest method is
our optimized version of the analytic SZ solution. We finally provide details
on one of the applications of PLIC: curvature calculation for the
Volume-of-Fluid model used for free surface fluid simulations in combination
with the lattice Boltzmann method.
\\ ( https://arxiv.org/abs/2006.12838 ,  2134kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13166 (*cross-listing*)
Date: Tue, 23 Jun 2020 17:15:18 GMT   (3438kb)

Title: A Family of Constant-Areas Deltoids Associated with the Ellipse
Authors: Ronaldo Garcia and Dan Reznik and Hellmuth Stachel and Mark Helman
Categories: math.DS cs.CG cs.GR math.DG math.MG
Comments: 22 pages, 14 figures, 4 tables, 6 video links
MSC-class: 51M04, 51N20, 65D18
\\
  The Negative Pedal Curve (NPC) of the Ellipse with respect to a boundary
point M is a 3-cusp deltoid which is the affine image of the Steiner Curve.
Over all M the family has invariant area and displays an array of interesting
properties.
\\ ( https://arxiv.org/abs/2006.13166 ,  3438kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12557 (*cross-listing*)
Date: Mon, 22 Jun 2020 18:34:08 GMT   (169kb,D)

Title: Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and
  Data Poisoning Attacks
Authors: Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, Tom
  Goldstein
Categories: cs.LG cs.CR cs.CV cs.CY stat.ML
Comments: 19 pages, 4 figures
\\
  Data poisoning and backdoor attacks manipulate training data in order to
cause models to fail during inference. A recent survey of industry
practitioners found that data poisoning is the number one concern among threats
ranging from model stealing to adversarial attacks. However, we find that the
impressive performance evaluations from data poisoning attacks are, in large
part, artifacts of inconsistent experimental design. Moreover, we find that
existing poisoning methods have been tested in contrived scenarios, and they
fail in realistic settings. In order to promote fair comparison in future work,
we develop unified benchmarks for data poisoning and backdoor attacks.
\\ ( https://arxiv.org/abs/2006.12557 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12585 (*cross-listing*)
Date: Mon, 22 Jun 2020 19:53:50 GMT   (2796kb,D)

Title: Semantic Features Aided Multi-Scale Reconstruction of Inter-Modality
  Magnetic Resonance Images
Authors: Preethi Srinivasan, Prabhjot Kaur, Aditya Nigam, Arnav Bhavsar
Categories: eess.IV cs.CV
Comments: Accepted in IEEE CMBS 2020
\\
  Long acquisition time (AQT) due to series acquisition of multi-modality MR
images (especially T2 weighted images (T2WI) with longer AQT), though
beneficial for disease diagnosis, is practically undesirable. We propose a
novel deep network based solution to reconstruct T2W images from T1W images
(T1WI) using an encoder-decoder architecture. The proposed learning is aided
with semantic features by using multi-channel input with intensity values and
gradient of image in two orthogonal directions. A reconstruction module (RM)
augmenting the network along with a domain adaptation module (DAM) which is an
encoder-decoder model built-in with sharp bottleneck module (SBM) is trained
via modular training. The proposed network significantly reduces the total AQT
with negligible qualitative artifacts and quantitative loss (reconstructs one
volume in approximately 1 second). The testing is done on publicly available
dataset with real MR images, and the proposed network shows (approximately 1dB)
increase in PSNR over SOTA.
\\ ( https://arxiv.org/abs/2006.12585 ,  2796kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12655 (*cross-listing*)
Date: Mon, 22 Jun 2020 22:40:46 GMT   (5092kb,D)

Title: Perceptual Adversarial Robustness: Defense Against Unseen Threat Models
Authors: Cassidy Laidlaw and Sahil Singla and Soheil Feizi
Categories: cs.LG cs.CV stat.ML
\\
  We present adversarial attacks and defenses for the perceptual adversarial
threat model: the set of all perturbations to natural images which can mislead
a classifier but are imperceptible to human eyes. The perceptual threat model
is broad and encompasses $L_2$, $L_\infty$, spatial, and many other existing
adversarial threat models. However, it is difficult to determine if an
arbitrary perturbation is imperceptible without humans in the loop. To solve
this issue, we propose to use a {\it neural perceptual distance}, an
approximation of the true perceptual distance between images using internal
activations of neural networks. In particular, we use the Learned Perceptual
Image Patch Similarity (LPIPS) distance. We then propose the {\it neural
perceptual threat model} that includes adversarial examples with a bounded
neural perceptual distance to natural images. Under the neural perceptual
threat model, we develop two novel perceptual adversarial attacks to find any
imperceptible perturbations to images which can fool a classifier. Through an
extensive perceptual study, we show that the LPIPS distance correlates well
with human judgements of perceptibility of adversarial examples, validating our
threat model. Because the LPIPS threat model is very broad, we find that
Perceptual Adversarial Training (PAT) against a perceptual attack gives
robustness against many other types of adversarial attacks. We test PAT on
CIFAR-10 and ImageNet-100 against 12 types of adversarial attacks and find
that, for each attack, PAT achieves close to the accuracy of adversarial
training against just that perturbation type. That is, PAT generalizes well to
unforeseen perturbation types. This is vital in sensitive applications where a
particular threat model cannot be assumed, and to the best of our knowledge,
PAT is the first adversarial defense with this property.
\\ ( https://arxiv.org/abs/2006.12655 ,  5092kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12674 (*cross-listing*)
Date: Tue, 23 Jun 2020 00:17:32 GMT   (5021kb,D)

Title: Inexact Derivative-Free Optimization for Bilevel Learning
Authors: Matthias J. Ehrhardt, Lindon Roberts
Categories: math.OC cs.CV cs.LG cs.NA math.NA stat.ML
\\
  Variational regularization techniques are dominant in the field of
mathematical imaging. A drawback of these techniques is that they are dependent
on a number of parameters which have to be set by the user. A by now common
strategy to resolve this issue is to learn these parameters from data. While
mathematically appealing this strategy leads to a nested optimization problem
(known as bilevel optimization) which is computationally very difficult to
handle. A key ingredient in solving the upper-level problem is the exact
solution of the lower-level problem which is practically infeasible. In this
work we propose to solve these problems using inexact derivative-free
optimization algorithms which never require to solve the lower-level problem
exactly. We provide global convergence and worst-case complexity analysis of
our approach, and test our proposed framework on ROF-denoising and learning MRI
sampling patterns. Dynamically adjusting the lower-level accuracy yields
learned parameters with similar reconstruction quality as high-accuracy
evaluations but with dramatic reductions in computational work (up to 100 times
faster in some cases).
\\ ( https://arxiv.org/abs/2006.12674 ,  5021kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12676 (*cross-listing*)
Date: Tue, 23 Jun 2020 00:34:05 GMT   (2791kb)

Title: Generalized Grasping for Mechanical Grippers for Unknown Objects with
  Partial Point Cloud Representations
Authors: Michael Hegedus, Kamal Gupta, Mehran Mehrandezh
Categories: cs.RO cs.CV eess.IV
Comments: 8 pages, 12 figures, submitted to 2020 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2018) on 2/24/2020
\\
  We present a generalized grasping algorithm that uses point clouds (i.e. a
group of points and their respective surface normals) to discover grasp pose
solutions for multiple grasp types, executed by a mechanical gripper, in near
real-time. The algorithm introduces two ideas: 1) a histogram of finger contact
normals is used to represent a grasp 'shape' to guide a gripper orientation
search in a histogram of object(s) surface normals, and 2) voxel grid
representations of gripper and object(s) are cross-correlated to match finger
contact points, i.e. grasp 'size', to discover a grasp pose. Constraints, such
as collisions with neighbouring objects, are optionally incorporated in the
cross-correlation computation. We show via simulations and experiments that 1)
grasp poses for three grasp types can be found in near real-time, 2) grasp pose
solutions are consistent with respect to voxel resolution changes for both
partial and complete point cloud scans, and 3) a planned grasp is executed with
a mechanical gripper.
\\ ( https://arxiv.org/abs/2006.12676 ,  2791kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12704 (*cross-listing*)
Date: Tue, 23 Jun 2020 02:40:45 GMT   (3056kb,D)

Title: Semi-Supervised Learning for Fetal Brain MRI Quality Assessment with ROI
  consistency
Authors: Junshen Xu, Sayeri Lala, Borjan Gagoski, Esra Abaci Turk, P. Ellen
  Grant, Polina Golland, Elfar Adalsteinsson
Categories: eess.IV cs.CV
\\
  Fetal brain MRI is useful for diagnosing brain abnormalities but is
challenged by fetal motion. The current protocol for T2-weighted fetal brain
MRI is not robust to motion so image volumes are degraded by inter- and intra-
slice motion artifacts. Besides, manual annotation for fetal MR image quality
assessment are usually time-consuming. Therefore, in this work, a
semi-supervised deep learning method that detects slices with artifacts during
the brain volume scan is proposed. Our method is based on the mean teacher
model, where we not only enforce consistency between student and teacher models
on the whole image, but also adopt an ROI consistency loss to guide the network
to focus on the brain region. The proposed method is evaluated on a fetal brain
MR dataset with 11,223 labeled images and more than 200,000 unlabeled images.
Results show that compared with supervised learning, the proposed method can
improve model accuracy by about 6\% and outperform other state-of-the-art
semi-supervised learning methods. The proposed method is also implemented and
evaluated on an MR scanner, which demonstrates the feasibility of online image
quality assessment and image reacquisition during fetal MR scans.
\\ ( https://arxiv.org/abs/2006.12704 ,  3056kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12745 (*cross-listing*)
Date: Tue, 23 Jun 2020 04:19:04 GMT   (3173kb,D)

Title: Learning Physical Constraints with Neural Projections
Authors: Shuqi Yang, Xingzhe He, Bo Zhu
Categories: cs.NE cs.CV
\\
  We propose a new family of neural networks to predict the behaviors of
physical systems by learning their underpinning constraints. A neural
projection operator liesat the heart of our approach, composed of a lightweight
network with an embedded recursive architecture that interactively enforces
learned underpinning constraints and predicts the various governed behaviors of
different physical systems. Our neural projection operator is motivated by the
position-based dynamics model that has been used widely in game and visual
effects industries to unify the various fast physics simulators. Our method can
automatically and effectively uncover a broad range of constraints from
observation point data, such as length, angle, bending, collision, boundary
effects, and their arbitrary combinations, without any connectivity priors. We
provide a multi-group point representation in conjunction with a configurable
network connection mechanism to incorporate prior inputs for processing complex
physical systems. We demonstrated the efficacy of our approach by learning a
set of challenging physical systems all in a unified and simple fashion
including: rigid bodies with complex geometries, ropes with varying length and
bending, articulated soft and rigid bodies, and multi-object collisions with
complex boundaries.
\\ ( https://arxiv.org/abs/2006.12745 ,  3173kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12800 (*cross-listing*)
Date: Tue, 23 Jun 2020 07:18:05 GMT   (1105kb,D)

Title: Calibration of Neural Networks using Splines
Authors: Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink,
  Cristian Sminchisescu, Richard Hartley
Categories: cs.LG cs.CV stat.ML
\\
  Calibrating neural networks is of utmost importance when employing them in
safety-critical applications where the downstream decision making depends on
the predicted probabilities. Measuring calibration error amounts to comparing
two empirical distributions. In this work, we introduce a binning-free
calibration measure inspired by the classical Kolmogorov-Smirnov (KS)
statistical test in which the main idea is to compare the respective cumulative
probability distributions. From this, by approximating the empirical cumulative
distribution using a differentiable function via splines, we obtain a
recalibration function, which maps the network outputs to actual (calibrated)
class assignment probabilities. The spine-fitting is performed using a held-out
calibration set and the obtained recalibration function is evaluated on an
unseen test set. We tested our method against existing calibration approaches
on various image classification datasets and our spline-based recalibration
approach consistently outperforms existing methods on KS error as well as other
commonly used calibration measures.
\\ ( https://arxiv.org/abs/2006.12800 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12807 (*cross-listing*)
Date: Tue, 23 Jun 2020 07:55:10 GMT   (2404kb,D)

Title: Post-hoc Calibration of Neural Networks
Authors: Amir Rahimi, Kartik Gupta, Thalaiyasingam Ajanthan, Thomas Mensink,
  Cristian Sminchisescu, Richard Hartley
Categories: cs.LG cs.CV stat.ML
\\
  Calibration of neural networks is a critical aspect to consider when
incorporating machine learning models in real-world decision-making systems
where the confidence of decisions are equally important as the decisions
themselves. In recent years, there is a surge of research on neural network
calibration and the majority of the works can be categorized into post-hoc
calibration methods, defined as methods that learn an additional function to
calibrate an already trained base network. In this work, we intend to
understand the post-hoc calibration methods from a theoretical point of view.
Especially, it is known that minimizing Negative Log-Likelihood (NLL) will lead
to a calibrated network on the training set if the global optimum is attained
(Bishop, 1994). Nevertheless, it is not clear learning an additional function
in a post-hoc manner would lead to calibration in the theoretical sense. To
this end, we prove that even though the base network ($f$) does not lead to the
global optimum of NLL, by adding additional layers ($g$) and minimizing NLL by
optimizing the parameters of $g$ one can obtain a calibrated network $g \circ
f$. This not only provides a less stringent condition to obtain a calibrated
network but also provides a theoretical justification of post-hoc calibration
methods. Our experiments on various image classification benchmarks confirm the
theory.
\\ ( https://arxiv.org/abs/2006.12807 ,  2404kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12809 (*cross-listing*)
Date: Tue, 23 Jun 2020 08:00:51 GMT   (4517kb,D)

Title: 3D Probabilistic Segmentation and Volumetry from 2D projection images
Authors: Athanasios Vlontzos, Samuel Budd, Benjamin Hou, Daniel Rueckert,
  Bernhard Kainz
Categories: eess.IV cs.CV
\\
  X-Ray imaging is quick, cheap and useful for front-line care assessment and
intra-operative real-time imaging (e.g., C-Arm Fluoroscopy). However, it
suffers from projective information loss and lacks vital volumetric information
on which many essential diagnostic biomarkers are based on. In this paper we
explore probabilistic methods to reconstruct 3D volumetric images from 2D
imaging modalities and measure the models' performance and confidence. We show
our models' performance on large connected structures and we test for
limitations regarding fine structures and image domain sensitivity. We utilize
fast end-to-end training of a 2D-3D convolutional networks, evaluate our method
on 117 CT scans segmenting 3D structures from digitally reconstructed
radiographs (DRRs) with a Dice score of $0.91 \pm 0.0013$. Source code will be
made available by the time of the conference.
\\ ( https://arxiv.org/abs/2006.12809 ,  4517kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12834 (*cross-listing*)
Date: Tue, 23 Jun 2020 08:50:37 GMT   (17093kb,D)

Title: Sparse-RS: a versatile framework for query-efficient sparse black-box
  adversarial attacks
Authors: Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas
  Flammarion, Matthias Hein
Categories: cs.LG cs.CR cs.CV stat.ML
\\
  A large body of research has focused on adversarial attacks which require to
modify all input features with small $l_2$- or $l_\infty$-norms. In this paper
we instead focus on query-efficient sparse attacks in the black-box setting.
Our versatile framework, Sparse-RS, based on random search achieves
state-of-the-art success rate and query efficiency for different sparse attack
models such as $l_0$-bounded perturbations (outperforming established white-box
methods), adversarial patches, and adversarial framing. We show the
effectiveness of Sparse-RS on different datasets considering problems from
image recognition and malware detection and multiple variations of sparse
threat models, including targeted and universal perturbations. In particular
Sparse-RS can be used for realistic attacks such as universal adversarial patch
attacks without requiring a substitute model. The code of our framework is
available at https://github.com/fra31/sparse-rs.
\\ ( https://arxiv.org/abs/2006.12834 ,  17093kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12852 (*cross-listing*)
Date: Tue, 23 Jun 2020 09:20:42 GMT   (6547kb,D)

Title: Scale-Space Autoencoders for Unsupervised Anomaly Segmentation in Brain
  MRI
Authors: Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab
Categories: eess.IV cs.CV cs.LG q-bio.NC
\\
  Brain pathologies can vary greatly in size and shape, ranging from few pixels
(i.e. MS lesions) to large, space-occupying tumors. Recently proposed
Autoencoder-based methods for unsupervised anomaly segmentation in brain MRI
have shown promising performance, but face difficulties in modeling
distributions with high fidelity, which is crucial for accurate delineation of
particularly small lesions. Here, similar to these previous works, we model the
distribution of healthy brain MRI to localize pathologies from erroneous
reconstructions. However, to achieve improved reconstruction fidelity at higher
resolutions, we learn to compress and reconstruct different frequency bands of
healthy brain MRI using the laplacian pyramid. In a range of experiments
comparing our method to different State-of-the-Art approaches on three
different brain MR datasets with MS lesions and tumors, we show improved
anomaly segmentation performance and the general capability to obtain much more
crisp reconstructions of input data at native resolution. The modeling of the
laplacian pyramid further enables the delineation and aggregation of lesions at
multiple scales, which allows to effectively cope with different pathologies
and lesion sizes using a single model.
\\ ( https://arxiv.org/abs/2006.12852 ,  6547kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12915 (*cross-listing*)
Date: Tue, 23 Jun 2020 11:50:21 GMT   (776kb)

Title: Deep Attentive Wasserstein Generative Adversarial Networks for MRI
  Reconstruction with Recurrent Context-Awareness
Authors: Yifeng Guo, Chengjia Wang, Heye Zhang and Guang Yang
Categories: eess.IV cs.CV
\\
  The performance of traditional compressive sensing-based MRI (CS-MRI)
reconstruction is affected by its slow iterative procedure and noise-induced
artefacts. Although many deep learning-based CS-MRI methods have been proposed
to mitigate the problems of traditional methods, they have not been able to
achieve more robust results at higher acceleration factors. Most of the deep
learning-based CS-MRI methods still can not fully mine the information from the
k-space, which leads to unsatisfactory results in the MRI reconstruction. In
this study, we propose a new deep learning-based CS-MRI reconstruction method
to fully utilise the relationship among sequential MRI slices by coupling
Wasserstein Generative Adversarial Networks (WGAN) with Recurrent Neural
Networks. Further development of an attentive unit enables our model to
reconstruct more accurate anatomical structures for the MRI data. By
experimenting on different MRI datasets, we have demonstrated that our method
can not only achieve better results compared to the state-of-the-arts but can
also effectively reduce residual noise generated during the reconstruction
process.
\\ ( https://arxiv.org/abs/2006.12915 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13011 (*cross-listing*)
Date: Tue, 23 Jun 2020 13:55:29 GMT   (825kb,D)

Title: Joint Left Atrial Segmentation and Scar Quantification Based on a DNN
  with Spatial Encoding and Shape Attention
Authors: Lei Li, Xin Weng, Julia A. Schnabel, Xiahai Zhuang
Categories: eess.IV cs.CV
Comments: 10 pages
Journal-ref: MICCAI 2020
\\
  We propose an end-to-end deep neural network (DNN) which can simultaneously
segment the left atrial (LA) cavity and quantify LA scars. The framework
incorporates the continuous spatial information of the target by introducing a
spatially encoded (SE) loss based on the distance transform map. Compared to
conventional binary label based loss, the proposed SE loss can reduce noisy
patches in the resulting segmentation, which is commonly seen for deep
learning-based methods. To fully utilize the inherent spatial relationship
between LA and LA scars, we further propose a shape attention (SA) mechanism
through an explicit surface projection to build an end-to-end-trainable model.
Specifically, the SA scheme is embedded into a two-task network to perform the
joint LA segmentation and scar quantification. Moreover, the proposed method
can alleviate the severe class-imbalance problem when detecting small and
discrete targets like scars. We evaluated the proposed framework on 60 LGE MRI
data from the MICCAI2018 LA challenge. For LA segmentation, the proposed method
reduced the mean Hausdorff distance from 36.4 mm to 20.0 mm compared to the 3D
basic U-Net using the binary cross-entropy loss. For scar quantification, the
method was compared with the results or algorithms reported in the literature
and demonstrated better performance.
\\ ( https://arxiv.org/abs/2006.13011 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13016 (*cross-listing*)
Date: Sun, 21 Jun 2020 08:00:14 GMT   (2654kb,D)

Title: Rotation-Equivariant Neural Networks for Privacy Protection
Authors: Hao Zhang, Yiting Chen, Haotian Ma, Xu Cheng, Qihan Ren, Liyao Xiang,
  Jie Shi, Quanshi Zhang
Categories: cs.LG cs.CR cs.CV stat.ML
Comments: arXiv admin note: text overlap with arXiv:2003.08365
\\
  In order to prevent leaking input information from intermediate-layer
features, this paper proposes a method to revise the traditional neural network
into the rotation-equivariant neural network (RENN). Compared to the
traditional neural network, the RENN uses d-ary vectors/tensors as features, in
which each element is a d-ary number. These d-ary features can be rotated
(analogous to the rotation of a d-dimensional vector) with a random angle as
the encryption process. Input information is hidden in this target phase of
d-ary features for attribute obfuscation. Even if attackers have obtained
network parameters and intermediate-layer features, they cannot extract input
information without knowing the target phase. Hence, the input privacy can be
effectively protected by the RENN. Besides, the output accuracy of RENNs only
degrades mildly compared to traditional neural networks, and the computational
cost is significantly less than the homomorphic encryption.
\\ ( https://arxiv.org/abs/2006.13016 ,  2654kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13022 (*cross-listing*)
Date: Tue, 23 Jun 2020 14:01:06 GMT   (8689kb,D)

Title: Bridging the Theoretical Bound and Deep Algorithms for Open Set Domain
  Adaptation
Authors: Li Zhong, Zhen Fang, Feng Liu, Bo Yuan, Guangquan Zhang, Jie Lu
Categories: cs.LG cs.CV stat.ML
\\
  In the unsupervised open set domain adaptation (UOSDA), the target domain
contains unknown classes that are not observed in the source domain.
Researchers in this area aim to train a classifier to accurately: 1) recognize
unknown target data (data with unknown classes) and, 2) classify other target
data. To achieve this aim, a previous study has proven an upper bound of the
target-domain risk, and the open set difference, as an important term in the
upper bound, is used to measure the risk on unknown target data. By minimizing
the upper bound, a shallow classifier can be trained to achieve the aim.
However, if the classifier is very flexible (e.g., deep neural networks
(DNNs)), the open set difference will converge to a negative value when
minimizing the upper bound, which causes an issue where most target data are
recognized as unknown data. To address this issue, we propose a new upper bound
of target-domain risk for UOSDA, which includes four terms: source-domain risk,
$\epsilon$-open set difference ($\Delta_\epsilon$), a distributional
discrepancy between domains, and a constant. Compared to the open set
difference, $\Delta_\epsilon$ is more robust against the issue when it is being
minimized, and thus we are able to use very flexible classifiers (i.e., DNNs).
Then, we propose a new principle-guided deep UOSDA method that trains DNNs via
minimizing the new upper bound. Specifically, source-domain risk and
$\Delta_\epsilon$ are minimized by gradient descent, and the distributional
discrepancy is minimized via a novel open-set conditional adversarial training
strategy. Finally, compared to existing shallow and deep UOSDA methods, our
method shows the state-of-the-art performance on several benchmark datasets,
including digit recognition (MNIST, SVHN, USPS), object recognition (Office-31,
Office-Home), and face recognition (PIE).
\\ ( https://arxiv.org/abs/2006.13022 ,  8689kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13026 (*cross-listing*)
Date: Sat, 20 Jun 2020 16:23:32 GMT   (7413kb,D)

Title: Deep Polynomial Neural Networks
Authors: Grigorios Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang
  Deng, Yannis Panagakis, Stefanos Zafeiriou
Categories: cs.LG cs.CV stat.ML
Comments: Under review in T-PAMI. arXiv admin note: substantial text overlap
  with arXiv:2003.03828
\\
  Deep Convolutional Neural Networks (DCNNs) are currently the method of choice
both for generative, as well as for discriminative learning in computer vision
and machine learning. The success of DCNNs can be attributed to the careful
selection of their building blocks (e.g., residual blocks, rectifiers,
sophisticated normalization schemes, to mention but a few). In this paper, we
propose $\Pi$-Nets, a new class of DCNNs. $\Pi$-Nets are polynomial neural
networks, i.e., the output is a high-order polynomial of the input. The unknown
parameters, which are naturally represented by high-order tensors, are
estimated through a collective tensor factorization with factors sharing. We
introduce three tensor decompositions that significantly reduce the number of
parameters and show how they can be efficiently implemented by hierarchical
neural networks. We empirically demonstrate that $\Pi$-Nets are very expressive
and they even produce good results without the use of non-linear activation
functions in a large battery of tasks and signals, i.e., images, graphs, and
audio. When used in conjunction with activation functions, $\Pi$-Nets produce
state-of-the-art results in three challenging tasks, i.e. image generation,
face verification and 3D mesh representation learning.
\\ ( https://arxiv.org/abs/2006.13026 ,  7413kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13163 (*cross-listing*)
Date: Tue, 23 Jun 2020 17:06:49 GMT   (1274kb,D)

Title: MANTRA: A Machine Learning reference lightcurve dataset for astronomical
  transient event recognition
Authors: Mauricio Neira, Catalina G\'omez, John F. Su\'arez-P\'erez, Diego A.
  G\'omez, Juan Pablo Reyes, Marcela Hern\'andez Hoyos, Pablo Arbel\'aez, Jaime
  E. Forero-Romero
Categories: astro-ph.IM cs.CV
\\
  We introduce MANTRA, an annotated dataset of 4869 transient and 71207
non-transient object lightcurves built from the Catalina Real Time Transient
Survey. We provide public access to this dataset as a plain text file to
facilitate standardized quantitative comparison of astronomical transient event
recognition algorithms. Some of the classes included in the dataset are:
supernovae, cataclysmic variables, active galactic nuclei, high proper motion
stars, blazars and flares. As an example of the tasks that can be performed on
the dataset we experiment with multiple data pre-processing methods, feature
selection techniques and popular machine learning algorithms (Support Vector
Machines, Random Forests and Neural Networks). We assess quantitative
performance in two classification tasks: binary (transient/non-transient) and
eight-class classification. The best performing algorithm in both tasks is the
Random Forest Classifier. It achieves an F1-score of 96.25% in the binary
classification and 52.79% in the eight-class classification. For the
eight-class classification, non-transients ( 96.83% ) is the class with the
highest F1-score, while the lowest corresponds to high-proper-motion stars (
16.79% ); for supernovae it achieves a value of 54.57% , close to the average
across classes. The next release of MANTRA includes images and benchmarks with
deep learning models.
\\ ( https://arxiv.org/abs/2006.13163 ,  1274kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13202 (*cross-listing*)
Date: Tue, 23 Jun 2020 17:57:47 GMT   (2646kb,D)

Title: Simple and Effective VAE Training with Calibrated Decoders
Authors: Oleh Rybkin, Kostas Daniilidis, Sergey Levine
Categories: cs.LG cs.CV eess.IV stat.ML
Comments: Project website: \url{https://orybkin.github.io/sigma-vae/}
\\
  Variational autoencoders (VAEs) provide an effective and simple method for
modeling complex distributions. However, training VAEs often requires
considerable hyperparameter tuning, and often utilizes a heuristic weight on
the prior KL-divergence term. In this work, we study how the performance of
VAEs can be improved while not requiring the use of this heuristic
hyperparameter, by learning calibrated decoders that accurately model the
decoding distribution. While in some sense it may seem obvious that calibrated
decoders should perform better than uncalibrated decoders, much of the recent
literature that employs VAEs uses uncalibrated Gaussian decoders with constant
variance. We observe empirically that the na\"{i}ve way of learning variance in
Gaussian decoders does not lead to good results. However, {other calibrated
decoders, such as discrete decoders or learning shared variance} can
substantially improve performance. To further improve results, we propose a
simple but novel modification to the commonly used Gaussian decoder, which
represents the prediction variance non-parametrically. We observe empirically
that using the heuristic weight hyperparameter is not necessary with our
method. We analyze the performance of various discrete and continuous decoders
on a range of datasets and several single-image and sequential VAE models.
Project website: \url{https://orybkin.github.io/sigma-vae/}
\\ ( https://arxiv.org/abs/2006.13202 ,  2646kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13205 (*cross-listing*)
Date: Tue, 23 Jun 2020 17:58:56 GMT   (4225kb,D)

Title: Long-Horizon Visual Planning with Goal-Conditioned Hierarchical
  Predictors
Authors: Karl Pertsch, Oleh Rybkin, Frederik Ebert, Chelsea Finn, Dinesh
  Jayaraman, Sergey Levine
Categories: cs.LG cs.AI cs.CV cs.RO stat.ML
Comments: Project page: orybkin.github.io/video-gcp
\\
  The ability to predict and plan into the future is fundamental for agents
acting in the world. To reach a faraway goal, we predict trajectories at
multiple timescales, first devising a coarse plan towards the goal and then
gradually filling in details. In contrast, current learning approaches for
visual prediction and planning fail on long-horizon tasks as they generate
predictions (1) without considering goal information, and (2) at the finest
temporal resolution, one step at a time. In this work we propose a framework
for visual prediction and planning that is able to overcome both of these
limitations. First, we formulate the problem of predicting towards a goal and
propose the corresponding class of latent space goal-conditioned predictors
(GCPs). GCPs significantly improve planning efficiency by constraining the
search space to only those trajectories that reach the goal. Further, we show
how GCPs can be naturally formulated as hierarchical models that, given two
observations, predict an observation between them, and by recursively
subdividing each part of the trajectory generate complete sequences. This
divide-and-conquer strategy is effective at long-term prediction, and enables
us to design an effective hierarchical planning algorithm that optimizes
trajectories in a coarse-to-fine manner. We show that by using both
goal-conditioning and hierarchical prediction, GCPs enable us to solve visual
planning tasks with much longer horizon than previously possible.
\\ ( https://arxiv.org/abs/2006.13205 ,  4225kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12561 (*cross-listing*)
Date: Mon, 22 Jun 2020 18:47:54 GMT   (328kb,D)

Title: Better approximation algorithms for maximum weight internal spanning
  trees in cubic graphs and claw-free graphs
Authors: Ahmad Biniaz
Categories: cs.DS cs.DM
\\
  Given a connected vertex-weighted graph $G$, the maximum weight internal
spanning tree (MaxwIST) problem asks for a spanning tree of $G$ that maximizes
the total weight of internal nodes. This problem is NP-hard and APX-hard, with
the currently best known approximation factor $1/2$ (Chen et al., Algorithmica
2019). For the case of claw-free graphs, Chen et al. present an involved
approximation algorithm with approximation factor $7/12$. They asked whether it
is possible to improve these ratios, in particular for claw-free graphs and
cubic graphs.
  We improve the approximation factors for the MaxwIST problem in cubic graphs
and claw-free graphs. For cubic graphs we present an algorithm that computes a
spanning tree whose total weight of internal vertices is at least
$\frac{3}{4}-\frac{3}{n}$ times the total weight of all vertices, where $n$ is
the number of vertices of $G$. This ratio is almost tight for large values of
$n$. For claw-free graphs of degree at least three, we present an algorithm
that computes a spanning tree whose total internal weight is at least
$\frac{3}{5}-\frac{1}{n}$ times the total vertex weight. The degree constraint
is necessary as this ratio may not be achievable if we allow vertices of degree
less than three.
  With the above ratios, we immediately obtain better approximation algorithms
with factors $\frac{3}{4}-\epsilon$ and $\frac{3}{5}-\epsilon$ for the MaxwIST
problem in cubic graphs and claw-free graphs of degree at least three, for any
$\epsilon>0$. In addition to improving the approximation factors, the new
algorithms are relatively short compared to that of Chen et al.. The new
algorithms are fairly simple, and employ a variant of the depth-first search
algorithm that selects a relatively-large-weight vertex in every branching
step. Moreover, the new algorithms take linear time while previous algorithms
for similar problem instances are super-linear.
\\ ( https://arxiv.org/abs/2006.12561 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12741 (*cross-listing*)
Date: Tue, 23 Jun 2020 04:15:46 GMT   (845kb,D)

Title: A survey of repositories in graph theory
Authors: Srinibas Swain, C. Paul Bonnington, Graham Farr, Kerri Morgan
Categories: math.CO cs.DM math.HO
\\
  Since the pioneering work of R. M. Foster in the 1930s, many graph
repositories have been created to support research in graph theory. This survey
reviews many of these graph repositories and summarises the scope and contents
of each repository. We identify opportunities for the development of
repositories that can be queried in more flexible ways.
\\ ( https://arxiv.org/abs/2006.12741 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12994 (*cross-listing*)
Date: Tue, 23 Jun 2020 13:43:46 GMT   (20kb)

Title: On the flip graphs on perfect matchings of complete graphs and signed
  reversal graphs
Authors: Sebastian M. Cioab\u{a}, Gordon Royle, Zhao Kuang Tan
Categories: math.CO cs.DM
Comments: 14 pages, 6 figures, 2 tables
\\
  In this paper, we study the flip graph on the perfect matchings of a complete
graph of even order. We investigate its combinatorial and spectral properties
including connections to the signed reversal graph and we improve a previous
upper bound on its chromatic number.
\\ ( https://arxiv.org/abs/2006.12994 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13162 (*cross-listing*)
Date: Tue, 23 Jun 2020 17:06:39 GMT   (330kb,D)

Title: Discrete correlations of order 2 of generalised Rudin-Shapiro sequences:
  a combinatorial approach
Authors: Ir\`ene Marcovici, Thomas Stoll and Pierre-Adrien Tahay
Categories: math.CO cs.DM math.DS math.NT math.PR
\\
  We introduce a family of block-additive automatic sequences, that are
obtained by allocating a weight to each couple of digits, and defining the
$n$th term of the sequence as being the total weight of the integer $n$ written
in base $k$. Under an additional difference condition on the weight function,
these sequences can be interpreted as generalised Rudin-Shapiro sequences, and
we prove that they have the same correlations of order 2 as sequences of
symbols chosen uniformly and independently at random. The speed of convergence
is very fast and is independent of the prime factor decomposition of $k$. This
extends recent work of Tahay. The proof relies on direct observations about
base-$k$ representations of integers and combinatorial considerations. We also
provide extensions of our results to higher-dimensional block-additive
sequences.
\\ ( https://arxiv.org/abs/2006.13162 ,  330kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1907.05296
replaced with revised version Mon, 22 Jun 2020 23:43:18 GMT   (448kb,D)

Title: Simplification of Polyline Bundles
Authors: Joachim Spoerhase, Sabine Storandt, Johannes Zink
Categories: cs.CG cs.CC
\\ ( https://arxiv.org/abs/1907.05296 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2003.02190
replaced with revised version Tue, 23 Jun 2020 06:22:58 GMT   (43kb)

Title: Incidences between points and curves with almost two degrees of freedom
Authors: Micha Sharir and Noam Solomon and Oleg Zlydenko
Categories: cs.CG math.CO
Comments: Author Noam Solomon added. Some revisions
\\ ( https://arxiv.org/abs/2003.02190 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:1712.01887
replaced with revised version Tue, 23 Jun 2020 03:28:30 GMT   (4417kb,D)

Title: Deep Gradient Compression: Reducing the Communication Bandwidth for
  Distributed Training
Authors: Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally
Categories: cs.CV cs.DC cs.LG stat.ML
Comments: we find 99.9% of the gradient exchange in distributed SGD is
  redundant; we reduce the communication bandwidth by two orders of magnitude
  without losing accuracy. Code is available at:
  https://github.com/synxlin/deep-gradient-compression
Journal-ref: ICLR 2018
\\ ( https://arxiv.org/abs/1712.01887 ,  4417kb)
------------------------------------------------------------------------------
\\
arXiv:1901.04206
replaced with revised version Tue, 23 Jun 2020 05:16:00 GMT   (4933kb,D)

Title: Multi-band Weighted $l_p$ Norm Minimization for Image Denoising
Authors: Yanchi Su and Zhanshan Li and Haihong Yu and Zeyu Wang
Categories: cs.CV
Comments: accepted by Information Sciences
DOI: 10.1016/j.ins.2020.05.049
\\ ( https://arxiv.org/abs/1901.04206 ,  4933kb)
------------------------------------------------------------------------------
\\
arXiv:1904.07172
replaced with revised version Tue, 23 Jun 2020 13:01:36 GMT   (9413kb,D)

Title: Deep Iterative Surface Normal Estimation
Authors: Jan Eric Lenssen, Christian Osendorfer, Jonathan Masci
Categories: cs.CV cs.CG
Comments: Presented at CVPR 2020
\\ ( https://arxiv.org/abs/1904.07172 ,  9413kb)
------------------------------------------------------------------------------
\\
arXiv:1910.03676
replaced with revised version Mon, 22 Jun 2020 22:30:34 GMT   (5452kb,D)

Title: Representation Learning with Statistical Independence to Mitigate Bias
Authors: Ehsan Adeli, Qingyu Zhao, Adolf Pfefferbaum, Edith V. Sullivan, Li
  Fei-Fei, Juan Carlos Niebles, Kilian M. Pohl
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/1910.03676 ,  5452kb)
------------------------------------------------------------------------------
\\
arXiv:1912.02908
replaced with revised version Tue, 23 Jun 2020 15:51:07 GMT   (9055kb,D)

Title: Why Having 10,000 Parameters in Your Camera Model is Better Than Twelve
Authors: Thomas Sch\"ops, Viktor Larsson, Marc Pollefeys, Torsten Sattler
Categories: cs.CV
Comments: 15 pages, 12 figures, accepted to CVPR 2020 as an oral
\\ ( https://arxiv.org/abs/1912.02908 ,  9055kb)
------------------------------------------------------------------------------
\\
arXiv:1912.04465
replaced with revised version Tue, 23 Jun 2020 03:16:38 GMT   (3864kb,D)

Title: SoccerDB: A Large-Scale Database for Comprehensive Video Understanding
Authors: Yudong Jiang, Kaixu Cui, Leilei Chen, Canjin Wang, Changliang Xu
Categories: cs.CV
Comments: pre-print draft version
\\ ( https://arxiv.org/abs/1912.04465 ,  3864kb)
------------------------------------------------------------------------------
\\
arXiv:1912.12033
replaced with revised version Tue, 23 Jun 2020 10:54:36 GMT   (3074kb,D)

Title: Deep Learning for 3D Point Clouds: A Survey
Authors: Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, Mohammed
  Bennamoun
Categories: cs.CV cs.LG cs.RO eess.IV
Comments: Accepted by IEEE TPAMI. Project page:
  https://github.com/QingyongHu/SoTA-Point-Cloud
\\ ( https://arxiv.org/abs/1912.12033 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2002.00179
replaced with revised version Tue, 23 Jun 2020 09:34:17 GMT   (554kb)

Title: AdvJND: Generating Adversarial Examples with Just Noticeable Difference
Authors: Zifei Zhang, Kai Qiao, Lingyun Jiang, Linyuan Wang, and Bin Yan
Categories: cs.CV cs.CR eess.IV
\\ ( https://arxiv.org/abs/2002.00179 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2002.05459
replaced with revised version Mon, 22 Jun 2020 19:18:33 GMT   (5482kb,D)

Title: EndoL2H: Deep Super-Resolution for Capsule Endoscopy
Authors: Yasin Almalioglu, Kutsev Bengisu Ozyoruk, Abdulkadir Gokce, Kagan
  Incetan, Guliz Irem Gokceler, Muhammed Ali Simsek, Kivanc Ararat, Richard J.
  Chen, Nicholas J. Durr, Faisal Mahmood, Mehmet Turan
Categories: cs.CV cs.LG eess.IV
Comments: 23 pages, submitted to IEEE Transactions on Medical Imaging,
  corresponding Author: Mehmet Turan
\\ ( https://arxiv.org/abs/2002.05459 ,  5482kb)
------------------------------------------------------------------------------
\\
arXiv:2002.05966
replaced with revised version Tue, 23 Jun 2020 13:06:17 GMT   (2395kb,D)

Title: MCENET: Multi-Context Encoder Network for Homogeneous Agent Trajectory
  Prediction in Mixed Traffic
Authors: Hao Cheng, Wentong Liao, Michael Ying Yang, Monika Sester, Bodo
  Rosenhahn
Categories: cs.CV cs.CY cs.MA
Comments: 8 pages, 5 figures, code is available on
  https://github.com/haohao11/MCENET
\\ ( https://arxiv.org/abs/2002.05966 ,  2395kb)
------------------------------------------------------------------------------
\\
arXiv:2002.08988
replaced with revised version Mon, 22 Jun 2020 20:28:23 GMT   (5737kb,D)

Title: BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled
  Images
Authors: Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, Niloy
  Mitra
Categories: cs.CV
Comments: For project page, see https://www.monkeyoverflow.com/#/blockgan/
\\ ( https://arxiv.org/abs/2002.08988 ,  5737kb)
------------------------------------------------------------------------------
\\
arXiv:2003.04367
replaced with revised version Tue, 23 Jun 2020 00:14:15 GMT   (3152kb,D)

Title: Category-wise Attack: Transferable Adversarial Examples for Anchor Free
  Object Detection
Authors: Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi
  Wu
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2003.04367 ,  3152kb)
------------------------------------------------------------------------------
\\
arXiv:2004.04069
replaced with revised version Tue, 23 Jun 2020 11:59:06 GMT   (341kb)

Title: Convolutional neural net face recognition works in non-human-like ways
Authors: P. J. B. Hancock, R. S. Somai and V. R. Mileva
Categories: cs.CV
Comments: 8 pages, 2 figures. Submitted to Royal Society Open Science
ACM-class: I.4; I.5
\\ ( https://arxiv.org/abs/2004.04069 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2004.07464
replaced with revised version Tue, 23 Jun 2020 16:26:07 GMT   (1820kb,D)

Title: PICK: Processing Key Information Extraction from Documents using
  Improved Graph Learning-Convolutional Networks
Authors: Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, Rong Xiao
Categories: cs.CV
Comments: Accepted in the 25th International Conference on Pattern Recognition
  (ICPR 2020), Milan, Italy
\\ ( https://arxiv.org/abs/2004.07464 ,  1820kb)
------------------------------------------------------------------------------
\\
arXiv:2005.13127
replaced with revised version Tue, 23 Jun 2020 01:07:33 GMT   (5957kb)

Title: Towards Mesh Saliency Detection in 6 Degrees of Freedom
Authors: Xiaoying Ding and Zhenzhong Chen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2005.13127 ,  5957kb)
------------------------------------------------------------------------------
\\
arXiv:2006.03638
replaced with revised version Tue, 23 Jun 2020 16:43:02 GMT   (2127kb,D)

Title: Robust Face Verification via Disentangled Representations
Authors: Marius Arvinte, Ahmed H. Tewfik and Sriram Vishwanath
Categories: cs.CV cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2006.03638 ,  2127kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05597
replaced with revised version Tue, 23 Jun 2020 05:18:56 GMT   (2061kb,D)

Title: Condensing Two-stage Detection with Automatic Object Key Part Discovery
Authors: Zhe Chen, Jing Zhang, Dacheng Tao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.05597 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2006.08247
replaced with revised version Tue, 23 Jun 2020 14:06:36 GMT   (6511kb,D)

Title: Learn to cycle: Time-consistent feature discovery for action recognition
Authors: Alexandros Stergiou and Ronald Poppe
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.08247 ,  6511kb)
------------------------------------------------------------------------------
\\
arXiv:2006.09029
replaced with revised version Tue, 23 Jun 2020 03:37:40 GMT   (4363kb,D)

Title: Real-time Universal Style Transfer on High-resolution Images via
  Zero-channel Pruning
Authors: Jie An, Tao Li, Haozhi Huang, Li Shen, Xuan Wang, Yongyi Tang, Jinwen
  Ma, Wei Liu, and Jiebo Luo
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2006.09029 ,  4363kb)
------------------------------------------------------------------------------
\\
arXiv:2006.09081
replaced with revised version Tue, 23 Jun 2020 14:41:08 GMT   (455kb,D)

Title: Progressive Skeletonization: Trimming more fat from a network at
  initialization
Authors: Pau de Jorge, Amartya Sanyal, Harkirat S. Behl, Philip H.S. Torr,
  Gregory Rogez, Puneet K. Dokania
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.09081 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11161
replaced with revised version Mon, 22 Jun 2020 20:34:00 GMT   (2619kb,D)

Title: iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks
Authors: Aman Chadha, John Britto, and M. Mani Roja
Categories: cs.CV cs.LG cs.MM eess.IV
Comments: 11 pages, 6 figures, 4 tables
Journal-ref: Springer Journal of Computational Visual Media, Tsinghua
  University Press, 6(3):1-11, 2020
\\ ( https://arxiv.org/abs/2006.11161 ,  2619kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11371
replaced with revised version Tue, 23 Jun 2020 01:48:56 GMT   (5618kb,D)

Title: Opportunities and Challenges in Explainable Artificial Intelligence
  (XAI): A Survey
Authors: Arun Das and Paul Rad
Categories: cs.CV cs.AI cs.LG
Comments: 24 pages, 20 figures, survey paper, submitting to IEEE
\\ ( https://arxiv.org/abs/2006.11371 ,  5618kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11436
replaced with revised version Tue, 23 Jun 2020 16:45:07 GMT   (4904kb,D)

Title: BEV-Seg: Bird's Eye View Semantic Segmentation Using Geometry and
  Semantic Point Cloud
Authors: Mong H. Ng, Kaahan Radia, Jianfei Chen, Dequan Wang, Ionel Gog, and
  Joseph E. Gonzalez
Categories: cs.CV
Comments: Accepted into CVPR 2020 Workshop Scalability in Autonomous Driving by
  Waymo
\\ ( https://arxiv.org/abs/2006.11436 ,  4904kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11999
replaced with revised version Tue, 23 Jun 2020 08:49:06 GMT   (3988kb,D)

Title: Modeling Lost Information in Lossy Image Compression
Authors: Yaolong Wang, Mingqing Xiao, Chang Liu, Shuxin Zheng, Tie-Yan Liu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.11999 ,  3988kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12226
replaced with revised version Tue, 23 Jun 2020 12:30:31 GMT   (5335kb,D)

Title: Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single
  Sample
Authors: Shir Gur, Sagie Benaim, Lior Wolf
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.12226 ,  5335kb)
------------------------------------------------------------------------------
\\
arXiv:2004.04712
replaced with revised version Tue, 23 Jun 2020 08:08:35 GMT   (27kb)

Title: Solutions for Subset Sum Problems with Special Digraph Constraints
Authors: Frank Gurski, Dominique Komander, and Carolin Rehs
Categories: cs.DM
MSC-class: 05C85, 90C39, 05C69
\\ ( https://arxiv.org/abs/2004.04712 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:1912.10637
replaced with revised version Tue, 23 Jun 2020 05:37:34 GMT   (8168kb,D)

Title: GrabAR: Occlusion-aware Grabbing Virtual Objects in AR
Authors: Xiao Tang, Xiaowei Hu, Chi-Wing Fu, Daniel Cohen-Or
Categories: cs.GR cs.HC
Comments: conditionally accepted to UIST 2020
\\ ( https://arxiv.org/abs/1912.10637 ,  8168kb)
------------------------------------------------------------------------------
\\
arXiv:1904.00865 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 15:43:09 GMT   (1638kb,D)

Title: Non-linear aggregation of filters to improve image denoising
Authors: Benjamin Guedj and Juliette Rengot
Categories: stat.ML cs.CV cs.LG eess.IV
Comments: To appear at Computing Conference 2020
\\ ( https://arxiv.org/abs/1904.00865 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:1908.10417
replaced with revised version Tue, 23 Jun 2020 10:14:02 GMT   (2391kb)

Title: Complex Deep Learning Models for Denoising of Human Heart ECG signals
Authors: Corneliu Arsene
Categories: cs.LG cs.CV eess.SP stat.ML
Comments: 51 pages, 23 figures
Journal-ref: EUSIPCO.2019 (Pages 11- 18)
DOI: 10.5281/zenodo.3904247
\\ ( https://arxiv.org/abs/1908.10417 ,  2391kb)
------------------------------------------------------------------------------
\\
arXiv:1909.01940 (*cross-listing*)
replaced with revised version Mon, 22 Jun 2020 22:50:10 GMT   (1660kb,D)

Title: Can we trust deep learning models diagnosis? The impact of domain shift
  in chest radiograph classification
Authors: Eduardo H. P. Pooch, Pedro L. Ballester, Rodrigo C. Barros
Categories: eess.IV cs.AI cs.CV cs.LG stat.ML
Comments: 10 pages, 3 figures
\\ ( https://arxiv.org/abs/1909.01940 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:1909.13082
replaced with revised version Mon, 22 Jun 2020 18:04:14 GMT   (9744kb,D)

Title: Wasserstein-2 Generative Networks
Authors: Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander
  Safin and Evgeny Burnaev
Categories: cs.LG cs.CV stat.ML
Comments: 29 pages, 21 figures, 3 tables
\\ ( https://arxiv.org/abs/1909.13082 ,  9744kb)
------------------------------------------------------------------------------
\\
arXiv:1910.14526
replaced with revised version Tue, 23 Jun 2020 12:52:47 GMT   (4452kb,AD)

Title: Towards vision-based robotic skins: a data-driven, multi-camera tactile
  sensor
Authors: Camill Trueeb, Carmelo Sferrazza and Raffaello D'Andrea
Categories: cs.RO cs.CV
Comments: Accompanying video: https://youtu.be/lbavqAlKl98
Journal-ref: Proceedings of the 2020 3rd IEEE International Conference on Soft
  Robotics (RoboSoft), pp. 333-338
DOI: 10.1109/RoboSoft48309.2020.9116060
\\ ( https://arxiv.org/abs/1910.14526 ,  4452kb)
------------------------------------------------------------------------------
\\
arXiv:1911.06786
replaced with revised version Tue, 23 Jun 2020 09:02:52 GMT   (81kb,D)

Title: Data Efficient Stagewise Knowledge Distillation
Authors: Akshay Kulkarni, Navid Panchi, Sharath Chandra Raparthy and Shital
  Chiddarwar
Categories: cs.LG cs.CV
Comments: 15 pages, 1 figure, 6 tables and 1 algorithm
\\ ( https://arxiv.org/abs/1911.06786 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2002.01180 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 14:35:30 GMT   (3619kb,D)

Title: Robust Generative Restricted Kernel Machines using Weighted Conjugate
  Feature Duality
Authors: Arun Pandey, Joachim Schreurs, Johan A. K. Suykens
Categories: stat.ML cs.CV cs.LG
\\ ( https://arxiv.org/abs/2002.01180 ,  3619kb)
------------------------------------------------------------------------------
\\
arXiv:2002.08327
replaced with revised version Tue, 23 Jun 2020 03:54:20 GMT   (5363kb)

Title: Fawkes: Protecting Privacy against Unauthorized Deep Learning Models
Authors: Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng, Ben
  Y. Zhao
Categories: cs.CR cs.CV cs.LG stat.ML
\\ ( https://arxiv.org/abs/2002.08327 ,  5363kb)
------------------------------------------------------------------------------
\\
arXiv:2002.08797 (*cross-listing*)
replaced with revised version Mon, 22 Jun 2020 18:26:19 GMT   (1117kb,D)

Title: Pruning untrained neural networks: Principles and Analysis
Authors: Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, Yee Whye Teh
Categories: stat.ML cs.CV cs.LG
Comments: 31 pages, 12 figures
\\ ( https://arxiv.org/abs/2002.08797 ,  1117kb)
------------------------------------------------------------------------------
\\
arXiv:2004.11210 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 03:03:08 GMT   (7849kb,D)

Title: Simulating Anisoplanatic Turbulence by Sampling Inter-modal and
  Spatially Correlated Zernike Coefficients
Authors: Nicholas Chimitt and Stanley H. Chan
Categories: physics.optics astro-ph.IM cs.CV eess.IV physics.flu-dyn
\\ ( https://arxiv.org/abs/2004.11210 ,  7849kb)
------------------------------------------------------------------------------
\\
arXiv:2006.02026 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 03:23:11 GMT   (5130kb,D)

Title: Image Classification in the Dark using Quanta Image Sensors
Authors: Abhiram Gnanasambandam and Stanley H. Chan
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2006.02026 ,  5130kb)
------------------------------------------------------------------------------
\\
arXiv:2006.09628
replaced with revised version Tue, 23 Jun 2020 04:37:24 GMT   (9485kb,D)

Title: Visor: Privacy-Preserving Video Analytics as a Cloud Service
Authors: Rishabh Poddar and Ganesh Ananthanarayanan and Srinath Setty and
  Stavros Volos and Raluca Ada Popa
Categories: cs.CR cs.CV
Comments: USENIX Security 2020
\\ ( https://arxiv.org/abs/2006.09628 ,  9485kb)
------------------------------------------------------------------------------
\\
arXiv:2006.09989 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 17:56:29 GMT   (519kb,D)

Title: Universal Lower-Bounds on Classification Error under Adversarial Attacks
  and Random Corruption
Authors: Elvis Dohmatob
Categories: stat.ML cs.CV cs.LG cs.NE
\\ ( https://arxiv.org/abs/2006.09989 ,  519kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11630 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 08:47:00 GMT   (674kb,D)

Title: A Fast Stochastic Plug-and-Play ADMM for Imaging Inverse Problems
Authors: Junqi Tang, Mike Davies
Categories: math.OC cs.CV
\\ ( https://arxiv.org/abs/2006.11630 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12090 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 09:55:50 GMT   (2725kb,D)

Title: Deep Low-rank Prior in Dynamic MR Imaging
Authors: Ziwen Ke, Wenqi Huang, Jing Cheng, Sen Jia, Haifeng Wang, Xin Liu,
  Hairong Zheng, Leslie Ying, Yanjie Zhu and Dong Liang
Categories: eess.IV cs.CV
Comments: 12 pages, 7 figures
\\ ( https://arxiv.org/abs/2006.12090 ,  2725kb)
------------------------------------------------------------------------------
\\
arXiv:1907.05944
replaced with revised version Tue, 23 Jun 2020 07:37:51 GMT   (25kb)

Title: Online learning for min-max discrete problems
Authors: Evripidis Bampis, Dimitris Christou, Bruno Escoffier, Nguyen Kim Thang
Categories: cs.DS cs.DM cs.LG
\\ ( https://arxiv.org/abs/1907.05944 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2006.08266
replaced with revised version Tue, 23 Jun 2020 01:59:06 GMT   (30kb,D)

Title: The PSPACE-hardness of understanding neural circuits
Authors: Vidya Sagar Sharma, Piyush Srivastava
Categories: cs.CC cs.DM q-bio.NC
Comments: 2 figures
\\ ( https://arxiv.org/abs/2006.08266 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2006.09094 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 09:38:19 GMT   (12kb)

Title: Another approach to non-repetitive colorings of graphs of bounded degree
Authors: Matthieu Rosenfeld
Categories: math.CO cs.DM
\\ ( https://arxiv.org/abs/2006.09094 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11798 (*cross-listing*)
replaced with revised version Tue, 23 Jun 2020 16:47:48 GMT   (22kb)

Title: Further progress towards Hadwiger's conjecture
Authors: Luke Postle
Categories: math.CO cs.DM
Comments: 16 pages; with 9 page appendix. Bound has been improved since
  previous version
\\ ( https://arxiv.org/abs/2006.11798 ,  22kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
