Delivered-To: brucelu2013@gmail.com
Received: by 2002:ab4:a48c:0:0:0:0:0 with SMTP id ds12csp100423ecb;
        Thu, 24 Sep 2020 02:13:04 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJx5YQwL1Ce30+5UPW/t88Pe2h1Qc/q5s6xJ93QXs+34m+6jiuyma4JSNxszU7zom/+DMGzh
X-Received: by 2002:ad4:500c:: with SMTP id s12mr4062407qvo.7.1600938783644;
        Thu, 24 Sep 2020 02:13:03 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1600938783; cv=none;
        d=google.com; s=arc-20160816;
        b=cGnJP4c+YKPd7jeL46/7BGgYNIUzsazKTDcbjE027T5x6RmbvLiAu0FOQuCf4HzwN2
         /TwXInzDSY2nMTOWcpKU3Ex6lH3e/uOHWA1G4Zfk/KaXohlKCm/4mdQM2YjGp0Dk+sKi
         Yt1XkNRM/C5q0PesUM1XTGOYov4ARUh9wa1Dx6vevc9GZzVQOVo/DW+mI+1qufcin8l6
         fF0HXFWoy6Ys0trINdr3QIRcKdI2DqKhs2Qt+lQhxfQlUxa7mf9jAYIgr9v99YFHGMCi
         InxxVly0jIK6M4XL8oo95t0hjfFgUAdzNtd58m2ovXmoMQJhZ2wW3HltNmi7RN7XJG2d
         BGGg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=YVyU0idZ2PNClagXcLFh7pNPJPsOzbN4byKrQRg3oYo=;
        b=UIeH/UGW9VJxbIIL2ZHzWnWd55lAfn/0RpNCXS1+R7z8m0px/VKb9yQeUldApzZhoX
         5VsSrc1EbpJYYzbKKdFI/vTAdMOUVXuwY/t4Qo6Pvxr2g6B6/HeMyR5Po2Hzi2KbTphM
         6E32D/M4giEfSccpH7P2ZPxQBdnDCGUMXTuQwq/W3hW4A45L5d+I9QlpiYdq3GbaRgE/
         Ias6p9Xi4n8n820tfMTg0fiBYde5sq1rG407OcpKZdgnDRwPGWLVxRVEu+RvSR1EDzmY
         J1MACq5rwwFtNbas2yiyvjfqgiTpOgT2j/gnOwqYYhQqyrUefgUuXoEQChtkjf1hzFRb
         hS5Q==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id b18si1589436qtc.379.2020.09.24.02.13.03
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 24 Sep 2020 02:13:03 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 08O9D2o1048849;
	Thu, 24 Sep 2020 05:13:02 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 08O9D2No044422;
	Thu, 24 Sep 2020 05:13:02 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 08O9D28t044420;
	Thu, 24 Sep 2020 05:13:02 -0400
Date: Thu, 24 Sep 2020 05:13:02 -0400
Message-Id: <202009240913.08O9D28t044420@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 2420 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computer Vision and Pattern Recognition
Discrete Mathematics
Graphics
 received from  Tue 22 Sep 20 18:00:00 GMT  to  Wed 23 Sep 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2009.10762
Date: Tue, 22 Sep 2020 18:46:05 GMT   (20120kb,D)

Title: Role of Orthogonality Constraints in Improving Properties of Deep
  Networks for Image Classification
Authors: Hongjun Choi, Anirudh Som, Pavan Turaga
Categories: cs.CV cs.LG
Comments: 8 figures, 4 tables, 1 pseudo-code
\\
  Standard deep learning models that employ the categorical cross-entropy loss
are known to perform well at image classification tasks. However, many standard
models thus obtained often exhibit issues like feature redundancy, low
interpretability, and poor calibration. A body of recent work has emerged that
has tried addressing some of these challenges by proposing the use of new
regularization functions in addition to the cross-entropy loss. In this paper,
we present some surprising findings that emerge from exploring the role of
simple orthogonality constraints as a means of imposing physics-motivated
constraints common in imaging. We propose an Orthogonal Sphere (OS) regularizer
that emerges from physics-based latent-representations under simplifying
assumptions. Under further simplifying assumptions, the OS constraint can be
written in closed-form as a simple orthonormality term and be used along with
the cross-entropy loss function. The findings indicate that orthonormality loss
function results in a) rich and diverse feature representations, b) robustness
to feature sub-selection, c) better semantic localization in the class
activation maps, and d) reduction in model calibration error. We demonstrate
the effectiveness of the proposed OS regularization by providing quantitative
and qualitative results on four benchmark datasets - CIFAR10, CIFAR100, SVHN
and tiny ImageNet.
\\ ( https://arxiv.org/abs/2009.10762 ,  20120kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10777
Date: Tue, 22 Sep 2020 19:28:57 GMT   (652kb)

Title: Efficient DWT-based fusion techniques using genetic algorithm for
  optimal parameter estimation
Authors: S. Kavitha, K. K. Thyagharajan
Categories: cs.CV cs.AI eess.IV
Comments: 17 pages, 3 figures
ACM-class: I.2.6; I.2.10; I.4.6; I.4.7; I.4.10; I.5
DOI: 10.1007/s00500-015-2009-6
\\
  Image fusion plays a vital role in medical imaging. Image fusion aims to
integrate complementary as well as redundant information from multiple
modalities into a single fused image without distortion or loss of information.
In this research work, discrete wavelet transform (DWT)and undecimated discrete
wavelet transform (UDWT)-based fusion techniques using genetic algorithm
(GA)foroptimalparameter(weight)estimationinthefusionprocessareimplemented and
analyzed with multi-modality brain images. The lack of shift variance while
performing image fusion using DWT is addressed using UDWT. The proposed fusion
model uses an efficient, modified GA in DWT and UDWT for optimal parameter
estimation, to improve the image quality and contrast. The complexity of the
basic GA (pixel level) has been reduced in the modified GA (feature level), by
limiting the search space. It is observed from our experiments that fusion
using DWT and UDWT techniques with GA for optimal parameter estimation resulted
in a better fused image in the aspects of retaining the information and
contrast without error, both in human perception as well as evaluation using
objective metrics. The contributions of this research work are (1) reduced time
and space complexity in estimating the weight values using GA for fusion (2)
system is scalable for input image of any size with similar time complexity,
owing to feature level GA implementation and (3) identification of source image
that contributes more to the fused image, from the weight values estimated.
\\ ( https://arxiv.org/abs/2009.10777 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10814
Date: Tue, 22 Sep 2020 21:02:00 GMT   (910kb)

Title: Kernelized dense layers for facial expression recognition
Authors: M.Amine Mahmoudi, Aladine Chetouani, Fatma Boufera and Hedi Tabia
Categories: cs.CV
\\
  Fully connected layer is an essential component of Convolutional Neural
Networks (CNNs), which demonstrates its efficiency in computer vision tasks.
The CNN process usually starts with convolution and pooling layers that first
break down the input images into features, and then analyze them independently.
The result of this process feeds into a fully connected neural network
structure which drives the final classification decision. In this paper, we
propose a Kernelized Dense Layer (KDL) which captures higher order feature
interactions instead of conventional linear relations. We apply this method to
Facial Expression Recognition (FER) and evaluate its performance on RAF,
FER2013 and ExpW datasets. The experimental results demonstrate the benefits of
such layer and show that our model achieves competitive results with respect to
the state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2009.10814 ,  910kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10825
Date: Tue, 22 Sep 2020 21:15:27 GMT   (6096kb,D)

Title: Angular Luminance for Material Segmentation
Authors: Jia Xue, Matthew Purri, Kristin Dana
Categories: cs.CV cs.RO
Comments: IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
  2020
Journal-ref: IEEE International Geoscience and Remote Sensing Symposium
  (IGARSS) 2020
\\
  Moving cameras provide multiple intensity measurements per pixel, yet often
semantic segmentation, material recognition, and object recognition do not
utilize this information. With basic alignment over several frames of a moving
camera sequence, a distribution of intensities over multiple angles is
obtained. It is well known from prior work that luminance histograms and the
statistics of natural images provide a strong material recognition cue. We
utilize per-pixel {\it angular luminance distributions} as a key feature in
discriminating the material of the surface. The angle-space sampling in a
multiview satellite image sequence is an unstructured sampling of the
underlying reflectance function of the material. For real-world materials there
is significant intra-class variation that can be managed by building a angular
luminance network (AngLNet). This network combines angular reflectance cues
from multiple images with spatial cues as input to fully convolutional networks
for material segmentation. We demonstrate the increased performance of AngLNet
over prior state-of-the-art in material segmentation from satellite imagery.
\\ ( https://arxiv.org/abs/2009.10825 ,  6096kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10868
Date: Wed, 23 Sep 2020 00:55:12 GMT   (4141kb,D)

Title: A Real-time Vision Framework for Pedestrian Behavior Recognition and
  Intention Prediction at Intersections Using 3D Pose Estimation
Authors: Ue-Hwan Kim, Dongho Ka, Hwasoo Yeo, Jong-Hwan Kim
Categories: cs.CV
Comments: 12 pages, 6 figures, 4 tables
\\
  Minimizing traffic accidents between vehicles and pedestrians is one of the
primary research goals in intelligent transportation systems. To achieve the
goal, pedestrian behavior recognition and prediction of pedestrian's crossing
or not-crossing intention play a central role. Contemporary approaches do not
guarantee satisfactory performance due to lack of generalization, the
requirement of manual data labeling, and high computational complexity. To
overcome these limitations, we propose a real-time vision framework for two
tasks: pedestrian behavior recognition (100.53 FPS) and intention prediction
(35.76 FPS). Our framework obtains satisfying generalization over multiple
sites because of the proposed site-independent features. At the center of the
feature extraction lies 3D pose estimation. The 3D pose analysis enables robust
and accurate recognition of pedestrian behaviors and prediction of intentions
over multiple sites. The proposed vision framework realizes 89.3% accuracy in
the behavior recognition task on the TUD dataset without any training process
and 91.28% accuracy in intention prediction on our dataset achieving new
state-of-the-art performance. To contribute to the corresponding research
community, we make our source codes public which are available at
https://github.com/Uehwan/VisionForPedestrian
\\ ( https://arxiv.org/abs/2009.10868 ,  4141kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10874
Date: Wed, 23 Sep 2020 01:20:19 GMT   (618kb,D)

Title: Hamming OCR: A Locality Sensitive Hashing Neural Network for Scene Text
  Recognition
Authors: Bingcong Li, Xin Tang, Xianbiao Qi, Yihao Chen, Rong Xiao
Categories: cs.CV cs.AI cs.LG
Comments: 9 Pages, 4 Figure
\\
  Recently, inspired by Transformer, self-attention-based scene text
recognition approaches have achieved outstanding performance. However, we find
that the size of model expands rapidly with the lexicon increasing.
Specifically, the number of parameters for softmax classification layer and
output embedding layer are proportional to the vocabulary size. It hinders the
development of a lightweight text recognition model especially applied for
Chinese and multiple languages. Thus, we propose a lightweight scene text
recognition model named Hamming OCR. In this model, a novel Hamming classifier,
which adopts locality sensitive hashing (LSH) algorithm to encode each
character, is proposed to replace the softmax regression and the generated LSH
code is directly employed to replace the output embedding. We also present a
simplified transformer decoder to reduce the number of parameters by removing
the feed-forward network and using cross-layer parameter sharing technique.
Compared with traditional methods, the number of parameters in both
classification and embedding layers is independent on the size of vocabulary,
which significantly reduces the storage requirement without loss of accuracy.
Experimental results on several datasets, including four public benchmaks and a
Chinese text dataset synthesized by SynthText with more than 20,000 characters,
shows that Hamming OCR achieves competitive results.
\\ ( https://arxiv.org/abs/2009.10874 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10891
Date: Wed, 23 Sep 2020 01:49:03 GMT   (6377kb,D)

Title: Leveraging Local and Global Descriptors in Parallel to Search
  Correspondences for Visual Localization
Authors: Pengju Zhang, Yihong Wu, Bingxi Liu
Categories: cs.CV
\\
  Visual localization to compute 6DoF camera pose from a given image has wide
applications such as in robotics, virtual reality, augmented reality, etc. Two
kinds of descriptors are important for the visual localization. One is global
descriptors that extract the whole feature from each image. The other is local
descriptors that extract the local feature from each image patch usually
enclosing a key point. More and more methods of the visual localization have
two stages: at first to perform image retrieval by global descriptors and then
from the retrieval feedback to make 2D-3D point correspondences by local
descriptors. The two stages are in serial for most of the methods. This simple
combination has not achieved superiority of fusing local and global
descriptors. The 3D points obtained from the retrieval feedback are as the
nearest neighbor candidates of the 2D image points only by global descriptors.
Each of the 2D image points is also called a query local feature when
performing the 2D-3D point correspondences. In this paper, we propose a novel
parallel search framework, which leverages advantages of both local and global
descriptors to get nearest neighbor candidates of a query local feature.
Specifically, besides using deep learning based global descriptors, we also
utilize local descriptors to construct random tree structures for obtaining
nearest neighbor candidates of the query local feature. We propose a new
probabilistic model and a new deep learning based local descriptor when
constructing the random trees. A weighted Hamming regularization term to keep
discriminativeness after binarization is given in the loss function for the
proposed local descriptor. The loss function co-trains both real and binary
descriptors of which the results are integrated into the random trees.
\\ ( https://arxiv.org/abs/2009.10891 ,  6377kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10892
Date: Wed, 23 Sep 2020 01:49:56 GMT   (1461kb,D)

Title: LoRRaL: Facial Action Unit Detection Based on Local Region Relation
  Learning
Authors: Ziqiang Shi and Liu Liu and Rujie Liu and Xiaoyu Mi and and Kentaro
  Murase
Categories: cs.CV
\\
  End-to-end convolution representation learning has been proved to be very
effective in facial action unit (AU) detection. Considering the co-occurrence
and mutual exclusion between facial AUs, in this paper, we propose convolution
neural networks with Local Region Relation Learning (LoRRaL), which can combine
latent relationships among AUs for an end-to-end approach to facial AU
occurrence detection. LoRRaL consists of 1) use bi-directional long short-term
memory (BiLSTM) to dynamically and sequentially encode local AU feature maps,
2) use self-attention mechanism to dynamically compute correspondences from
local facial regions and to re-aggregate AU feature maps considering AU
co-occurrences and mutual exclusions, 3) use a continuous-state modern Hopfield
network to encode and map local facial features to more discriminative AU
feature maps, that all these networks take the facial image as input and map it
to AU occurrences. Our experiments on the challenging BP4D and DISFA Benchmarks
without any external data or pre-trained models results in F1-scores of 63.5%
and 61.4% respectively, which shows our proposed networks can lead to
performance improvement on the AU detection task.
\\ ( https://arxiv.org/abs/2009.10892 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10916
Date: Wed, 23 Sep 2020 03:10:12 GMT   (18806kb,D)

Title: CLASS: Cross-Level Attention and Supervision for Salient Objects
  Detection
Authors: Tang Lv and Bo Li
Categories: cs.CV
\\
  Salient object detection (SOD) is a fundamental computer vision task.
Recently, with the revival of deep neural networks, SOD has made great
progresses. However, there still exist two thorny issues that cannot be well
addressed by existing methods, indistinguishable regions and complex
structures. To address these two issues, in this paper we propose a novel deep
network for accurate SOD, named CLASS. First, in order to leverage the
different advantages of low-level and high-level features, we propose a novel
non-local cross-level attention (CLA), which can capture the long-range feature
dependencies to enhance the distinction of complete salient object. Second, a
novel cross-level supervision (CLS) is designed to learn complementary context
for complex structures through pixel-level, region-level and object-level. Then
the fine structures and boundaries of salient objects can be well restored. In
experiments, with the proposed CLA and CLS, our CLASS net. consistently
outperforms 13 state-of-the-art methods on five datasets
\\ ( https://arxiv.org/abs/2009.10916 ,  18806kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10939
Date: Wed, 23 Sep 2020 06:27:54 GMT   (9228kb,D)

Title: Scene Graph to Image Generation with Contextualized Object Layout
  Refinement
Authors: Maor Ivgi, Yaniv Benny, Avichai Ben-David, Jonathan Berant, and Lior
  Wolf
Categories: cs.CV
\\
  Generating high-quality images from scene graphs, that is, graphs that
describe multiple entities in complex relations, is a challenging task that
attracted substantial interest recently. Prior work trained such models by
using supervised learning, where the goal is to produce the exact target image
layout for each scene graph. It relied on predicting object locations and
shapes independently and in parallel. However, scene graphs are underspecified,
and thus the same scene graph often occurs with many target images in the
training data. This leads to generated images with high inter-object overlap,
empty areas, blurry objects, and overall compromised quality. In this work, we
propose a method that alleviates these issues by generating all object layouts
together and reducing the reliance on such supervision. Our model predicts
layouts directly from embeddings (without predicting intermediate boxes) by
gradually upsampling, refining and contextualizing object layouts. It is
trained with a novel adversarial loss, that optimizes the interaction between
object pairs. This improves coverage and removes overlaps, while maintaining
sensible contours and respecting objects relations. We empirically show on the
COCO-STUFF dataset that our proposed approach substantially improves the
quality of generated layouts as well as the overall image quality. Our
evaluation shows that we improve layout coverage by almost 20 points, and drop
object overlap to negligible amounts. This leads to better image generation,
relation fulfillment and objects quality.
\\ ( https://arxiv.org/abs/2009.10939 ,  9228kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10942
Date: Wed, 23 Sep 2020 06:29:09 GMT   (2169kb,D)

Title: Exploring global diverse attention via pairwise temporal relation for
  video summarization
Authors: Ping Li, Qinghao Ye, Luming Zhang, Li Yuan, Xianghua Xu, Ling Shao
Categories: cs.CV cs.MM
Comments: 12 pages, 8 figures
Journal-ref: Pattern Recognition, 2020
\\
  Video summarization is an effective way to facilitate video searching and
browsing. Most of existing systems employ encoder-decoder based recurrent
neural networks, which fail to explicitly diversify the system-generated
summary frames while requiring intensive computations. In this paper, we
propose an efficient convolutional neural network architecture for video
SUMmarization via Global Diverse Attention called SUM-GDA, which adapts
attention mechanism in a global perspective to consider pairwise temporal
relations of video frames. Particularly, the GDA module has two advantages: 1)
it models the relations within paired frames as well as the relations among all
pairs, thus capturing the global attention across all frames of one video; 2)
it reflects the importance of each frame to the whole video, leading to diverse
attention on these frames. Thus, SUM-GDA is beneficial for generating diverse
frames to form satisfactory video summary. Extensive experiments on three data
sets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its
extension outperform other competing state-of-the-art methods with remarkable
improvements. In addition, the proposed models can be run in parallel with
significantly less computational costs, which helps the deployment in highly
demanding applications.
\\ ( https://arxiv.org/abs/2009.10942 ,  2169kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10945
Date: Wed, 23 Sep 2020 06:31:59 GMT   (3144kb,D)

Title: MAFF-Net: Filter False Positive for 3D Vehicle Detection with
  Multi-modal Adaptive Feature Fusion
Authors: Zehan Zhang, Ming Zhang, Zhidong Liang, Xian Zhao, Ming Yang, Wenming
  Tan, and ShiLiang Pu
Categories: cs.CV
Comments: 8 pages, 6 figures
\\
  3D vehicle detection based on multi-modal fusion is an important task of many
applications such as autonomous driving. Although significant progress has been
made, we still observe two aspects that need to be further improvement: First,
the specific gain that camera images can bring to 3D detection is seldom
explored by previous works. Second, many fusion algorithms run slowly, which is
essential for applications with high real-time requirements(autonomous
driving). To this end, we propose an end-to-end trainable single-stage
multi-modal feature adaptive network in this paper, which uses image
information to effectively reduce false positive of 3D detection and has a fast
detection speed. A multi-modal adaptive feature fusion module based on channel
attention mechanism is proposed to enable the network to adaptively use the
feature of each modal. Based on the above mechanism, two fusion technologies
are proposed to adapt to different usage scenarios: PointAttentionFusion is
suitable for filtering simple false positive and faster; DenseAttentionFusion
is suitable for filtering more difficult false positive and has better overall
performance. Experimental results on the KITTI dataset demonstrate significant
improvement in filtering false positive over the approach using only point
cloud data. Furthermore, the proposed method can provide competitive results
and has the fastest speed compared to the published state-of-the-art
multi-modal methods in the KITTI benchmark.
\\ ( https://arxiv.org/abs/2009.10945 ,  3144kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10962
Date: Wed, 23 Sep 2020 07:04:08 GMT   (3114kb,D)

Title: What is the Reward for Handwriting? -- Handwriting Generation by
  Imitation Learning
Authors: Keisuke Kanda, Brian Kenji Iwana, Seiichi Uchida
Categories: cs.CV
Comments: Accepted at ICFHR2020
\\
  Analyzing the handwriting generation process is an important issue and has
been tackled by various generation models, such as kinematics based models and
stochastic models. In this study, we use a reinforcement learning (RL)
framework to realize handwriting generation with the careful future planning
ability. In fact, the handwriting process of human beings is also supported by
their future planning ability; for example, the ability is necessary to
generate a closed trajectory like '0' because any shortsighted model, such as a
Markovian model, cannot generate it. For the algorithm, we employ generative
adversarial imitation learning (GAIL). Typical RL algorithms require the manual
definition of the reward function, which is very crucial to control the
generation process. In contrast, GAIL trains the reward function along with the
other modules of the framework. In other words, through GAIL, we can understand
the reward of the handwriting generation process from handwriting examples. Our
experimental results qualitatively and quantitatively show that the learned
reward catches the trends in handwriting generation and thus GAIL is well
suited for the acquisition of handwriting behavior.
\\ ( https://arxiv.org/abs/2009.10962 ,  3114kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11016
Date: Wed, 23 Sep 2020 09:33:24 GMT   (13808kb,D)

Title: Generative Model without Prior Distribution Matching
Authors: Cong Geng, Jia Wang, Li Chen, Zhiyong Gao
Categories: cs.CV
Comments: 8 pages, 8 figures
\\
  Variational Autoencoder (VAE) and its variations are classic generative
models by learning a low-dimensional latent representation to satisfy some
prior distribution (e.g., Gaussian distribution). Their advantages over GAN are
that they can simultaneously generate high dimensional data and learn latent
representations to reconstruct the inputs. However, it has been observed that a
trade-off exists between reconstruction and generation since matching prior
distribution may destroy the geometric structure of data manifold. To mitigate
this problem, we propose to let the prior match the embedding distribution
rather than imposing the latent variables to fit the prior. The embedding
distribution is trained using a simple regularized autoencoder architecture
which preserves the geometric structure to the maximum. Then an adversarial
strategy is employed to achieve a latent mapping. We provide both theoretical
and experimental support for the effectiveness of our method, which alleviates
the contradiction between topological properties' preserving of data manifold
and distribution matching in latent space.
\\ ( https://arxiv.org/abs/2009.11016 ,  13808kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11042
Date: Wed, 23 Sep 2020 10:33:01 GMT   (7877kb,D)

Title: Few-shot Font Generation with Localized Style Representations and
  Factorization
Authors: Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim
Categories: cs.CV
Comments: 11 pages, 10 figures, first two authors contributed equally
\\
  Automatic few-shot font generation is in high demand because manual designs
are expensive and sensitive to the expertise of designers. Existing few-shot
font generation methods aim to learn to disentangle the style and content
element from a few reference glyphs, and mainly focus on a universal style
representation for each font style. However, such approach limits the model in
representing diverse local styles, and thus makes it unsuitable to the most
complicated letter system, e.g., Chinese, whose characters consist of a varying
number of components (often called "radical") with a highly complex structure.
In this paper, we propose a novel font generation method by learning localized
styles, namely component-wise style representations, instead of universal
styles. The proposed style representations enable us to synthesize complex
local details in text designs. However, learning component-wise styles solely
from reference glyphs is infeasible in the few-shot font generation scenario,
when a target script has a large number of components, e.g., over 200 for
Chinese. To reduce the number of reference glyphs, we simplify component-wise
styles by a product of component factor and style factor, inspired by low-rank
matrix factorization. Thanks to the combination of strong representation and a
compact factorization strategy, our method shows remarkably better few-shot
font generation results (with only 8 reference glyph images) than other
state-of-the-arts, without utilizing strong locality supervision, e.g.,
location of each component, skeleton, or strokes. The source code is available
at https://github.com/clovaai/lffont.
\\ ( https://arxiv.org/abs/2009.11042 ,  7877kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11044
Date: Wed, 23 Sep 2020 10:40:03 GMT   (1757kb,D)

Title: Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem
  Formulation
Authors: Dimche Kostadinov and Davide Scaramuzza
Categories: cs.CV cs.AI cs.LG cs.RO
Journal-ref: IAPR IEEE/Computer Society International Conference on Pattern
  Recognition (ICPR), Milan, 2021
\\
  Event-based cameras record an asynchronous stream of per-pixel brightness
changes. As such, they have numerous advantages over the standard frame-based
cameras, including high temporal resolution, high dynamic range, and no motion
blur. Due to the asynchronous nature, efficient learning of compact
representation for event data is challenging. While it remains not explored the
extent to which the spatial and temporal event "information" is useful for
pattern recognition tasks. In this paper, we focus on single-layer
architectures. We analyze the performance of two general problem formulations:
the direct and the inverse, for unsupervised feature learning from local event
data (local volumes of events described in space-time). We identify and show
the main advantages of each approach. Theoretically, we analyze guarantees for
an optimal solution, possibility for asynchronous, parallel parameter update,
and the computational complexity. We present numerical experiments for object
recognition. We evaluate the solution under the direct and the inverse problem
and give a comparison with the state-of-the-art methods. Our empirical results
highlight the advantages of both approaches for representation learning from
event data. We show improvements of up to 9 % in the recognition accuracy
compared to the state-of-the-art methods from the same class of methods.
\\ ( https://arxiv.org/abs/2009.11044 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11050
Date: Wed, 23 Sep 2020 10:47:24 GMT   (6356kb,D)

Title: Robust and efficient post-processing for video object detection
Authors: Alberto Sabater, Luis Montesano, Ana C. Murillo
Categories: cs.CV cs.LG
Comments: Submitted to the International Conference on Intelligent Robots and
  Systems, IROS 2020
\\
  Object recognition in video is an important task for plenty of applications,
including autonomous driving perception, surveillance tasks, wearable devices
or IoT networks. Object recognition using video data is more challenging than
using still images due to blur, occlusions or rare object poses. Specific video
detectors with high computational cost or standard image detectors together
with a fast post-processing algorithm achieve the current state-of-the-art.
This work introduces a novel post-processing pipeline that overcomes some of
the limitations of previous post-processing methods by introducing a
learning-based similarity evaluation between detections across frames. Our
method improves the results of state-of-the-art specific video detectors,
specially regarding fast moving objects, and presents low resource
requirements. And applied to efficient still image detectors, such as YOLO,
provides comparable results to much more computationally intensive detectors.
\\ ( https://arxiv.org/abs/2009.11050 ,  6356kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11063
Date: Mon, 21 Sep 2020 18:36:17 GMT   (9235kb,D)

Title: A Sparse Sampling-based framework for Semantic Fast-Forward of
  First-Person Videos
Authors: Michel Melo Silva, Washington Luis Souza Ramos, Mario Fernando
  Montenegro Campos, Erickson Rangel Nascimento
Categories: cs.CV
Comments: Accepted at the IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI) 2020. arXiv admin note: text overlap with
  arXiv:1802.08722
DOI: 10.1109/TPAMI.2020.2983929
\\
  Technological advances in sensors have paved the way for digital cameras to
become increasingly ubiquitous, which, in turn, led to the popularity of the
self-recording culture. As a result, the amount of visual data on the Internet
is moving in the opposite direction of the available time and patience of the
users. Thus, most of the uploaded videos are doomed to be forgotten and
unwatched stashed away in some computer folder or website. In this paper, we
address the problem of creating smooth fast-forward videos without losing the
relevant content. We present a new adaptive frame selection formulated as a
weighted minimum reconstruction problem. Using a smoothing frame transition and
filling visual gaps between segments, our approach accelerates first-person
videos emphasizing the relevant segments and avoids visual discontinuities.
Experiments conducted on controlled videos and also on an unconstrained dataset
of First-Person Videos (FPVs) show that, when creating fast-forward videos, our
method is able to retain as much relevant information and smoothness as the
state-of-the-art techniques, but in less processing time.
\\ ( https://arxiv.org/abs/2009.11063 ,  9235kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11072
Date: Tue, 22 Sep 2020 02:57:28 GMT   (12779kb,D)

Title: Differential Viewpoints for Ground Terrain Material Recognition
Authors: Jia Xue, Hang Zhang, Ko Nishino, Kristin J. Dana
Categories: cs.CV
Comments: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI). arXiv admin note: substantial text overlap with arXiv:1612.02372
Journal-ref: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI) 2020
\\
  Computational surface modeling that underlies material recognition has
transitioned from reflectance modeling using in-lab controlled radiometric
measurements to image-based representations based on internet-mined single-view
images captured in the scene. We take a middle-ground approach for material
recognition that takes advantage of both rich radiometric cues and flexible
image capture. A key concept is differential angular imaging, where small
angular variations in image capture enables angular-gradient features for an
enhanced appearance representation that improves recognition. We build a
large-scale material database, Ground Terrain in Outdoor Scenes (GTOS)
database, to support ground terrain recognition for applications such as
autonomous driving and robot navigation. The database consists of over 30,000
images covering 40 classes of outdoor ground terrain under varying weather and
lighting conditions. We develop a novel approach for material recognition
called texture-encoded angular network (TEAN) that combines deep encoding
pooling of RGB information and differential angular images for angular-gradient
features to fully leverage this large dataset. With this novel network
architecture, we extract characteristics of materials encoded in the angular
and spatial gradients of their appearance. Our results show that TEAN achieves
recognition performance that surpasses single view performance and standard
(non-differential/large-angle sampling) multiview performance.
\\ ( https://arxiv.org/abs/2009.11072 ,  12779kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11084
Date: Wed, 23 Sep 2020 12:10:06 GMT   (5563kb,D)

Title: Multiplexed Illumination for Classifying Visually Similar Objects
Authors: Taihua Wang and Donald G. Dansereau
Categories: cs.CV
Comments: Submitted to Computer Vision and Image Understanding (CVIU)
\\
  Distinguishing visually similar objects like forged/authentic bills and
healthy/unhealthy plants is beyond the capabilities of even the most
sophisticated classifiers. We propose the use of multiplexed illumination to
extend the range of objects that can be successfully classified. We construct a
compact RGB-IR light stage that images samples under different combinations of
illuminant position and colour. We then develop a methodology for selecting
illumination patterns and training a classifier using the resulting imagery. We
use the light stage to model and synthetically relight training samples, and
propose a greedy pattern selection scheme that exploits this ability to train
in simulation. We then apply the trained patterns to carry out fast
classification of new objects. We demonstrate the approach on visually similar
artificial and real fruit samples, showing a marked improvement compared with
fixed-illuminant approaches as well as a more conventional code selection
scheme. This work allows fast classification of previously indistinguishable
objects, with potential applications in forgery detection, quality control in
agriculture and manufacturing, and skin lesion classification.
\\ ( https://arxiv.org/abs/2009.11084 ,  5563kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11110
Date: Wed, 23 Sep 2020 12:40:04 GMT   (2906kb,D)

Title: Residual Embedding Similarity-Based Network Selection for Predicting
  Brain Network Evolution Trajectory from a Single Observation
Authors: Ahmet Serkan Goktas, Alaa Bessadok and Islem Rekik
Categories: cs.CV cs.LG
\\
  While existing predictive frameworks are able to handle Euclidean structured
data (i.e, brain images), they might fail to generalize to geometric
non-Euclidean data such as brain networks. Besides, these are rooted the sample
selection step in using Euclidean or learned similarity measure between
vectorized training and testing brain networks. Such sample connectomic
representation might include irrelevant and redundant features that could
mislead the training sample selection step. Undoubtedly, this fails to exploit
and preserve the topology of the brain connectome. To overcome this major
drawback, we propose Residual Embedding Similarity-Based Network selection
(RESNets) for predicting brain network evolution trajectory from a single
timepoint. RESNets first learns a compact geometric embedding of each training
and testing sample using adversarial connectome embedding network. This nicely
reduces the high-dimensionality of brain networks while preserving their
topological properties via graph convolutional networks. Next, to compute the
similarity between subjects, we introduce the concept of a connectional brain
template (CBT), a fixed network reference, where we further represent each
training and testing network as a deviation from the reference CBT in the
embedding space. As such, we select the most similar training subjects to the
testing subject at baseline by comparing their learned residual embeddings with
respect to the pre-defined CBT. Once the best training samples are selected at
baseline, we simply average their corresponding brain networks at follow-up
timepoints to predict the evolution trajectory of the testing network. Our
experiments on both healthy and disordered brain networks demonstrate the
success of our proposed method in comparison to RESNets ablated versions and
traditional approaches.
\\ ( https://arxiv.org/abs/2009.11110 ,  2906kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11118
Date: Wed, 23 Sep 2020 12:54:34 GMT   (1186kb,D)

Title: Multiple interaction learning with question-type prior knowledge for
  constraining answer search space in visual question answering
Authors: Tuong Do, Binh X. Nguyen, Huy Tran, Erman Tjiputra, Quang D. Tran,
  Thanh-Toan Do
Categories: cs.CV
Comments: Accepted in ECCV Workshop 2020
\\
  Different approaches have been proposed to Visual Question Answering (VQA).
However, few works are aware of the behaviors of varying joint modality methods
over question type prior knowledge extracted from data in constraining answer
search space, of which information gives a reliable cue to reason about answers
for questions asked in input images. In this paper, we propose a novel VQA
model that utilizes the question-type prior information to improve VQA by
leveraging the multiple interactions between different joint modality methods
based on their behaviors in answering questions from different types. The solid
experiments on two benchmark datasets, i.e., VQA 2.0 and TDIUC, indicate that
the proposed method yields the best performance with the most competitive
approaches.
\\ ( https://arxiv.org/abs/2009.11118 ,  1186kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11150
Date: Wed, 23 Sep 2020 13:51:16 GMT   (5351kb,D)

Title: Information-Theoretic Visual Explanation for Black-Box Classifiers
Authors: Jihun Yi, Eunji Kim, Siwon Kim, Sungroh Yoon
Categories: cs.CV
\\
  In this work, we attempt to explain the prediction of any black-box
classifier from an information-theoretic perspective. For this purpose, we
propose two attribution maps: an information gain (IG) map and a point-wise
mutual information (PMI) map. IG map provides a class-independent answer to
"How informative is each pixel?", and PMI map offers a class-specific
explanation by answering "How much does each pixel support a specific class?"
In this manner, we propose (i) a theory-backed attribution method. The
attribution (ii) provides both supporting and opposing explanations for each
class and (iii) pinpoints most decisive parts in the image, not just the
relevant objects. In addition, the method (iv) offers a complementary
class-independent explanation. Lastly, the algorithmic enhancement in our
method (v) improves faithfulness of the explanation in terms of a quantitative
evaluation metric. We showed the five strengths of our method through various
experiments on the ImageNet dataset. The code of the proposed method is
available online.
\\ ( https://arxiv.org/abs/2009.11150 ,  5351kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11154
Date: Wed, 23 Sep 2020 13:58:12 GMT   (682kb,D)

Title: 2D-3D Geometric Fusion Network using Multi-Neighbourhood Graph
  Convolution for RGB-D Indoor Scene Classification
Authors: Albert Mosella-Montoro, Javier Ruiz-Hidalgo
Categories: cs.CV cs.AI
\\
  Multi-modal fusion has been proved to help enhance the performance of scene
classification tasks. This paper presents a 2D-3D fusion stage that combines 3D
Geometric features with 2D Texture features obtained by 2D Convolutional Neural
Networks. To get a robust 3D Geometric embedding, a network that uses two novel
layers is proposed. The first layer, Multi-Neighbourhood Graph Convolution,
aims to learn a more robust geometric descriptor of the scene combining two
different neighbourhoods: one in the Euclidean space and the other in the
Feature space. The second proposed layer, Nearest Voxel Pooling, improves the
performance of the well-known Voxel Pooling. Experimental results, using
NYU-Depth-v2 and SUN RGB-D datasets, show that the proposed method outperforms
the current state-of-the-art in RGB-D indoor scene classification tasks.
\\ ( https://arxiv.org/abs/2009.11154 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11160
Date: Wed, 23 Sep 2020 14:12:17 GMT   (535kb,D)

Title: Label-Efficient Multi-Task Segmentation using Contrastive Learning
Authors: Junichiro Iwasawa, Yuichiro Hirano and Yohei Sugawara
Categories: cs.CV q-bio.TO
Comments: Accepted to MICCAI BrainLes 2020 workshop
\\
  Obtaining annotations for 3D medical images is expensive and time-consuming,
despite its importance for automating segmentation tasks. Although multi-task
learning is considered an effective method for training segmentation models
using small amounts of annotated data, a systematic understanding of various
subtasks is still lacking. In this study, we propose a multi-task segmentation
model with a contrastive learning based subtask and compare its performance
with other multi-task models, varying the number of labeled data for training.
We further extend our model so that it can utilize unlabeled data through the
regularization branch in a semi-supervised manner. We experimentally show that
our proposed method outperforms other multi-task methods including the
state-of-the-art fully supervised model when the amount of annotated data is
limited.
\\ ( https://arxiv.org/abs/2009.11160 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11204
Date: Wed, 23 Sep 2020 15:12:24 GMT   (4503kb,D)

Title: Learning Visual Voice Activity Detection with an Automatically Annotated
  Dataset
Authors: Sylvain Guy, St\'ephane Lathuili\`ere, Pablo Mesejo and Radu Horaud
Categories: cs.CV
Comments: International Conference on Pattern Recognition, Milan, Italy,
  January 2021
\\
  Visual voice activity detection (V-VAD) uses visual features to predict
whether a person is speaking or not. V-VAD is useful whenever audio VAD (A-VAD)
is inefficient either because the acoustic signal is difficult to analyze or
because it is simply missing. We propose two deep architectures for V-VAD, one
based on facial landmarks and one based on optical flow. Moreover, available
datasets, used for learning and for testing V-VAD, lack content variability. We
introduce a novel methodology to automatically create and annotate very large
datasets in-the-wild -- WildVVAD -- based on combining A-VAD with face
detection and tracking. A thorough empirical evaluation shows the advantage of
training the proposed deep V-VAD models with this dataset.
\\ ( https://arxiv.org/abs/2009.11204 ,  4503kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11232
Date: Wed, 23 Sep 2020 16:03:00 GMT   (11573kb,D)

Title: A Simple Yet Effective Method for Video Temporal Grounding with
  Cross-Modality Attention
Authors: Binjie Zhang, Yu Li, Chun Yuan, Dejing Xu, Pin Jiang, Ying Shan
Categories: cs.CV
Comments: 9 pages
\\
  The task of language-guided video temporal grounding is to localize the
particular video clip corresponding to a query sentence in an untrimmed video.
Though progress has been made continuously in this field, some issues still
need to be resolved. First, most of the existing methods rely on the
combination of multiple complicated modules to solve the task. Second, due to
the semantic gaps between the two different modalities, aligning the
information at different granularities (local and global) between the video and
the language is significant, which is less addressed. Last, previous works do
not consider the inevitable annotation bias due to the ambiguities of action
boundaries. To address these limitations, we propose a simple two-branch
Cross-Modality Attention (CMA) module with intuitive structure design, which
alternatively modulates two modalities for better matching the information both
locally and globally. In addition, we introduce a new task-specific regression
loss function, which improves the temporal grounding accuracy by alleviating
the impact of annotation bias. We conduct extensive experiments to validate our
method, and the results show that just with this simple model, it can
outperform the state of the arts on both Charades-STA and ActivityNet Captions
datasets.
\\ ( https://arxiv.org/abs/2009.11232 ,  11573kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11250
Date: Wed, 23 Sep 2020 16:57:28 GMT   (9484kb,D)

Title: Interactive Learning for Semantic Segmentation in Earth Observation
Authors: Gaston Lenczner, Adrien Chan-Hon-Tong, Nicola Luminari, Bertrand Le
  Saux, Guy Le Besnerais
Categories: cs.CV
Comments: 8 pages, 4 Figures, ECML-PKDD Workshop MACLEAN
\\
  Dense pixel-wise classification maps output by deep neural networks are of
extreme importance for scene understanding. However, these maps are often
partially inaccurate due to a variety of possible factors. Therefore, we
propose to interactively refine them within a framework named DISCA (Deep Image
Segmentation with Continual Adaptation). It consists of continually adapting a
neural network to a target image using an interactive learning process with
sparse user annotations as ground-truth. We show through experiments on three
datasets using synthesized annotations the benefits of the approach, reaching
an IoU improvement up to 4.7% for ten sampled clicks. Finally, we exhibit that
our approach can be particularly rewarding when it is faced to additional
issues such as domain adaptation.
\\ ( https://arxiv.org/abs/2009.11250 ,  9484kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11262
Date: Wed, 23 Sep 2020 17:19:19 GMT   (8368kb,D)

Title: A Linear Transportation $\mathrm{L}^p$ Distance for Pattern Recognition
Authors: Oliver M. Crook, Mihai Cucuringu, Tim Hurst, Carola-Bibiane
  Sch\"onlieb, Matthew Thorpe and Konstantinos C. Zygalakis
Categories: cs.CV math.OC
\\
  The transportation $\mathrm{L}^p$ distance, denoted $\mathrm{TL}^p$, has been
proposed as a generalisation of Wasserstein $\mathrm{W}^p$ distances motivated
by the property that it can be applied directly to colour or multi-channelled
images, as well as multivariate time-series without normalisation or mass
constraints. These distances, as with $\mathrm{W}^p$, are powerful tools in
modelling data with spatial or temporal perturbations. However, their
computational cost can make them infeasible to apply to even moderate pattern
recognition tasks. We propose linear versions of these distances and show that
the linear $\mathrm{TL}^p$ distance significantly improves over the linear
$\mathrm{W}^p$ distance on signal processing tasks, whilst being several orders
of magnitude faster to compute than the $\mathrm{TL}^p$ distance.
\\ ( https://arxiv.org/abs/2009.11262 ,  8368kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11278
Date: Wed, 23 Sep 2020 17:45:17 GMT   (26491kb,D)

Title: X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal
  Transformers
Authors: Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha
  Kembhavi
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: EMNLP 2020
\\
  Mirroring the success of masked language models, vision-and-language
counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art
performance on a variety of multimodal discriminative tasks like visual
question answering and visual grounding. Recent work has also successfully
adapted such models towards the generative task of image captioning. This begs
the question: Can these models go the other way and generate images from pieces
of text? Our analysis of a popular representative from this model family -
LXMERT - finds that it is unable to generate rich and semantically meaningful
imagery with its current training setup. We introduce X-LXMERT, an extension to
LXMERT with training refinements including: discretizing visual
representations, using uniform masking with a large range of masking ratios and
aligning the right pre-training datasets to the right objectives which enables
it to paint. X-LXMERT's image generation capabilities rival state of the art
generative models while its question answering and captioning abilities remains
comparable to LXMERT. Finally, we demonstrate the generality of these training
refinements by adding image generation capabilities into UNITER to produce
X-UNITER.
\\ ( https://arxiv.org/abs/2009.11278 ,  26491kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11279
Date: Wed, 23 Sep 2020 17:52:09 GMT   (8799kb,D)

Title: Augmented Convolutional LSTMs for Generation of High-Resolution Climate
  Change Projections
Authors: Nidhin Harilal, Udit Bhatia, Mayank Singh
Categories: cs.CV cs.LG physics.ao-ph
\\
  Projection of changes in extreme indices of climate variables such as
temperature and precipitation are critical to assess the potential impacts of
climate change on human-made and natural systems, including critical
infrastructures and ecosystems. While impact assessment and adaptation planning
rely on high-resolution projections (typically in the order of a few
kilometers), state-of-the-art Earth System Models (ESMs) are available at
spatial resolutions of few hundreds of kilometers. Current solutions to obtain
high-resolution projections of ESMs include downscaling approaches that
consider the information at a coarse-scale to make predictions at local scales.
Complex and non-linear interdependence among local climate variables (e.g.,
temperature and precipitation) and large-scale predictors (e.g., pressure
fields) motivate the use of neural network-based super-resolution
architectures. In this work, we present auxiliary variables informed
spatio-temporal neural architecture for statistical downscaling. The current
study performs daily downscaling of precipitation variable from an ESM output
at 1.15 degrees (~115 km) to 0.25 degrees (25 km) over the world's most
climatically diversified country, India. We showcase significant improvement
gain against three popular state-of-the-art baselines with a better ability to
predict extreme events. To facilitate reproducible research, we make available
all the codes, processed datasets, and trained models in the public domain.
\\ ( https://arxiv.org/abs/2009.11279 ,  8799kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10886
Date: Wed, 23 Sep 2020 01:27:38 GMT   (39kb)

Title: The Quotient in Preorder Theories
Authors: \'I\~nigo X. \'Incer Romeo (University of California, Berkeley),
  Leonardo Mangeruca (Raytheon Technologies Research Center, Rome, Italy),
  Tiziano Villa (Universit\`a di Verona, Italy), Alberto
  Sangiovanni-Vincentelli (University of California, Berkeley)
Categories: cs.DM cs.FL cs.LO
Comments: In Proceedings GandALF 2020, arXiv:2009.09360
Journal-ref: EPTCS 326, 2020, pp. 216-233
DOI: 10.4204/EPTCS.326.14
\\
  Seeking the largest solution to an expression of the form A x <= B is a
common task in several domains of engineering and computer science. This
largest solution is commonly called quotient. Across domains, the meanings of
the binary operation and the preorder are quite different, yet the syntax for
computing the largest solution is remarkably similar. This paper is about
finding a common framework to reason about quotients. We only assume we operate
on a preorder endowed with an abstract monotonic multiplication and an
involution. We provide a condition, called admissibility, which guarantees the
existence of the quotient, and which yields its closed form. We call preordered
heaps those structures satisfying the admissibility condition. We show that
many existing theories in computer science are preordered heaps, and we are
thus able to derive a quotient for them, subsuming existing solutions when
available in the literature. We introduce the concept of sieved heaps to deal
with structures which are given over multiple domains of definition. We show
that sieved heaps also have well-defined quotients.
\\ ( https://arxiv.org/abs/2009.10886 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10796
Date: Tue, 22 Sep 2020 20:23:32 GMT   (29536kb,D)

Title: Scaling Probe-Based Real-Time Dynamic Global Illumination for Production
Authors: Zander Majercik, Adam Marrs, Josef Spjut, Morgan McGuire
Categories: cs.GR
\\
  We contribute several practical extensions to the probe based
irradiance-field-with-visibility representation to improve image quality,
constant and asymptotic performance, memory efficiency, and artist control. We
developed these extensions in the process of incorporating the previous work
into the global illumination solutions of the NVIDIA RTXGI SDK, the Unity and
Unreal Engine 4 game engines, and proprietary engines for several commercial
games. These extensions include: a single, intuitive tuning parameter (the
"self-shadow" bias); heuristics to speed transitions in the global
illumination; reuse of irradiance data as prefiltered radiance for recursive
glossy reflection; a probe state machine to prune work that will not affect the
final image; and multiresolution cascaded volumes for large worlds.
\\ ( https://arxiv.org/abs/2009.10796 ,  29536kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11148
Date: Wed, 23 Sep 2020 13:50:11 GMT   (3138kb,D)

Title: Visualization of Human Spine Biomechanics for Spinal Surgery
Authors: Pepe Eulzer, Sabine Bauer, Francis Kilian, Kai Lawonn
Categories: cs.GR
Comments: 9+2 pages, 11 figures, to be published in IEEE Transactions on
  Visualization and Computer Graphics
\\
  We propose a visualization application, designed for the exploration of human
spine simulation data. Our goal is to support research in biomechanical spine
simulation and advance efforts to implement simulation-backed analysis in
surgical applications. Biomechanical simulation is a state-of-the-art technique
for analyzing load distributions of spinal structures. Through the inclusion of
patient-specific data, such simulations may facilitate personalized treatment
and customized surgical interventions. Difficulties in spine modelling and
simulation can be partly attributed to poor result representation, which may
also be a hindrance when introducing such techniques into a clinical
environment. Comparisons of measurements across multiple similar anatomical
structures and the integration of temporal data make commonly available
diagrams and charts insufficient for an intuitive and systematic display of
results. Therefore, we facilitate methods such as multiple coordinated views,
abstraction and focus and context to display simulation outcomes in a dedicated
tool. By linking the result data with patient-specific anatomy, we make
relevant parameters tangible for clinicians. Furthermore, we introduce new
concepts to show the directions of impact force vectors, which were not
accessible before. We integrated our toolset into a spine segmentation and
simulation pipeline and evaluated our methods with both surgeons and
biomechanical researchers. When comparing our methods against standard
representations that are currently in use, we found increases in accuracy and
speed in data exploration tasks. In a qualitative review, domain experts deemed
the tool highly useful when dealing with simulation result data, which
typically combines time-dependent patient movement and the resulting force
distributions on spinal structures.
\\ ( https://arxiv.org/abs/2009.11148 ,  3138kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2009.10765 (*cross-listing*)
Date: Tue, 22 Sep 2020 19:04:02 GMT   (1341kb,D)

Title: Age-Net: An MRI-Based Iterative Framework for Biological Age Estimation
Authors: Karim Armanious, Sherif Abdulatif, Wenbin Shi, Shashank Salian, Thomas
  K\"ustner, Daniel Weiskopf, Tobias Hepp, Sergios Gatidis, Bin Yang
Categories: eess.IV cs.CV
Comments: 10 pages, 11 figures, 2 tables
\\
  The concept of biological age (BA) - although important in clinical practice
- is hard to grasp mainly due to lack of a clearly defined reference standard.
For specific applications, especially in pediatrics, medical image data are
used for BA estimation in a routine clinical context. Beyond this young age
group, BA estimation is restricted to whole-body assessment using non-imaging
indicators such as blood biomarkers, genetic and cellular data. However,
various organ systems may exhibit different aging characteristics due to
lifestyle and genetic factors. Thus, a whole-body assessment of the BA does not
reflect the deviations of aging behavior between organs. To this end, we
propose a new imaging-based framework for organ-specific BA estimation. As a
first step, we introduce a chronological age (CA) estimation framework using
deep convolutional neural networks (Age-Net). We quantitatively assess the
performance of this framework in comparison to existing CA estimation
approaches. Furthermore, we expand upon Age-Net with a novel iterative
data-cleaning algorithm to segregate atypical-aging patients (BA $\not \approx$
CA) from the given population. In this manner, we hypothesize that the
remaining population should approximate the true BA behaviour. For this initial
study, we apply the proposed methodology on a brain magnetic resonance image
(MRI) dataset containing healthy individuals as well as Alzheimer's patients
with different dementia ratings. We demonstrate the correlation between the
predicted BAs and the expected cognitive deterioration in Alzheimer's patients.
A statistical and visualization-based analysis has provided evidence regarding
the potential and current challenges of the proposed methodology.
\\ ( https://arxiv.org/abs/2009.10765 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10769 (*cross-listing*)
Date: Tue, 22 Sep 2020 19:16:16 GMT   (2127kb,D)

Title: Cranial Implant Prediction using Low-Resolution 3D Shape Completion and
  High-Resolution 2D Refinement
Authors: Amirhossein Bayat, Suprosanna Shit, Adrian Kilian, J\"urgen T.
  Liechtenstein, Jan S. Kirschke, Bjoern H. Menze
Categories: eess.IV cs.CV
\\
  Designing of a cranial implant needs a 3D understanding of the complete skull
shape. Thus, taking a 2D approach is sub-optimal, since a 2D model lacks a
holistic 3D view of both the defective and healthy skulls. Further, loading the
whole 3D skull shapes at its original image resolution is not feasible in
commonly available GPUs. To mitigate these issues, we propose a fully
convolutional network composed of two subnetworks. The first subnetwork is
designed to complete the shape of the downsampled defective skull. The second
subnetwork upsamples the reconstructed shape slice-wise. We train the 3D and 2D
networks together end-to-end, with a hierarchical loss function. Our proposed
solution accurately predicts a high-resolution 3D implant in the challenge test
case in terms of dice-score and the Hausdorff distance.
\\ ( https://arxiv.org/abs/2009.10769 ,  2127kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10804 (*cross-listing*)
Date: Tue, 22 Sep 2020 20:44:20 GMT   (9405kb,D)

Title: Adaptive Debanding Filter
Authors: Zhengzhong Tu, Jessie Lin, Yilin Wang, Balu Adsumilli, and Alan C.
  Bovik
Categories: eess.IV cs.CV
Comments: 4 pages, 7 figures, 1 table. Accepted to IEEE Signal Processing
  Letters
\\
  Banding artifacts, which manifest as staircase-like color bands on pictures
or video frames, is a common distortion caused by compression of low-textured
smooth regions. These false contours can be very noticeable even on
high-quality videos, especially when displayed on high-definition screens. Yet,
relatively little attention has been applied to this problem. Here we consider
banding artifact removal as a visual enhancement problem, and accordingly, we
solve it by applying a form of content-adaptive smoothing filtering followed by
dithered quantization, as a post-processing module. The proposed debanding
filter is able to adaptively smooth banded regions while preserving image edges
and details, yielding perceptually enhanced gradient rendering with limited
bit-depths. Experimental results show that our proposed debanding filter
outperforms state-of-the-art false contour removing algorithms both visually
and quantitatively.
\\ ( https://arxiv.org/abs/2009.10804 ,  9405kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10858 (*cross-listing*)
Date: Tue, 22 Sep 2020 23:32:59 GMT   (1061kb,D)

Title: Improving Medical Annotation Quality to Decrease Labeling Burden Using
  Stratified Noisy Cross-Validation
Authors: Joy Hsu, Sonia Phene, Akinori Mitani, Jieying Luo, Naama Hammel,
  Jonathan Krause, Rory Sayres
Categories: cs.LG cs.CV eess.IV
Journal-ref: ACM Conference on Health, Inference, and Learning, April 02-04,
  2020, Toronto, Canada
\\
  As machine learning has become increasingly applied to medical imaging data,
noise in training labels has emerged as an important challenge. Variability in
diagnosis of medical images is well established; in addition, variability in
training and attention to task among medical labelers may exacerbate this
issue. Methods for identifying and mitigating the impact of low quality labels
have been studied, but are not well characterized in medical imaging tasks. For
instance, Noisy Cross-Validation splits the training data into halves, and has
been shown to identify low-quality labels in computer vision tasks; but it has
not been applied to medical imaging tasks specifically. In this work we
introduce Stratified Noisy Cross-Validation (SNCV), an extension of noisy cross
validation. SNCV can provide estimates of confidence in model predictions by
assigning a quality score to each example; stratify labels to handle class
imbalance; and identify likely low-quality labels to analyze the causes. We
assess performance of SNCV on diagnosis of glaucoma suspect risk from retinal
fundus photographs, a clinically important yet nuanced labeling task. Using
training data from a previously-published deep learning model, we compute a
continuous quality score (QS) for each training example. We relabel 1,277
low-QS examples using a trained glaucoma specialist; the new labels agree with
the SNCV prediction over the initial label >85% of the time, indicating that
low-QS examples mostly reflect labeler errors. We then quantify the impact of
training with only high-QS labels, showing that strong model performance may be
obtained with many fewer examples. By applying the method to randomly
sub-sampled training dataset, we show that our method can reduce labelling
burden by approximately 50% while achieving model performance non-inferior to
using the full dataset on multiple held-out test sets.
\\ ( https://arxiv.org/abs/2009.10858 ,  1061kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10893 (*cross-listing*)
Date: Wed, 23 Sep 2020 01:51:47 GMT   (239kb,D)

Title: Pruning Convolutional Filters using Batch Bridgeout
Authors: Najeeb Khan and Ian Stavness
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  State-of-the-art computer vision models are rapidly increasing in capacity,
where the number of parameters far exceeds the number required to fit the
training set. This results in better optimization and generalization
performance. However, the huge size of contemporary models results in large
inference costs and limits their use on resource-limited devices. In order to
reduce inference costs, convolutional filters in trained neural networks could
be pruned to reduce the run-time memory and computational requirements during
inference. However, severe post-training pruning results in degraded
performance if the training algorithm results in dense weight vectors. We
propose the use of Batch Bridgeout, a sparsity inducing stochastic
regularization scheme, to train neural networks so that they could be pruned
efficiently with minimal degradation in performance. We evaluate the proposed
method on common computer vision models VGGNet, ResNet, and Wide-ResNet on the
CIFAR image classification task. For all the networks, experimental results
show that Batch Bridgeout trained networks achieve higher accuracy across a
wide range of pruning intensities compared to Dropout and weight decay
regularization.
\\ ( https://arxiv.org/abs/2009.10893 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10978 (*cross-listing*)
Date: Wed, 23 Sep 2020 07:42:14 GMT   (828kb,D)

Title: Semantics-Preserving Adversarial Training
Authors: Wonseok Lee, Hanbit Lee, Sang-goo Lee
Categories: cs.LG cs.CV stat.ML
Comments: Preprint. Under Review
\\
  Adversarial training is a defense technique that improves adversarial
robustness of a deep neural network (DNN) by including adversarial examples in
the training data. In this paper, we identify an overlooked problem of
adversarial training in that these adversarial examples often have different
semantics than the original data, introducing unintended biases into the model.
We hypothesize that such non-semantics-preserving (and resultingly ambiguous)
adversarial data harm the robustness of the target models. To mitigate such
unintended semantic changes of adversarial examples, we propose
semantics-preserving adversarial training (SPAT) which encourages perturbation
on the pixels that are shared among all classes when generating adversarial
examples in the training stage. Experiment results show that SPAT improves
adversarial robustness and achieves state-of-the-art results in CIFAR-10 and
CIFAR-100.
\\ ( https://arxiv.org/abs/2009.10978 ,  828kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10987 (*cross-listing*)
Date: Wed, 23 Sep 2020 07:59:49 GMT   (2411kb,D)

Title: Learning Non-Unique Segmentation with Reward-Penalty Dice Loss
Authors: Jiabo He, Sarah Erfani, Sudanthi Wijewickrema, Stephen O'Leary,
  Kotagiri Ramamohanarao
Categories: eess.IV cs.CV
\\
  Semantic segmentation is one of the key problems in the field of computer
vision, as it enables computer image understanding. However, most research and
applications of semantic segmentation focus on addressing unique segmentation
problems, where there is only one gold standard segmentation result for every
input image. This may not be true in some problems, e.g., medical applications.
We may have non-unique segmentation annotations as different surgeons may
perform successful surgeries for the same patient in slightly different ways.
To comprehensively learn non-unique segmentation tasks, we propose the
reward-penalty Dice loss (RPDL) function as the optimization objective for deep
convolutional neural networks (DCNN). RPDL is capable of helping DCNN learn
non-unique segmentation by enhancing common regions and penalizing outside
ones. Experimental results show that RPDL improves the performance of DCNN
models by up to 18.4% compared with other loss functions on our collected
surgical dataset.
\\ ( https://arxiv.org/abs/2009.10987 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11008 (*cross-listing*)
Date: Wed, 23 Sep 2020 09:05:24 GMT   (5870kb,D)

Title: Attention with Multiple Sources Knowledges for COVID-19 from CT Images
Authors: Duy M. H. Nguyen, Duy M. Nguyen, Huong Vu, Binh T. Nguyen, Fabrizio
  Nunnari, Daniel Sonntag
Categories: eess.IV cs.CV cs.HC
Comments: Version 1
\\
  Until now, Coronavirus SARS-CoV-2 has caused more than 850,000 deaths and
infected more than 27 million individuals in over 120 countries. Besides
principal polymerase chain reaction (PCR) tests, automatically identifying
positive samples based on computed tomography (CT) scans can present a
promising option in the early diagnosis of COVID-19. Recently, there have been
increasing efforts to utilize deep networks for COVID-19 diagnosis based on CT
scans. While these approaches mostly focus on introducing novel architectures,
transfer learning techniques, or construction large scale data, we propose a
novel strategy to improve the performance of several baselines by leveraging
multiple useful information sources relevant to doctors' judgments.
Specifically, infected regions and heat maps extracted from learned networks
are integrated with the global image via an attention mechanism during the
learning process. This procedure not only makes our system more robust to noise
but also guides the network focusing on local lesion areas. Extensive
experiments illustrate the superior performance of our approach compared to
recent baselines. Furthermore, our learned network guidance presents an
explainable feature to doctors as we can understand the connection between
input and output in a grey-box model.
\\ ( https://arxiv.org/abs/2009.11008 ,  5870kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11009 (*cross-listing*)
Date: Wed, 23 Sep 2020 09:08:24 GMT   (2390kb,D)

Title: Automatic Breast Lesion Classification by Joint Neural Analysis of
  Mammography and Ultrasound
Authors: Gavriel Habib, Nahum Kiryati, Miri Sklair-Levy, Anat Shalmon, Osnat
  Halshtok Neiman, Renata Faermann Weidenfeld, Yael Yagil, Eli Konen, Arnaldo
  Mayer
Categories: eess.IV cs.CV
Comments: 10 pages including references, 8 figures, 1 table. Accepted to MICCAI
  ML-CDS 2020 workshop (ML-CDS 2020/CLIP 2020, LNCS 12445 proceedings)
\\
  Mammography and ultrasound are extensively used by radiologists as
complementary modalities to achieve better performance in breast cancer
diagnosis. However, existing computer-aided diagnosis (CAD) systems for the
breast are generally based on a single modality. In this work, we propose a
deep-learning based method for classifying breast cancer lesions from their
respective mammography and ultrasound images. We present various approaches and
show a consistent improvement in performance when utilizing both modalities.
The proposed approach is based on a GoogleNet architecture, fine-tuned for our
data in two training steps. First, a distinct neural network is trained
separately for each modality, generating high-level features. Then, the
aggregated features originating from each modality are used to train a
multimodal network to provide the final classification. In quantitative
experiments, the proposed approach achieves an AUC of 0.94, outperforming
state-of-the-art models trained over a single modality. Moreover, it performs
similarly to an average radiologist, surpassing two out of four radiologists
participating in a reader study. The promising results suggest that the
proposed method may become a valuable decision support tool for breast
radiologists.
\\ ( https://arxiv.org/abs/2009.11009 ,  2390kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11080 (*cross-listing*)
Date: Wed, 23 Sep 2020 12:02:55 GMT   (8037kb,D)

Title: GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution
  from Low-Resolution Functional Brain Connectomes
Authors: Megi Isallari and Islem Rekik
Categories: eess.IV cs.CV
\\
  Catchy but rigorous deep learning architectures were tailored for image
super-resolution (SR), however, these fail to generalize to non-Euclidean data
such as brain connectomes. Specifically, building generative models for
super-resolving a low-resolution (LR) brain connectome at a higher resolution
(HR) (i.e., adding new graph nodes/edges) remains unexplored although this
would circumvent the need for costly data collection and manual labelling of
anatomical brain regions (i.e. parcellation). To fill this gap, we introduce
GSR-Net (Graph Super-Resolution Network), the first super-resolution framework
operating on graph-structured data that generates high-resolution brain graphs
from low-resolution graphs. First, we adopt a U-Net like architecture based on
graph convolution, pooling and unpooling operations specific to non-Euclidean
data. However, unlike conventional U-Nets where graph nodes represent samples
and node features are mapped to a low-dimensional space (encoding and decoding
node attributes or sample features), our GSR-Net operates directly on a single
connectome: a fully connected graph where conventionally, a node denotes a
brain region, nodes have no features, and edge weights denote brain
connectivity strength between two regions of interest (ROIs). In the absence of
original node features, we initially assign identity feature vectors to each
brain ROI (node) and then leverage the learned local receptive fields to learn
node feature representations. Second, inspired by spectral theory, we break the
symmetry of the U-Net architecture by topping it up with a graph
super-resolution (GSR) layer and two graph convolutional network layers to
predict a HR graph while preserving the characteristics of the LR input. Our
proposed GSR-Net framework outperformed its variants for predicting
high-resolution brain functional connectomes from low-resolution connectomes.
\\ ( https://arxiv.org/abs/2009.11080 ,  8037kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11090 (*cross-listing*)
Date: Wed, 23 Sep 2020 12:18:05 GMT   (1076kb,D)

Title: Robustification of Segmentation Models Against Adversarial Perturbations
  In Medical Imaging
Authors: Hanwool Park, Amirhossein Bayat, Mohammad Sabokrou, Jan S. Kirschke,
  Bjoern H. Menze
Categories: eess.IV cs.CV
\\
  This paper presents a novel yet efficient defense framework for segmentation
models against adversarial attacks in medical imaging. In contrary to the
defense methods against adversarial attacks for classification models which
widely are investigated, such defense methods for segmentation models has been
less explored. Our proposed method can be used for any deep learning models
without revising the target deep learning models, as well as can be independent
of adversarial attacks. Our framework consists of a frequency domain converter,
a detector, and a reformer. The frequency domain converter helps the detector
detects adversarial examples by using a frame domain of an image. The reformer
helps target models to predict more precisely. We have experiments to
empirically show that our proposed method has a better performance compared to
the existing defense method.
\\ ( https://arxiv.org/abs/2009.11090 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11120 (*cross-listing*)
Date: Wed, 23 Sep 2020 12:56:14 GMT   (9583kb,D)

Title: Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from
  Multi-Planar MRI
Authors: Anneke Meyer, Grzegorz Chlebus, Marko Rak, Daniel Schindele, Martin
  Schostak, Bram van Ginneken, Andrea Schenk, Hans Meine, Horst K. Hahn,
  Andreas Schreiber, Christian Hansen
Categories: eess.IV cs.CV
Comments: Submitted to Elsevier Computer Methods and Programs in Biomedicine.
  Anneke Meyer and Grzegorz Chlebus contributed equally to this work.
  Sourcecode and data available at
  https://github.com/AnnekeMeyer/AnisotropicMultiStreamCNN
\\
  Background and Objective: Accurate and reliable segmentation of the prostate
gland in MR images can support the clinical assessment of prostate cancer, as
well as the planning and monitoring of focal and loco-regional therapeutic
interventions. Despite the availability of multi-planar MR scans due to
standardized protocols, the majority of segmentation approaches presented in
the literature consider the axial scans only. Methods: We propose an
anisotropic 3D multi-stream CNN architecture, which processes additional scan
directions to produce a higher-resolution isotropic prostate segmentation. We
investigate two variants of our architecture, which work on two (dual-plane)
and three (triple-plane) image orientations, respectively. We compare them with
the standard baseline (single-plane) used in literature, i.e., plain axial
segmentation. To realize a fair comparison, we employ a hyperparameter
optimization strategy to select optimal configurations for the individual
approaches. Results: Training and evaluation on two datasets spanning multiple
sites obtain statistical significant improvement over the plain axial
segmentation ($p<0.05$ on the Dice similarity coefficient). The improvement can
be observed especially at the base ($0.898$ single-plane vs. $0.906$
triple-plane) and apex ($0.888$ single-plane vs. $0.901$ dual-plane).
Conclusion: This study indicates that models employing two or three scan
directions are superior to plain axial segmentation. The knowledge of precise
boundaries of the prostate is crucial for the conservation of risk structures.
Thus, the proposed models have the potential to improve the outcome of prostate
cancer diagnosis and therapies.
\\ ( https://arxiv.org/abs/2009.11120 ,  9583kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11166 (*cross-listing*)
Date: Wed, 23 Sep 2020 14:25:40 GMT   (1729kb,D)

Title: Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial
  Network Normalizer
Authors: Zeynep Gurler, Ahmed Nebli and Islem Rekik
Categories: eess.IV cs.CV
\\
  Foreseeing the brain evolution as a complex highly inter-connected system,
widely modeled as a graph, is crucial for mapping dynamic interactions between
different anatomical regions of interest (ROIs) in health and disease.
Interestingly, brain graph evolution models remain almost absent in the
literature. Here we design an adversarial brain network normalizer for
representing each brain network as a transformation of a fixed centered
population-driven connectional template. Such graph normalization with respect
to a fixed reference paves the way for reliably identifying the most similar
training samples (i.e., brain graphs) to the testing sample at baseline
timepoint. The testing evolution trajectory will be then spanned by the
selected training graphs and their corresponding evolution trajectories. We
base our prediction framework on geometric deep learning which naturally
operates on graphs and nicely preserves their topological properties.
Specifically, we propose the first graph-based Generative Adversarial Network
(gGAN) that not only learns how to normalize brain graphs with respect to a
fixed connectional brain template (CBT) (i.e., a brain template that
selectively captures the most common features across a brain population) but
also learns a high-order representation of the brain graphs also called
embeddings. We use these embeddings to compute the similarity between training
and testing subjects which allows us to pick the closest training subjects at
baseline timepoint to predict the evolution of the testing brain graph over
time. A series of benchmarks against several comparison methods showed that our
proposed method achieved the lowest brain disease evolution prediction error
using a single baseline timepoint. Our gGAN code is available at
http://github.com/basiralab/gGAN.
\\ ( https://arxiv.org/abs/2009.11166 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11169 (*cross-listing*)
Date: Wed, 23 Sep 2020 14:31:15 GMT   (26583kb,D)

Title: Whole Slide Images based Cancer Survival Prediction using Attention
  Guided Deep Multiple Instance Learning Networks
Authors: Jiawen Yao, Xinliang Zhu, Jitendra Jonnagaddala, Nicholas Hawkins,
  Junzhou Huang
Categories: eess.IV cs.CV
Comments: 22 pages, 13 figures, published in Medical Image Analysis 65, 101789
Journal-ref: Medical Image Analysis, Volume 65, October 2020, 101789
DOI: 10.1016/j.media.2020.101789
\\
  Traditional image-based survival prediction models rely on discriminative
patch labeling which make those methods not scalable to extend to large
datasets. Recent studies have shown Multiple Instance Learning (MIL) framework
is useful for histopathological images when no annotations are available in
classification task. Different to the current image-based survival models that
limit to key patches or clusters derived from Whole Slide Images (WSIs), we
propose Deep Attention Multiple Instance Survival Learning (DeepAttnMISL) by
introducing both siamese MI-FCN and attention-based MIL pooling to efficiently
learn imaging features from the WSI and then aggregate WSI-level information to
patient-level. Attention-based aggregation is more flexible and adaptive than
aggregation techniques in recent survival models. We evaluated our methods on
two large cancer whole slide images datasets and our results suggest that the
proposed approach is more effective and suitable for large datasets and has
better interpretability in locating important patterns and features that
contribute to accurate cancer survival predictions. The proposed framework can
also be used to assess individual patient's risk and thus assisting in
delivering personalized medicine. Codes are available at
https://github.com/uta-smile/DeepAttnMISL_MEDIA.
\\ ( https://arxiv.org/abs/2009.11169 ,  26583kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11253 (*cross-listing*)
Date: Wed, 23 Sep 2020 17:01:09 GMT   (1691kb,D)

Title: Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task
  Generalization in Few-shot Learning
Authors: Henry Kvinge, Zachary New, Nico Courts, Jung H. Lee, Lauren A.
  Phillips, Courtney D. Corley, Aaron Tuor, Andrew Avila, Nathan O. Hodas
Categories: cs.LG cs.AI cs.CV math.GN stat.ML
Comments: 17 pages
\\
  Deep learning has shown great success in settings with massive amounts of
data but has struggled when data is limited. Few-shot learning algorithms,
which seek to address this limitation, are designed to generalize well to new
tasks with limited data. Typically, models are evaluated on unseen classes and
datasets that are defined by the same fundamental task as they are trained for
(e.g. category membership). One can also ask how well a model can generalize to
fundamentally different tasks within a fixed dataset (for example: moving from
category membership to tasks that involve detecting object orientation or
quantity). To formalize this kind of shift we define a notion of "independence
of tasks" and identify three new sets of labels for established computer vision
datasets that test a model's ability to generalize to tasks which draw on
orthogonal attributes in the data. We use these datasets to investigate the
failure modes of metric-based few-shot models. Based on our findings, we
introduce a new few-shot model called Fuzzy Simplicial Networks (FSN) which
leverages a construction from topology to more flexibly represent each class
from limited data. In particular, FSN models can not only form multiple
representations for a given class but can also begin to capture the
low-dimensional structure which characterizes class manifolds in the encoded
space of deep networks. We show that FSN outperforms state-of-the-art models on
the challenging tasks we introduce in this paper while remaining competitive on
standard few-shot benchmarks.
\\ ( https://arxiv.org/abs/2009.11253 ,  1691kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11056 (*cross-listing*)
Date: Wed, 23 Sep 2020 11:21:44 GMT   (6kb)

Title: A simple $(2+\epsilon)$-approximation algorithm for Split Vertex
  Deletion
Authors: Matthew Drescher, Samuel Fiorini, Tony Huynh
Categories: math.CO cs.DM
Comments: 3 pages, 0 figures
MSC-class: 05C17, 68W25, 05C69
\\
  A split graph is a graph whose vertex set can be partitioned into a clique
and a stable set. Given a graph $G$ and weight function $w: V(G) \to
\mathbb{Q}_{\geq 0}$, the Split Vertex Deletion (SVD) problem asks to find a
minimum weight set of vertices $X$ such that $G-X$ is a split graph. It is easy
to show that a graph is a split graph if and only it it does not contain a
$4$-cycle, $5$-cycle, or a two edge matching as an induced subgraph. Therefore,
SVD admits an easy $5$-approximation algorithm. On the other hand, for every
$\delta >0$, SVD does not admit a $(2-\delta)$-approximation algorithm, unless
P=NP or the Unique Games Conjecture fails.
  For every $\epsilon >0$, Lokshtanov, Misra, Panolan, Philip, and Saurabh
recently gave a randomized $(2+\epsilon)$-approximation algorithm for SVD. In
this work we give an extremely simple deterministic
$(2+\epsilon)$-approximation algorithm for SVD.
\\ ( https://arxiv.org/abs/2009.11056 ,  6kb)
------------------------------------------------------------------------------
\\
arXiv:2009.11133 (*cross-listing*)
Date: Wed, 23 Sep 2020 13:19:44 GMT   (9kb)

Title: Bounds on the Spectral Sparsification of Symmetric and Off-Diagonal
  Nonnegative Real Matrices
Authors: Sergio Mercado and Marcos Villagra
Categories: cs.DS cs.DM math.SP
Comments: 9 pages
MSC-class: 05C50, 68R01, 15B57, 15A42
\\
  We say that a square real matrix $M$ is \emph{off-diagonal nonnegative} if
and only if all entries outside its diagonal are nonnegative real numbers. In
this note we show that for any off-diagonal nonnegative symmetric matrix $M$,
there exists a nonnegative symmetric matrix $\widehat{M}$ which is sparse and
close in spectrum to $M$.
\\ ( https://arxiv.org/abs/2009.11133 ,  9kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1703.04394
replaced with revised version Wed, 23 Sep 2020 14:55:12 GMT   (253kb,D)

Title: Zero-Shot Learning -- The Good, the Bad and the Ugly
Authors: Yongqin Xian, Bernt Schiele, Zeynep Akata
Categories: cs.CV
Comments: Accepted as a full paper in IEEE Computer Vision and Pattern
  Recognition (CVPR) 2017. We introduce Proposed Split Version 2.0 (Please
  download it from the project page)
\\ ( https://arxiv.org/abs/1703.04394 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:1806.06406
replaced with revised version Wed, 23 Sep 2020 01:17:57 GMT   (1622kb,D)

Title: Fast Kernelized Correlation Filters without Boundary Effect
Authors: Ming Tang, Linyu Zheng, Bin Yu, Jinqiao Wang
Categories: cs.CV
Comments: A minor revision of its last version. 11+6 pages, 4+2 figures, 5
  tables
\\ ( https://arxiv.org/abs/1806.06406 ,  1622kb)
------------------------------------------------------------------------------
\\
arXiv:1809.02226
replaced with revised version Wed, 23 Sep 2020 15:30:27 GMT   (6575kb,D)

Title: Content-based Propagation of User Markings for Interactive Segmentation
  of Patterned Images
Authors: Vedrana Andersen Dahl, Camilla Himmelstrup Trinderup, Monica Jane
  Emerson and Anders Bjorholm Dahl
Categories: cs.CV
Comments: 9 pages, 7 figures, PDFLaTeX
Journal-ref: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, June 2020
\\ ( https://arxiv.org/abs/1809.02226 ,  6575kb)
------------------------------------------------------------------------------
\\
arXiv:1902.07776
replaced with revised version Wed, 23 Sep 2020 07:37:46 GMT   (1494kb,D)

Title: Perceptual Quality-preserving Black-Box Attack against Deep Learning
  Image Classifiers
Authors: Diego Gragnaniello, Francesco Marra, Giovanni Poggi and Luisa
  Verdoliva
Categories: cs.CV
Comments: 8 pages, journal
\\ ( https://arxiv.org/abs/1902.07776 ,  1494kb)
------------------------------------------------------------------------------
\\
arXiv:1909.02068
replaced with revised version Wed, 23 Sep 2020 05:47:34 GMT   (2945kb,D)

Title: ApproxNet: Content and Contention-Aware Video Analytics System for
  Embedded Clients
Authors: Ran Xu, Rakesh Kumar, Pengcheng Wang, Peter Bai, Ganga Meghanath,
  Somali Chaterji, Subrata Mitra, Saurabh Bagchi
Categories: cs.CV eess.IV
Comments: This paper is under revision for TOSN
\\ ( https://arxiv.org/abs/1909.02068 ,  2945kb)
------------------------------------------------------------------------------
\\
arXiv:1910.11012
replaced with revised version Wed, 23 Sep 2020 05:12:54 GMT   (494kb,D)

Title: Multi-label Co-regularization for Semi-supervised Facial Action Unit
  Recognition
Authors: Xuesong Niu and Hu Han and Shiguang Shan and Xilin Chen
Categories: cs.CV
\\ ( https://arxiv.org/abs/1910.11012 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:1910.11792
replaced with revised version Wed, 23 Sep 2020 05:07:16 GMT   (34451kb,D)

Title: JRDB: A Dataset and Benchmark of Egocentric Visual Perception for
  Navigation in Human Environments
Authors: Roberto Mart\'in-Mart\'in, Mihir Patel, Hamid Rezatofighi, Abhijeet
  Shenoi, JunYoung Gwak, Eric Frankel, Amir Sadeghian, Silvio Savarese
Categories: cs.CV cs.RO
\\ ( https://arxiv.org/abs/1910.11792 ,  34451kb)
------------------------------------------------------------------------------
\\
arXiv:2004.11968
replaced with revised version Wed, 23 Sep 2020 03:46:45 GMT   (9719kb,D)

Title: Visible fingerprint of X-ray images of epoxy resins using singular value
  decomposition of deep learning features
Authors: Edgar Avalos, Kazuto Akagi and Yasumasa Nishiura
Categories: cs.CV
Comments: 43 pages, 16 figures
Journal-ref: COMMAT Volume 186, January 2021, 109996
DOI: 10.1016/j.commatsci.2020.109996
\\ ( https://arxiv.org/abs/2004.11968 ,  9719kb)
------------------------------------------------------------------------------
\\
arXiv:2005.03230
replaced with revised version Wed, 23 Sep 2020 00:42:39 GMT   (924kb,D)

Title: Hierarchical Predictive Coding Models in a Deep-Learning Framework
Authors: Matin Hosseini, Anthony Maida
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2005.03230 ,  924kb)
------------------------------------------------------------------------------
\\
arXiv:2005.04619
replaced with revised version Wed, 23 Sep 2020 00:51:20 GMT   (1105kb,D)

Title: A Unified Weight Learning and Low-Rank Regression Model for Robust
  Complex Error Modeling
Authors: Miaohua Zhang, Yongsheng Gao, and Jun Zhou
Categories: cs.CV
\\ ( https://arxiv.org/abs/2005.04619 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2005.10353
replaced with revised version Tue, 22 Sep 2020 22:54:45 GMT   (6654kb,D)

Title: WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose
Authors: Yijun Zhou, James Gregson
Categories: cs.CV
Comments: Accepted at BMVC 2020
\\ ( https://arxiv.org/abs/2005.10353 ,  6654kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12212
replaced with revised version Wed, 23 Sep 2020 11:41:12 GMT   (1011kb,D)

Title: ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions
Authors: Anurag Roy, Vinay Kumar Verma, Kripabandhu Ghosh, Saptarshi Ghosh
Categories: cs.CV cs.CL cs.IR cs.LG
Comments: Accepted in CIKM-2020
\\ ( https://arxiv.org/abs/2007.12212 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2007.14226
replaced with revised version Tue, 22 Sep 2020 19:34:50 GMT   (358kb,D)

Title: A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption
  2020 Task
Authors: Marimuthu Kalimuthu, Fabrizio Nunnari, Daniel Sonntag
Categories: cs.CV cs.LG
Comments: Camera-ready version for ImageCLEF-2020
\\ ( https://arxiv.org/abs/2007.14226 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2008.00923
replaced with revised version Wed, 23 Sep 2020 09:05:32 GMT   (5142kb,D)

Title: Cross-Domain Facial Expression Recognition: A Unified Evaluation
  Benchmark and Adversarial Graph Learning
Authors: Tianshui Chen, Tao Pu, Yuan Xie, Hefeng Wu, Lingbo Liu, Liang Lin
Categories: cs.CV
Comments: Extension of our ACM MM 2020 paper. arXiv admin note: substantial
  text overlap with arXiv:2008.00859
\\ ( https://arxiv.org/abs/2008.00923 ,  5142kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05196
replaced with revised version Wed, 23 Sep 2020 03:29:15 GMT   (1175kb,D)

Title: RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion
  Judgement and Objective AU Annotations
Authors: Wenjing Yan, Shan Li, Chengtao Que, JiQuan Pei, Weihong Deng
Categories: cs.CV
Comments: ACCV 2020
\\ ( https://arxiv.org/abs/2008.05196 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2009.05609
replaced with revised version Wed, 23 Sep 2020 15:47:54 GMT   (3702kb,D)

Title: Deep Hiearchical Multi-Label Classification Applied to Chest X-Ray
  Abnormality Taxonomies
Authors: Haomin Chen, Shun Miao, Daguang Xu, Gregory D. Hager, Adam P. Harrison
Categories: cs.CV cs.AI
Journal-ref: MEDIMA 101811, 5 September 2020
\\ ( https://arxiv.org/abs/2009.05609 ,  3702kb)
------------------------------------------------------------------------------
\\
arXiv:2009.07386
replaced with revised version Tue, 22 Sep 2020 22:59:02 GMT   (7822kb,D)

Title: Creation and Validation of a Chest X-Ray Dataset with Eye-tracking and
  Report Dictation for AI Development
Authors: Alexandros Karargyris, Satyananda Kashyap, Ismini Lourentzou, Joy Wu,
  Arjun Sharma, Matthew Tong, Shafiq Abedin, David Beymer, Vandana Mukherjee,
  Elizabeth A Krupinski, Mehdi Moradi
Categories: cs.CV
\\ ( https://arxiv.org/abs/2009.07386 ,  7822kb)
------------------------------------------------------------------------------
\\
arXiv:2009.09976
replaced with revised version Wed, 23 Sep 2020 09:45:21 GMT   (3929kb,D)

Title: Depth-Adapted CNN for RGB-D cameras
Authors: Zongwei Wu, Guillaume Allibert, Christophe Stolz, Cedric Demonceaux
Categories: cs.CV
Comments: Accepted manuscript in ACCV 2020 (Oral)
\\ ( https://arxiv.org/abs/2009.09976 ,  3929kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10468
replaced with revised version Wed, 23 Sep 2020 07:51:39 GMT   (1017kb)

Title: Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories
  Prediction
Authors: Xiong Dan
Categories: cs.CV cs.AI
Comments: 12 pages, 5 figures
\\ ( https://arxiv.org/abs/2009.10468 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10569
replaced with revised version Wed, 23 Sep 2020 08:18:00 GMT   (20041kb,D)

Title: Improving Point Cloud Semantic Segmentation by Learning 3D Object
  Proposal Generation
Authors: Ozan Unal, Luc Van Gool, Dengxin Dai
Categories: cs.CV
Comments: 8 pages, 4 figures, 5 tables
\\ ( https://arxiv.org/abs/2009.10569 ,  20041kb)
------------------------------------------------------------------------------
\\
arXiv:1902.06155
replaced with revised version Tue, 22 Sep 2020 19:09:17 GMT   (206kb,D)

Title: Deep Generalized Convolutional Sum-Product Networks
Authors: Jos van de Wolfshaar, Andrzej Pronobis
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/1902.06155 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:1912.13256
replaced with revised version Tue, 22 Sep 2020 18:47:42 GMT   (6566kb,D)

Title: Scalable NAS with Factorizable Architectural Parameters
Authors: Lanfei Wang and Lingxi Xie and Tianyi Zhang and Jun Guo and Qi Tian
Categories: cs.LG cs.CV stat.ML
Comments: 10 pages, 4 figures
\\ ( https://arxiv.org/abs/1912.13256 ,  6566kb)
------------------------------------------------------------------------------
\\
arXiv:2001.10420
replaced with revised version Wed, 23 Sep 2020 14:06:47 GMT   (1311kb,D)

Title: OPFython: A Python-Inspired Optimum-Path Forest Classifier
Authors: Gustavo Henrique de Rosa, Jo\~ao Paulo Papa, Alexandre Xavier Falc\~ao
Categories: cs.LG cs.CV stat.ML
Comments: 14 pages, 11 figures
MSC-class: 68T01
ACM-class: I.2.0; I.5.0
\\ ( https://arxiv.org/abs/2001.10420 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2003.04286 (*cross-listing*)
replaced with revised version Tue, 22 Sep 2020 22:53:45 GMT   (134kb,D)

Title: Manifold Regularization for Locally Stable Deep Neural Networks
Authors: Charles Jin, Martin Rinard
Categories: stat.ML cs.CV cs.LG cs.NE
\\ ( https://arxiv.org/abs/2003.04286 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2003.07443
replaced with revised version Wed, 23 Sep 2020 15:39:03 GMT   (1379kb,D)

Title: Learnergy: Energy-based Machine Learners
Authors: Mateus Roder, Gustavo Henrique de Rosa, Jo\~ao Paulo Papa
Categories: cs.LG cs.CV stat.ML
Comments: 12 pages, 12 figures
MSC-class: 68T07
ACM-class: I.2.0; I.5.0
\\ ( https://arxiv.org/abs/2003.07443 ,  1379kb)
------------------------------------------------------------------------------
\\
arXiv:2005.03297
replaced with revised version Wed, 23 Sep 2020 09:14:20 GMT   (2219kb,D)

Title: Knowledge Enhanced Neural Fashion Trend Forecasting
Authors: Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng
  Chua
Categories: cs.IR cs.CV cs.MM
Comments: 8 pages, 9 figures, ICMR 2020
\\ ( https://arxiv.org/abs/2005.03297 ,  2219kb)
------------------------------------------------------------------------------
\\
arXiv:2006.02108
replaced with revised version Wed, 23 Sep 2020 12:49:46 GMT   (4687kb,D)

Title: Self-Supervised Localisation between Range Sensors and Overhead Imagery
Authors: Tim Y. Tang, Daniele De Martini, Shangzhe Wu, Paul Newman
Categories: cs.RO cs.CV
Comments: Robotics: Science and Systems (RSS) 2020
\\ ( https://arxiv.org/abs/2006.02108 ,  4687kb)
------------------------------------------------------------------------------
\\
arXiv:2006.16120 (*cross-listing*)
replaced with revised version Wed, 23 Sep 2020 16:34:15 GMT   (14112kb,D)

Title: Shape from Projections via Differentiable Forward Projector for Computed
  Tomography
Authors: Jakeoung Koo, Anders B. Dahl, J. Andreas B{\ae}rentzen, Qiongyang
  Chen, Vedrana A. Dahl
Categories: eess.IV cs.CV
Comments: Submitted to Pattern Recognition Letters
\\ ( https://arxiv.org/abs/2006.16120 ,  14112kb)
------------------------------------------------------------------------------
\\
arXiv:2009.07698
replaced with revised version Tue, 22 Sep 2020 21:37:02 GMT   (8343kb,D)

Title: Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News
Authors: Reuben Tan, Kate Saenko, Bryan A. Plummer
Categories: cs.AI cs.CL cs.CV
Comments: Accepted at EMNLP 2020
\\ ( https://arxiv.org/abs/2009.07698 ,  8343kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10190 (*cross-listing*)
replaced with revised version Wed, 23 Sep 2020 00:11:40 GMT   (4568kb,D)

Title: Federated Learning for Computational Pathology on Gigapixel Whole Slide
  Images
Authors: Ming Y. Lu, Dehan Kong, Jana Lipkova, Richard J. Chen, Rajendra Singh,
  Drew F. K. Williamson, Tiffany Y. Chen, Faisal Mahmood
Categories: eess.IV cs.CV cs.LG q-bio.TO
\\ ( https://arxiv.org/abs/2009.10190 ,  4568kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10549 (*cross-listing*)
replaced with revised version Wed, 23 Sep 2020 01:03:45 GMT   (3352kb,D)

Title: CA-Net: Comprehensive Attention Convolutional Neural Networks for
  Explainable Medical Image Segmentation
Authors: Ran Gu, Guotai Wang, Tao Song, Rui Huang, Michael Aertsen, Jan
  Deprest, S\'ebastien Ourselin, Tom Vercauteren, Shaoting Zhang
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2009.10549 ,  3352kb)
------------------------------------------------------------------------------
\\
arXiv:2004.14789
replaced with revised version Wed, 23 Sep 2020 13:47:30 GMT   (91kb,D)

Title: Twin-width I: tractable FO model checking
Authors: \'Edouard Bonnet, Eun Jung Kim, St\'ephan Thomass\'e, R\'emi Watrigant
Categories: cs.DS cs.DM cs.LO
Comments: 48 pages, 9 figures
MSC-class: 68Q25
ACM-class: F.2.2
\\ ( https://arxiv.org/abs/2004.14789 ,  91kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
