Delivered-To: brucelu2013@gmail.com
Received: by 2002:ab4:a06d:0:0:0:0:0 with SMTP id cz13csp185083ecb;
        Fri, 14 Aug 2020 01:49:41 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJyX19gdCitxnNPC1zS7Hxbsyhzvt8k4Umgphvgfn3/zKKzcp3GE4lYVMcOLt+dcZOI5R4dK
X-Received: by 2002:a37:a64e:: with SMTP id p75mr1071145qke.115.1597394981477;
        Fri, 14 Aug 2020 01:49:41 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1597394981; cv=none;
        d=google.com; s=arc-20160816;
        b=KV15JoKJmADWDM0KN5gC1XWpDwFGlxx0S0zH05A3zalls1S2o5k2qeUnffL2o4vQpe
         w7IZuO9AhorDedZkdQxIQ7kU0pCioDmgTsDGsTHkvtzf/SXbXkd3RmWOHNrFUqmXZxAK
         p/ZRZ8aaIky86u8hF3fJdoguzyD/T0bbjMImjc2GS51gbnC9jkVjrxdfafsBC3zmfIjL
         NcJfpRcutLynST5es2BHH8sSUV2StaFt6Gdbk3J2a+VAY4SPMZ41clSXS4/DNyNQeUcx
         vroKGxgb+/Nz9NlhKJVmf6OYPewAH/+T4h6KIFU6yLMXCV6igFZVBJZtYAjDl4iauxXy
         sNIQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=RXBsr/vP+Cu/2xWc/o9T/w8UB6VYJArmNYwJUiU9oV0=;
        b=sDWGrYhXJyawKjEWngmAsbOWW6mpS6f0Uz7nHD3yZm7cwjqkq6AZa7piPE9ddIqnOk
         M6tiHV3sjIu1d/C4iHDF2HPEea/iSgwj70Pbn1sXAP4TqHlv3Nmv+V+en83kZ3ZcfSBO
         /tmOQM4kjeXYFQlgg7Olc2nS+1cfxjoQnpCUj4h1tB8qUH0Al++sZjq5rkjT7eCzj1an
         tSBA5XkCWY8MFNnA4JHbmX1mfu9NRahhxlH7AqepfnOIdANlxEwIB7yjwLUfHr9xmBLD
         X7mgN0FCNK+ztDv1I6kKcHTKyxpEmtB1ASu/3ak8F7JS0Ua7ClPP8zCHHCGGFg7Hal8b
         HUDg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id h3si4594126qvs.44.2020.08.14.01.49.41
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 14 Aug 2020 01:49:41 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07E8nebK025547;
	Fri, 14 Aug 2020 04:49:40 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07E8neif064562;
	Fri, 14 Aug 2020 04:49:40 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 07E8ne0G064561;
	Fri, 14 Aug 2020 04:49:40 -0400
Date: Fri, 14 Aug 2020 04:49:40 -0400
Message-Id: <202008140849.07E8ne0G064561@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 2890 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Graphics
 received from  Wed 12 Aug 20 18:00:00 GMT  to  Thu 13 Aug 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2008.05503
Date: Wed, 12 Aug 2020 18:08:35 GMT   (319kb,D)

Title: Multi-level Stress Assessment Using Multi-domain Fusion of ECG Signal
Authors: Zeeshan Ahmad and Naimul Khan
Categories: cs.CV cs.LG
\\
  Stress analysis and assessment of affective states of mind using ECG as a
physiological signal is a burning research topic in biomedical signal
processing. However, existing literature provides only binary assessment of
stress, while multiple levels of assessment may be more beneficial for
healthcare applications. Furthermore, in present research, ECG signal for
stress analysis is examined independently in spatial domain or in transform
domains but the advantage of fusing these domains has not been fully utilized.
To get the maximum advantage of fusing diferent domains, we introduce a dataset
with multiple stress levels and then classify these levels using a novel deep
learning approach by converting ECG signal into signal images based on R-R
peaks without any feature extraction. Moreover, We made signal images
multimodal and multidomain by converting them into time-frequency and frequency
domain using Gabor wavelet transform (GWT) and Discrete Fourier Transform (DFT)
respectively. Convolutional Neural networks (CNNs) are used to extract features
from different modalities and then decision level fusion is performed for
improving the classification accuracy. The experimental results on an in-house
dataset collected with 15 users show that with proposed fusion framework and
using ECG signal to image conversion, we reach an average accuracy of 85.45%.
\\ ( https://arxiv.org/abs/2008.05503 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05511
Date: Wed, 12 Aug 2020 18:16:08 GMT   (11262kb,D)

Title: Free View Synthesis
Authors: Gernot Riegler, Vladlen Koltun
Categories: cs.CV
Comments: published at ECCV 2020, https://youtu.be/JDJPn3ZtfZs
\\
  We present a method for novel view synthesis from input images that are
freely distributed around a scene. Our method does not rely on a regular
arrangement of input views, can synthesize images for free camera movement
through the scene, and works for general scenes with unconstrained geometric
layouts. We calibrate the input images via SfM and erect a coarse geometric
scaffold via MVS. This scaffold is used to create a proxy depth map for a novel
view of the scene. Based on this depth map, a recurrent encoder-decoder network
processes reprojected features from nearby views and synthesizes the new view.
Our network does not need to be optimized for a given scene. After training on
a dataset, it works in previously unseen environments with no fine-tuning or
per-scene optimization. We evaluate the presented approach on challenging
real-world datasets, including Tanks and Temples, where we demonstrate
successful view synthesis for the first time and substantially outperform prior
and concurrent work.
\\ ( https://arxiv.org/abs/2008.05511 ,  11262kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05524
Date: Wed, 12 Aug 2020 18:40:38 GMT   (25820kb,D)

Title: Mitigating Dataset Imbalance via Joint Generation and Classification
Authors: Aadarsh Sahoo, Ankit Singh, Rameswar Panda, Rogerio Feris, Abir Das
Categories: cs.CV
Comments: Accepted in ECCV2020 Workshop on Imbalance Problems in Computer
  Vision (IPCV)
\\
  Supervised deep learning methods are enjoying enormous success in many
practical applications of computer vision and have the potential to
revolutionize robotics. However, the marked performance degradation to biases
and imbalanced data questions the reliability of these methods. In this work we
address these questions from the perspective of dataset imbalance resulting out
of severe under-representation of annotated training data for certain classes
and its effect on both deep classification and generation methods. We introduce
a joint dataset repairment strategy by combining a neural network classifier
with Generative Adversarial Networks (GAN) that makes up for the deficit of
training examples from the under-representated class by producing additional
training examples. We show that the combined training helps to improve the
robustness of both the classifier and the GAN against severe class imbalance.
We show the effectiveness of our proposed approach on three very different
datasets with different degrees of imbalance in them. The code is available at
https://github.com/AadSah/ImbalanceCycleGAN .
\\ ( https://arxiv.org/abs/2008.05524 ,  25820kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05534
Date: Wed, 12 Aug 2020 19:08:59 GMT   (42769kb,D)

Title: Co-training for On-board Deep Object Detection
Authors: Gabriel Villalonga and Antonio M. Lopez
Categories: cs.CV
\\
  Providing ground truth supervision to train visual models has been a
bottleneck over the years, exacerbated by domain shifts which degenerate the
performance of such models. This was the case when visual tasks relied on
handcrafted features and shallow machine learning and, despite its
unprecedented performance gains, the problem remains open within the deep
learning paradigm due to its data-hungry nature. Best performing deep
vision-based object detectors are trained in a supervised manner by relying on
human-labeled bounding boxes which localize class instances (i.e.objects)
within the training images.Thus, object detection is one of such tasks for
which human labeling is a major bottleneck. In this paper, we assess
co-training as a semi-supervised learning method for self-labeling objects in
unlabeled images, so reducing the human-labeling effort for developing deep
object detectors. Our study pays special attention to a scenario involving
domain shift; in particular, when we have automatically generated virtual-world
images with object bounding boxes and we have real-world images which are
unlabeled. Moreover, we are particularly interested in using co-training for
deep object detection in the context of driver assistance systems and/or
self-driving vehicles. Thus, using well-established datasets and protocols for
object detection in these application contexts, we will show how co-training is
a paradigm worth to pursue for alleviating object labeling, working both alone
and together with task-agnostic domain adaptation.
\\ ( https://arxiv.org/abs/2008.05534 ,  42769kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05557
Date: Wed, 12 Aug 2020 20:08:39 GMT   (1756kb,D)

Title: Continual Class Incremental Learning for CT Thoracic Segmentation
Authors: Abdelrahman Elskhawy, Aneta Lisowska, Matthias Keicher, Josep Henry,
  Paul Thomson, Nassir Navab
Categories: cs.CV
\\
  Deep learning organ segmentation approaches require large amounts of
annotated training data, which is limited in supply due to reasons of
confidentiality and the time required for expert manual annotation. Therefore,
being able to train models incrementally without having access to previously
used data is desirable. A common form of sequential training is fine tuning
(FT). In this setting, a model learns a new task effectively, but loses
performance on previously learned tasks. The Learning without Forgetting (LwF)
approach addresses this issue via replaying its own prediction for past tasks
during model training. In this work, we evaluate FT and LwF for class
incremental learning in multi-organ segmentation using the publicly available
AAPM dataset. We show that LwF can successfully retain knowledge on previous
segmentations, however, its ability to learn a new class decreases with the
addition of each class. To address this problem we propose an adversarial
continual learning segmentation approach (ACLSeg), which disentangles feature
space into task-specific and task-invariant features. This enables preservation
of performance on past tasks and effective acquisition of new knowledge.
\\ ( https://arxiv.org/abs/2008.05557 ,  1756kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05563
Date: Wed, 12 Aug 2020 20:25:07 GMT   (379kb)

Title: Facial Expression Recognition Under Partial Occlusion from Virtual
  Reality Headsets based on Transfer Learning
Authors: Bita Houshmand, Naimul Khan
Categories: cs.CV
Comments: To be presented at the IEEE BigMM 2020
\\
  Facial expressions of emotion are a major channel in our daily
communications, and it has been subject of intense research in recent years. To
automatically infer facial expressions, convolutional neural network based
approaches has become widely adopted due to their proven applicability to
Facial Expression Recognition (FER) task.On the other hand Virtual Reality (VR)
has gained popularity as an immersive multimedia platform, where FER can
provide enriched media experiences. However, recognizing facial expression
while wearing a head-mounted VR headset is a challenging task due to the upper
half of the face being completely occluded. In this paper we attempt to
overcome these issues and focus on facial expression recognition in presence of
a severe occlusion where the user is wearing a head-mounted display in a VR
setting. We propose a geometric model to simulate occlusion resulting from a
Samsung Gear VR headset that can be applied to existing FER datasets. Then, we
adopt a transfer learning approach, starting from two pretrained networks,
namely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB
datasets. Experimental results show that our approach achieves comparable
results to existing methods while training on three modified benchmark datasets
that adhere to realistic occlusion resulting from wearing a commodity VR
headset. Code for this paper is available at:
https://github.com/bita-github/MRP-FER
\\ ( https://arxiv.org/abs/2008.05563 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05570
Date: Wed, 12 Aug 2020 21:00:10 GMT   (5671kb,D)

Title: Generating Person-Scene Interactions in 3D Scenes
Authors: Siwei Zhang, Yan Zhang, Qianli Ma, Michael J. Black, Siyu Tang
Categories: cs.CV
\\
  High fidelity digital 3D environments have been proposed in recent years;
however, it remains extreme challenging to automatically equip such environment
with realistic human bodies. Existing work utilizes images, depths, or semantic
maps to represent the scene, and parametric human models to represent 3D bodies
in the scene. While being straightforward, their generated human-scene
interactions are often lack of naturalness and physical plausibility. Our key
observation is that humans interact with the world through body-scene contact.
To explicitly and effectively represent the physical contact between the body
and the world is essential for modeling human-scene interaction. To that end,
we propose a novel interaction representation, which explicitly encodes the
proximity between the human body and the 3D scene around it. Specifically,
given a set of basis points on a scene mesh, we leverage a conditional
variational autoencoder to synthesize the distance from every basis point to
its closest point on a human body. The synthesized proximal relationship
between the human body and the scene can indicate which region a person tends
to contact. Furthermore, based on such synthesized proximity, we can
effectively obtain expressive 3D human bodies that naturally interact with the
3D scene. Our perceptual study shows that our model significantly improves the
state-of-the-art method, approaching the realism of real human-scene
interaction. We believe our method makes an important step towards the fully
automatic synthesis of realistic 3D human bodies in 3D scenes. Our code and
model will be publicly available for research purpose.
\\ ( https://arxiv.org/abs/2008.05570 ,  5671kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05571
Date: Wed, 12 Aug 2020 21:02:32 GMT   (7388kb,D)

Title: Self-Path: Self-supervision for Classification of Pathology Images with
  Limited Annotations
Authors: Navid Alemi Koohbanani, Balagopal Unnikrishnan, Syed Ali Khurram,
  Pavitra Krishnaswamy, Nasir Rajpoot
Categories: cs.CV eess.IV
\\
  While high-resolution pathology images lend themselves well to `data hungry'
deep learning algorithms, obtaining exhaustive annotations on these images is a
major challenge. In this paper, we propose a self-supervised CNN approach to
leverage unlabeled data for learning generalizable and domain invariant
representations in pathology images. The proposed approach, which we term as
Self-Path, is a multi-task learning approach where the main task is tissue
classification and pretext tasks are a variety of self-supervised tasks with
labels inherent to the input data. We introduce novel domain specific
self-supervision tasks that leverage contextual, multi-resolution and semantic
features in pathology images for semi-supervised learning and domain
adaptation. We investigate the effectiveness of Self-Path on 3 different
pathology datasets. Our results show that Self-Path with the domain-specific
pretext tasks achieves state-of-the-art performance for semi-supervised
learning when small amounts of labeled data are available. Further, we show
that Self-Path improves domain adaptation for classification of histology image
patches when there is no labeled data available for the target domain. This
approach can potentially be employed for other applications in computational
pathology, where annotation budget is often limited or large amount of
unlabeled image data is available.
\\ ( https://arxiv.org/abs/2008.05571 ,  7388kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05596
Date: Wed, 12 Aug 2020 22:57:44 GMT   (4963kb,D)

Title: We Have So Much In Common: Modeling Semantic Relational Set Abstractions
  in Videos
Authors: Alex Andonian, Camilo Fosco, Mathew Monfort, Allen Lee, Rogerio Feris,
  Carl Vondrick, and Aude Oliva
Categories: cs.CV
Comments: European Conference on Computer Vision (ECCV) 2020, accepted
\\
  Identifying common patterns among events is a key ability in human and
machine perception, as it underlies intelligent decision making. We propose an
approach for learning semantic relational set abstractions on videos, inspired
by human learning. We combine visual features with natural language supervision
to generate high-level representations of similarities across a set of videos.
This allows our model to perform cognitive tasks such as set abstraction (which
general concept is in common among a set of videos?), set completion (which new
video goes well with the set?), and odd one out detection (which video does not
belong to the set?). Experiments on two video benchmarks, Kinetics and
Multi-Moments in Time, show that robust and versatile representations emerge
when learning to recognize commonalities among sets. We compare our model to
several baseline algorithms and show that significant improvements result from
explicitly learning relational abstractions with semantic supervision.
\\ ( https://arxiv.org/abs/2008.05596 ,  4963kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05654
Date: Thu, 13 Aug 2020 02:47:01 GMT   (4056kb,D)

Title: Few shot clustering for indoor occupancy detection with extremely
  low-quality images from battery free cameras
Authors: Homagni Saha, Sin Yon Tan, Ali Saffari, Mohamad Katanbaf, Joshua R.
  Smith, Soumik Sarkar
Categories: cs.CV cs.LG
Comments: 9 pages
\\
  Reliable detection of human occupancy in indoor environments is critical for
various energy efficiency, security, and safety applications. We consider this
challenge of occupancy detection using extremely low-quality,
privacy-preserving images from low power image sensors. We propose a combined
few shot learning and clustering algorithm to address this challenge that has
very low commissioning and maintenance cost. While the few shot learning
concept enables us to commission our system with a few labeled examples, the
clustering step serves the purpose of online adaptation to changing imaging
environment over time. Apart from validating and comparing our algorithm on
benchmark datasets, we also demonstrate performance of our algorithm on
streaming images collected from real homes using our novel battery free camera
hardware.
\\ ( https://arxiv.org/abs/2008.05654 ,  4056kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05655
Date: Thu, 13 Aug 2020 02:48:27 GMT   (3164kb,D)

Title: ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked
  Global-Local Attention Network
Authors: Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei,
  Xiaolin Wei, Shuqiang Jiang
Categories: cs.CV cs.MM
Comments: Accepted by ACM Multimedia 2020
\\
  Food recognition has received more and more attention in the multimedia
community for its various real-world applications, such as diet management and
self-service restaurants. A large-scale ontology of food images is urgently
needed for developing advanced large-scale food recognition algorithms, as well
as for providing the benchmark dataset for such algorithms. To encourage
further progress in food recognition, we introduce the dataset ISIA Food- 500
with 500 categories from the list in the Wikipedia and 399,726 images, a more
comprehensive food dataset that surpasses existing popular benchmark datasets
by category coverage and data volume. Furthermore, we propose a stacked
global-local attention network, which consists of two sub-networks for food
recognition. One subnetwork first utilizes hybrid spatial-channel attention to
extract more discriminative features, and then aggregates these multi-scale
discriminative features from multiple layers into global-level representation
(e.g., texture and shape information about food). The other one generates
attentional regions (e.g., ingredient relevant regions) from different regions
via cascaded spatial transformers, and further aggregates these multi-scale
regional features from different layers into local-level representation. These
two types of features are finally fused as comprehensive representation for
food recognition. Extensive experiments on ISIA Food-500 and other two popular
benchmark datasets demonstrate the effectiveness of our proposed method, and
thus can be considered as one strong baseline. The dataset, code and models can
be found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.
\\ ( https://arxiv.org/abs/2008.05655 ,  3164kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05657
Date: Thu, 13 Aug 2020 02:59:31 GMT   (651kb,D)

Title: Sparse Coding Driven Deep Decision Tree Ensembles for Nuclear
  Segmentation in Digital Pathology Images
Authors: Jie Song, Liang Xiao, Mohsen Molaei, and Zhichao Lian
Categories: cs.CV
Comments: Submitted to IEEE Transactions on Image Processing
\\
  In this paper, we propose an easily trained yet powerful representation
learning approach with performance highly competitive to deep neural networks
in a digital pathology image segmentation task. The method, called sparse
coding driven deep decision tree ensembles that we abbreviate as ScD2TE,
provides a new perspective on representation learning. We explore the
possibility of stacking several layers based on non-differentiable pairwise
modules and generate a densely concatenated architecture holding the
characteristics of feature map reuse and end-to-end dense learning. Under this
architecture, fast convolutional sparse coding is used to extract multi-level
features from the output of each layer. In this way, rich image appearance
models together with more contextual information are integrated by learning a
series of decision tree ensembles. The appearance and the high-level context
features of all the previous layers are seamlessly combined by concatenating
them to feed-forward as input, which in turn makes the outputs of subsequent
layers more accurate and the whole model efficient to train. Compared with deep
neural networks, our proposed ScD2TE does not require back-propagation
computation and depends on less hyper-parameters. ScD2TE is able to achieve a
fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the
superiority of our segmentation technique by evaluating it on the multi-disease
state and multi-organ dataset where consistently higher performances were
obtained for comparison against several state-of-the-art deep learning methods
such as convolutional neural networks (CNN), fully convolutional networks
(FCN), etc.
\\ ( https://arxiv.org/abs/2008.05657 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05659
Date: Thu, 13 Aug 2020 03:02:32 GMT   (2028kb,D)

Title: What Should Not Be Contrastive in Contrastive Learning
Authors: Tete Xiao, Xiaolong Wang, Alexei A. Efros, Trevor Darrell
Categories: cs.CV
\\
  Recent self-supervised contrastive methods have been able to produce
impressive transferable visual representations by learning to be invariant to
different data augmentations. However, these methods implicitly assume a
particular set of representational invariances (e.g., invariance to color), and
can perform poorly when a downstream task violates this assumption (e.g.,
distinguishing red vs. yellow cars). We introduce a contrastive learning
framework which does not require prior knowledge of specific, task-dependent
invariances. Our model learns to capture varying and invariant factors for
visual representations by constructing separate embedding spaces, each of which
is invariant to all but one augmentation. We use a multi-head network with a
shared backbone which captures information across each augmentation and alone
outperforms all baselines on downstream tasks. We further find that the
concatenation of the invariant and varying spaces performs best across all
tasks we investigate, including coarse-grained, fine-grained, and few-shot
downstream classification tasks, and various data corruptions.
\\ ( https://arxiv.org/abs/2008.05659 ,  2028kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05667
Date: Thu, 13 Aug 2020 03:20:01 GMT   (12895kb,D)

Title: Feature Binding with Category-Dependant MixUp for Semantic Segmentation
  and Adversarial Robustness
Authors: Md Amirul Islam, Matthew Kowal, Konstantinos G. Derpanis, Neil D. B.
  Bruce
Categories: cs.CV
Comments: Accepted to BMVC 2020 (Oral)
\\
  In this paper, we present a strategy for training convolutional neural
networks to effectively resolve interference arising from competing hypotheses
relating to inter-categorical information throughout the network. The premise
is based on the notion of feature binding, which is defined as the process by
which activation's spread across space and layers in the network are
successfully integrated to arrive at a correct inference decision. In our work,
this is accomplished for the task of dense image labelling by blending images
based on their class labels, and then training a feature binding network, which
simultaneously segments and separates the blended images. Subsequent feature
denoising to suppress noisy activations reveals additional desirable properties
and high degrees of successful predictions. Through this process, we reveal a
general mechanism, distinct from any prior methods, for boosting the
performance of the base segmentation network while simultaneously increasing
robustness to adversarial attacks.
\\ ( https://arxiv.org/abs/2008.05667 ,  12895kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05676
Date: Thu, 13 Aug 2020 03:52:37 GMT   (3076kb,D)

Title: Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance
  Segmentation
Authors: Jialian Wu, Liangchen Song, Tiancai Wang, Qian Zhang, Junsong Yuan
Categories: cs.CV
Comments: Accepted to Proceedings of the 28th ACM International Conference on
  Multimedia (ACM MM'20), Seattle, WA, USA
\\
  Despite the previous success of object analysis, detecting and segmenting a
large number of object categories with a long-tailed data distribution remains
a challenging problem and is less investigated. For a large-vocabulary
classifier, the chance of obtaining noisy logits is much higher, which can
easily lead to a wrong recognition. In this paper, we exploit prior knowledge
of the relations among object categories to cluster fine-grained classes into
coarser parent classes, and construct a classification tree that is responsible
for parsing an object instance into a fine-grained category via its parent
class. In the classification tree, as the number of parent class nodes are
significantly less, their logits are less noisy and can be utilized to suppress
the wrong/noisy logits existed in the fine-grained class nodes. As the way to
construct the parent class is not unique, we further build multiple trees to
form a classification forest where each tree contributes its vote to the
fine-grained classification. To alleviate the imbalanced learning caused by the
long-tail phenomena, we propose a simple yet effective resampling method, NMS
Resampling, to re-balance the data distribution. Our method, termed as Forest
R-CNN, can serve as a plug-and-play module being applied to most object
recognition models for recognizing more than 1000 categories. Extensive
experiments are performed on the large vocabulary dataset LVIS. Compared with
the Mask R-CNN baseline, the Forest R-CNN significantly boosts the performance
with 11.5% and 3.9% AP improvements on the rare categories and overall
categories, respectively. Moreover, we achieve state-of-the-art results on the
LVIS dataset. Code is available at https://github.com/JialianW/Forest_RCNN.
\\ ( https://arxiv.org/abs/2008.05676 ,  3076kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05678
Date: Thu, 13 Aug 2020 03:59:34 GMT   (4614kb,D)

Title: Visual Localization for Autonomous Driving: Mapping the Accurate
  Location in the City Maze
Authors: Dongfang Liu, Yiming Cui, Xiaolei Guo, Wei Ding, Baijian Yang, and
  Yingjie Chen
Categories: cs.CV
\\
  Accurate localization is a foundational capacity, required for autonomous
vehicles to accomplish other tasks such as navigation or path planning. It is a
common practice for vehicles to use GPS to acquire location information.
However, the application of GPS can result in severe challenges when vehicles
run within the inner city where different kinds of structures may shadow the
GPS signal and lead to inaccurate location results. To address the localization
challenges of urban settings, we propose a novel feature voting technique for
visual localization. Different from the conventional front-view-based method,
our approach employs views from three directions (front, left, and right) and
thus significantly improves the robustness of location prediction. In our work,
we craft the proposed feature voting method into three state-of-the-art visual
localization networks and modify their architectures properly so that they can
be applied for vehicular operation. Extensive field test results indicate that
our approach can predict location robustly even in challenging inner-city
settings. Our research sheds light on using the visual localization approach to
help autonomous vehicles to find accurate location information in a city maze,
within a desirable time constraint.
\\ ( https://arxiv.org/abs/2008.05678 ,  4614kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05700
Date: Thu, 13 Aug 2020 05:51:35 GMT   (17538kb,D)

Title: What leads to generalization of object proposals?
Authors: Rui Wang, Dhruv Mahajan, Vignesh Ramanathan
Categories: cs.CV
\\
  Object proposal generation is often the first step in many detection models.
It is lucrative to train a good proposal model, that generalizes to unseen
classes. This could help scaling detection models to larger number of classes
with fewer annotations. Motivated by this, we study how a detection model
trained on a small set of source classes can provide proposals that generalize
to unseen classes. We systematically study the properties of the dataset -
visual diversity and label space granularity - required for good
generalization. We show the trade-off between using fine-grained labels and
coarse labels. We introduce the idea of prototypical classes: a set of
sufficient and necessary classes required to train a detection model to obtain
generalized proposals in a more data-efficient way. On the Open Images V4
dataset, we show that only 25% of the classes can be selected to form such a
prototypical set. The resulting proposals from a model trained with these
classes is only 4.3% worse than using all the classes, in terms of average
recall (AR). We also demonstrate that Faster R-CNN model leads to better
generalization of proposals compared to a single-stage network like RetinaNet.
\\ ( https://arxiv.org/abs/2008.05700 ,  17538kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05706
Date: Thu, 13 Aug 2020 06:15:57 GMT   (2754kb,D)

Title: Network Architecture Search for Domain Adaptation
Authors: Yichen Li, Xingchao Peng
Categories: cs.CV
\\
  Deep networks have been used to learn transferable representations for domain
adaptation. Existing deep domain adaptation methods systematically employ
popular hand-crafted networks designed specifically for image-classification
tasks, leading to sub-optimal domain adaptation performance. In this paper, we
present Neural Architecture Search for Domain Adaptation (NASDA), a principle
framework that leverages differentiable neural architecture search to derive
the optimal network architecture for domain adaptation task. NASDA is designed
with two novel training strategies: neural architecture search with
multi-kernel Maximum Mean Discrepancy to derive the optimal architecture, and
adversarial training between a feature generator and a batch of classifiers to
consolidate the feature generator. We demonstrate experimentally that NASDA
leads to state-of-the-art performance on several domain adaptation benchmarks.
\\ ( https://arxiv.org/abs/2008.05706 ,  2754kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05708
Date: Thu, 13 Aug 2020 06:21:33 GMT   (3066kb,D)

Title: Robust Image Matching By Dynamic Feature Selection
Authors: Hao Huang, Jianchun Chen, Xiang Li, Lingjing Wang, Yi Fang
Categories: cs.CV
\\
  Estimating dense correspondences between images is a long-standing image
under-standing task. Recent works introduce convolutional neural networks
(CNNs) to extract high-level feature maps and find correspondences through
feature matching. However,high-level feature maps are in low spatial resolution
and therefore insufficient to provide accurate and fine-grained features to
distinguish intra-class variations for correspondence matching. To address this
problem, we generate robust features by dynamically selecting features at
different scales. To resolve two critical issues in feature selection,i.e.,how
many and which scales of features to be selected, we frame the feature
selection process as a sequential Markov decision-making process (MDP) and
introduce an optimal selection strategy using reinforcement learning (RL). We
define an RL environment for image matching in which each individual action
either requires new features or terminates the selection episode by referring a
matching score. Deep neural networks are incorporated into our method and
trained for decision making. Experimental results show that our method achieves
comparable/superior performance with state-of-the-art methods on three
benchmarks, demonstrating the effectiveness of our feature selection strategy.
\\ ( https://arxiv.org/abs/2008.05708 ,  3066kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05711
Date: Thu, 13 Aug 2020 06:29:01 GMT   (4785kb,D)

Title: Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by
  Implicitly Unprojecting to 3D
Authors: Jonah Philion, Sanja Fidler
Categories: cs.CV
Comments: ECCV 2020
\\
  The goal of perception for autonomous vehicles is to extract semantic
representations from multiple sensors and fuse these representations into a
single "bird's-eye-view" coordinate frame for consumption by motion planning.
We propose a new end-to-end architecture that directly extracts a
bird's-eye-view representation of a scene given image data from an arbitrary
number of cameras. The core idea behind our approach is to "lift" each image
individually into a frustum of features for each camera, then "splat" all
frustums into a rasterized bird's-eye-view grid. By training on the entire
camera rig, we provide evidence that our model is able to learn not only how to
represent images but how to fuse predictions from all cameras into a single
cohesive representation of the scene while being robust to calibration error.
On standard bird's-eye-view tasks such as object segmentation and map
segmentation, our model outperforms all baselines and prior work. In pursuit of
the goal of learning dense representations for motion planning, we show that
the representations inferred by our model enable interpretable end-to-end
motion planning by "shooting" template trajectories into a bird's-eye-view cost
map output by our network. We benchmark our approach against models that use
oracle depth from lidar. Project page with code:
https://nv-tlabs.github.io/lift-splat-shoot .
\\ ( https://arxiv.org/abs/2008.05711 ,  4785kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05714
Date: Thu, 13 Aug 2020 06:31:01 GMT   (4332kb,D)

Title: Modeling Caricature Expressions by 3D Blendshape and Dynamic Texture
Authors: Keyu Chen, Jianmin Zheng, Jianfei Cai, Juyong Zhang
Categories: cs.CV
Comments: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020)
DOI: 10.1145/3394171.3413643
\\
  The problem of deforming an artist-drawn caricature according to a given
normal face expression is of interest in applications such as social media,
animation and entertainment. This paper presents a solution to the problem,
with an emphasis on enhancing the ability to create desired expressions and
meanwhile preserve the identity exaggeration style of the caricature, which
imposes challenges due to the complicated nature of caricatures. The key of our
solution is a novel method to model caricature expression, which extends
traditional 3DMM representation to caricature domain. The method consists of
shape modelling and texture generation for caricatures. Geometric optimization
is developed to create identity-preserving blendshapes for reconstructing
accurate and stable geometric shape, and a conditional generative adversarial
network (cGAN) is designed for generating dynamic textures under target
expressions. The combination of both shape and texture components makes the
non-trivial expressions of a caricature be effectively defined by the extension
of the popular 3DMM representation and a caricature can thus be flexibly
deformed into arbitrary expressions with good results visually in both shape
and color spaces. The experiments demonstrate the effectiveness of the proposed
method.
\\ ( https://arxiv.org/abs/2008.05714 ,  4332kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05717
Date: Thu, 13 Aug 2020 06:41:49 GMT   (6046kb,D)

Title: Alleviating Human-level Shift : A Robust Domain Adaptation Method for
  Multi-person Pose Estimation
Authors: Xixia Xu, Qi Zou, Xue Lin
Categories: cs.CV
Comments: Accepted By ACM MM'2020
DOI: 10.1145/3394171.3414040
\\
  Human pose estimation has been widely studied with much focus on supervised
learning requiring sufficient annotations. However, in real applications, a
pretrained pose estimation model usually need be adapted to a novel domain with
no labels or sparse labels. Such domain adaptation for 2D pose estimation
hasn't been explored. The main reason is that a pose, by nature, has typical
topological structure and needs fine-grained features in local keypoints. While
existing adaptation methods do not consider topological structure of
object-of-interest and they align the whole images coarsely. Therefore, we
propose a novel domain adaptation method for multi-person pose estimation to
conduct the human-level topological structure alignment and fine-grained
feature alignment. Our method consists of three modules: Cross-Attentive
Feature Alignment (CAFA), Intra-domain Structure Adaptation (ISA) and
Inter-domain Human-Topology Alignment (IHTA) module. The CAFA adopts a
bidirectional spatial attention module (BSAM)that focuses on fine-grained local
feature correlation between two humans to adaptively aggregate consistent
features for adaptation. We adopt ISA only in semi-supervised domain adaptation
(SSDA) to exploit the corresponding keypoint semantic relationship for reducing
the intra-domain bias. Most importantly, we propose an IHTA to learn more
domain-invariant human topological representation for reducing the inter-domain
discrepancy. We model the human topological structure via the graph convolution
network (GCN), by passing messages on which, high-order relations can be
considered. This structure preserving alignment based on GCN is beneficial to
the occluded or extreme pose inference. Extensive experiments are conducted on
two popular benchmarks and results demonstrate the competency of our method
compared with existing supervised approaches.
\\ ( https://arxiv.org/abs/2008.05717 ,  6046kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05721
Date: Thu, 13 Aug 2020 06:56:52 GMT   (3473kb,D)

Title: Learning Temporally Invariant and Localizable Features via Data
  Augmentation for Video Recognition
Authors: Taeoh Kim, Hyeongmin Lee, MyeongAh Cho, Ho Seong Lee, Dong Heon Cho,
  Sangyoun Lee
Categories: cs.CV
Comments: European Conference on Computer Vision (ECCV) 2020, 1st Visual
  Inductive Priors for Data-Efficient Deep Learning Workshop (Oral)
\\
  Deep-Learning-based video recognition has shown promising improvements along
with the development of large-scale datasets and spatiotemporal network
architectures. In image recognition, learning spatially invariant features is a
key factor in improving recognition performance and robustness. Data
augmentation based on visual inductive priors, such as cropping, flipping,
rotating, or photometric jittering, is a representative approach to achieve
these features. Recent state-of-the-art recognition solutions have relied on
modern data augmentation strategies that exploit a mixture of augmentation
operations. In this study, we extend these strategies to the temporal dimension
for videos to learn temporally invariant or temporally localizable features to
cover temporal perturbations or complex actions in videos. Based on our novel
temporal data augmentation algorithms, video recognition performances are
improved using only a limited amount of training data compared to the
spatial-only data augmentation algorithms, including the 1st Visual Inductive
Priors (VIPriors) for data-efficient action recognition challenge. Furthermore,
learned features are temporally localizable that cannot be achieved using
spatial augmentation algorithms. Our source code is available at
https://github.com/taeoh-kim/temporal_data_augmentation.
\\ ( https://arxiv.org/abs/2008.05721 ,  3473kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05723
Date: Thu, 13 Aug 2020 07:04:15 GMT   (7097kb,D)

Title: Contextual Diversity for Active Learning
Authors: Sharat Agarwal and Himanshu Arora and Saket Anand and Chetan Arora
Categories: cs.CV
Comments: A variant of this report is accepted in ECCV 2020
\\
  Requirement of large annotated datasets restrict the use of deep
convolutional neural networks (CNNs) for many practical applications. The
problem can be mitigated by using active learning (AL) techniques which, under
a given annotation budget, allow to select a subset of data that yields maximum
accuracy upon fine tuning. State of the art AL approaches typically rely on
measures of visual diversity or prediction uncertainty, which are unable to
effectively capture the variations in spatial context. On the other hand,
modern CNN architectures make heavy use of spatial context for achieving highly
accurate predictions. Since the context is difficult to evaluate in the absence
of ground-truth labels, we introduce the notion of contextual diversity that
captures the confusion associated with spatially co-occurring classes.
Contextual Diversity (CD) hinges on a crucial observation that the probability
vector predicted by a CNN for a region of interest typically contains
information from a larger receptive field. Exploiting this observation, we use
the proposed CD measure within two AL frameworks: (1) a core-set based strategy
and (2) a reinforcement learning based policy, for active frame selection. Our
extensive empirical evaluation establish state of the art results for active
learning on benchmark datasets of Semantic Segmentation, Object Detection and
Image Classification. Our ablation studies show clear advantages of using
contextual diversity for active learning. The source code and additional
results are available at https://github.com/sharat29ag/CDAL.
\\ ( https://arxiv.org/abs/2008.05723 ,  7097kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05731
Date: Thu, 13 Aug 2020 07:34:05 GMT   (8341kb,D)

Title: ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy
  Diagnosis
Authors: Gwenol\'e Quellec, Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze,
  Pascale Massin, B\'eatrice Cochener
Categories: cs.CV
\\
  In recent years, Artificial Intelligence (AI) has proven its relevance for
medical decision support. However, the "black-box" nature of successful AI
algorithms still holds back their wide-spread deployment. In this paper, we
describe an eXplanatory Artificial Intelligence (XAI) that reaches the same
level of performance as black-box AI, for the task of Diabetic Retinopathy (DR)
diagnosis using Color Fundus Photography (CFP). This algorithm, called ExplAIn,
learns to segment and categorize lesions in images; the final diagnosis is
directly derived from these multivariate lesion segmentations. The novelty of
this explanatory framework is that it is trained from end to end, with image
supervision only, just like black-box AI algorithms: the concepts of lesions
and lesion categories emerge by themselves. For improved lesion localization,
foreground/background separation is trained through self-supervision, in such a
way that occluding foreground pixels transforms the input image into a
healthy-looking image. The advantage of such an architecture is that automatic
diagnoses can be explained simply by an image and/or a few sentences. ExplAIn
is evaluated at the image level and at the pixel level on various CFP image
datasets. We expect this new framework, which jointly offers high
classification performance and explainability, to facilitate AI deployment.
\\ ( https://arxiv.org/abs/2008.05731 ,  8341kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05732
Date: Thu, 13 Aug 2020 07:37:27 GMT   (2207kb,D)

Title: An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture
  Recognition
Authors: Kenneth Lai and Svetlana Yanushkevich
Categories: cs.CV
Comments: Accepted at International Joint Conference on Neural Network
\\
  The focus of this paper is dynamic gesture recognition in the context of the
interaction between humans and machines. We propose a model consisting of two
sub-networks, a transformer and an ordered-neuron long-short-term-memory
(ON-LSTM) based recurrent neural network (RNN). Each sub-network is trained to
perform the task of gesture recognition using only skeleton joints. Since each
sub-network extracts different types of features due to the difference in
architecture, the knowledge can be shared between the sub-networks. Through
knowledge distillation, the features and predictions from each sub-network are
fused together into a new fusion classifier. In addition, a cyclical learning
rate can be used to generate a series of models that are combined in an
ensemble, in order to yield a more generalizable prediction. The proposed
ensemble of knowledge-sharing models exhibits an overall accuracy of 86.11%
using only skeleton information, as tested using the Dynamic Hand Gesture-14/28
dataset
\\ ( https://arxiv.org/abs/2008.05732 ,  2207kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05735
Date: Thu, 13 Aug 2020 07:43:14 GMT   (6284kb,D)

Title: Reliability of Decision Support in Cross-spectral Biometric-enabled
  Systems
Authors: Kenneth Lai, Svetlana N. Yanushkevich, and Vlad Shmerko
Categories: cs.CV
Comments: submitted to IEEE International Conference on Systems, Man, and
  Cybernetics
\\
  This paper addresses the evaluation of the performance of the decision
support system that utilizes face and facial expression biometrics. The
evaluation criteria include risk of error and related reliability of decision,
as well as their contribution to the changes in the perceived operator's trust
in the decision. The relevant applications include human behavior monitoring
and stress detection in individuals and teams, and in situational awareness
system. Using an available database of cross-spectral videos of faces and
facial expressions, we conducted a series of experiments that demonstrate the
phenomenon of biases in biometrics that affect the evaluated measures of the
performance in human-machine systems.
\\ ( https://arxiv.org/abs/2008.05735 ,  6284kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05742
Date: Thu, 13 Aug 2020 07:59:25 GMT   (8813kb,D)

Title: SkeletonNet: A Topology-Preserving Solution for Learning Mesh
  Reconstruction of Object Surfaces from RGB Images
Authors: Jiapeng Tang, Xiaoguang Han, Mingkui Tan, Xin Tong, Kui Jia
Categories: cs.CV
Comments: 15 pages, 13 figures
\\
  This paper focuses on the challenging task of learning 3D object surface
reconstructions from RGB images. Existingmethods achieve varying degrees of
success by using different surface representations. However, they all have
their own drawbacks,and cannot properly reconstruct the surface shapes of
complex topologies, arguably due to a lack of constraints on the
topologicalstructures in their learning frameworks. To this end, we propose to
learn and use the topology-preserved, skeletal shape representationto assist
the downstream task of object surface reconstruction from RGB images.
Technically, we propose the novelSkeletonNetdesign that learns a volumetric
representation of a skeleton via a bridged learning of a skeletal point set,
where we use paralleldecoders each responsible for the learning of points on 1D
skeletal curves and 2D skeletal sheets, as well as an efficient module
ofglobally guided subvolume synthesis for a refined, high-resolution skeletal
volume; we present a differentiablePoint2Voxellayer tomake SkeletonNet
end-to-end and trainable. With the learned skeletal volumes, we propose two
models, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the
Skeleton-Regularized Deep Implicit Surface Network (SkeDISN), which
respectivelybuild upon and improve over the existing frameworks of explicit
mesh deformation and implicit field learning for the downstream
surfacereconstruction task. We conduct thorough experiments that verify the
efficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing
methods as well, and they have their own merits when measured by different
metrics. Additional results ingeneralized task settings further demonstrate the
usefulness of our proposed methods. We have made both our implementation
codeand the ShapeNet-Skeleton dataset publicly available at ble at
https://github.com/tangjiapeng/SkeletonNet.
\\ ( https://arxiv.org/abs/2008.05742 ,  8813kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05743
Date: Thu, 13 Aug 2020 08:01:48 GMT   (8363kb,D)

Title: Pose Estimation for Vehicle-mounted Cameras via Horizontal and Vertical
  Planes
Authors: Istan Gergo Gal, Daniel Barath, Levente Hajder
Categories: cs.CV
\\
  We propose two novel solvers for estimating the egomotion of a calibrated
camera mounted to a moving vehicle from a single affine correspondence via
recovering special homographies. For the first class of solvers, the sought
plane is expected to be perpendicular to one of the camera axes. For the second
class, the plane is orthogonal to the ground with unknown normal, e.g., it is a
building facade. Both methods are solved via a linear system with a small
coefficient matrix, thus, being extremely efficient. Both the minimal and
over-determined cases can be solved by the proposed methods. They are tested on
synthetic data and on publicly available real-world datasets. The novel methods
are more accurate or comparable to the traditional algorithms and are faster
when included in state of the art robust estimators.
\\ ( https://arxiv.org/abs/2008.05743 ,  8363kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05746
Date: Thu, 13 Aug 2020 08:04:27 GMT   (1219kb,D)

Title: Adversarial Knowledge Transfer from Unlabeled Data
Authors: Akash Gupta, Rameswar Panda, Sujoy Paul, Jianming Zhang, Amit K.
  Roy-Chowdhury
Categories: cs.CV
Comments: Accepted to ACM Multimedia 2020
\\
  While machine learning approaches to visual recognition offer great promise,
most of the existing methods rely heavily on the availability of large
quantities of labeled training data. However, in the vast majority of
real-world settings, manually collecting such large labeled datasets is
infeasible due to the cost of labeling data or the paucity of data in a given
domain. In this paper, we present a novel Adversarial Knowledge Transfer (AKT)
framework for transferring knowledge from internet-scale unlabeled data to
improve the performance of a classifier on a given visual recognition task. The
proposed adversarial learning framework aligns the feature space of the
unlabeled source data with the labeled target data such that the target
classifier can be used to predict pseudo labels on the source data. An
important novel aspect of our method is that the unlabeled source data can be
of different classes from those of the labeled target data, and there is no
need to define a separate pretext task, unlike some existing approaches.
Extensive experiments well demonstrate that models learned using our approach
hold a lot of promise across a variety of visual recognition tasks on multiple
standard datasets.
\\ ( https://arxiv.org/abs/2008.05746 ,  1219kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05763
Date: Thu, 13 Aug 2020 09:02:17 GMT   (32494kb,D)

Title: Powers of layers for image-to-image translation
Authors: Hugo Touvron, Matthijs Douze, Matthieu Cord, Herv\'e J\'egou
Categories: cs.CV eess.IV
\\
  We propose a simple architecture to address unpaired image-to-image
translation tasks: style or class transfer, denoising, deblurring, deblocking,
etc. We start from an image autoencoder architecture with fixed weights. For
each task we learn a residual block operating in the latent space, which is
iteratively called until the target domain is reached. A specific training
schedule is required to alleviate the exponentiation effect of the iterations.
At test time, it offers several advantages: the number of weight parameters is
limited and the compositional design allows one to modulate the strength of the
transformation with the number of iterations. This is useful, for instance,
when the type or amount of noise to suppress is not known in advance.
Experimentally, we provide proofs of concepts showing the interest of our
method for many transformations. The performance of our model is comparable or
better than CycleGAN with significantly fewer parameters.
\\ ( https://arxiv.org/abs/2008.05763 ,  32494kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05770
Date: Thu, 13 Aug 2020 09:26:01 GMT   (1388kb,D)

Title: Weakly Supervised Generative Network for Multiple 3D Human Pose
  Hypotheses
Authors: Chen Li and Gim Hee Lee
Categories: cs.CV
Comments: Accepted to BMVC2020
\\
  3D human pose estimation from a single image is an inverse problem due to the
inherent ambiguity of the missing depth. Several previous works addressed the
inverse problem by generating multiple hypotheses. However, these works are
strongly supervised and require ground truth 2D-to-3D correspondences which can
be difficult to obtain. In this paper, we propose a weakly supervised deep
generative network to address the inverse problem and circumvent the need for
ground truth 2D-to-3D correspondences. To this end, we design our network to
model a proposal distribution which we use to approximate the unknown
multi-modal target posterior distribution. We achieve the approximation by
minimizing the KL divergence between the proposal and target distributions, and
this leads to a 2D reprojection error and a prior loss term that can be weakly
supervised. Furthermore, we determine the most probable solution as the
conditional mode of the samples using the mean-shift algorithm. We evaluate our
method on three benchmark datasets -- Human3.6M, MPII and MPI-INF-3DHP.
Experimental results show that our approach is capable of generating multiple
feasible hypotheses and achieves state-of-the-art results compared to existing
weakly supervised approaches. Our source code is available at the project
website.
\\ ( https://arxiv.org/abs/2008.05770 ,  1388kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05772
Date: Thu, 13 Aug 2020 09:30:12 GMT   (7520kb,D)

Title: CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration
Authors: Boah Kim, Dong Hwan Kim, Seong Ho Park, Jieun Kim, June-Goo Lee, Jong
  Chul Ye
Categories: cs.CV cs.LG eess.IV stat.ML
\\
  Image registration is a fundamental task in medical image analysis. Recently,
deep learning based image registration methods have been extensively
investigated due to their excellent performance despite the ultra-fast
computational time. However, the existing deep learning methods still have
limitation in the preservation of original topology during the deformation with
registration vector fields. To address this issues, here we present a
cycle-consistent deformable image registration. The cycle consistency enhances
image registration performance by providing an implicit regularization to
preserve topology during the deformation. The proposed method is so flexible
that can be applied for both 2D and 3D registration problems for various
applications, and can be easily extended to multi-scale implementation to deal
with the memory issues in large volume registration. Experimental results on
various datasets from medical and non-medical applications demonstrate that the
proposed method provides effective and accurate registration on diverse image
pairs within a few seconds. Qualitative and quantitative evaluations on
deformation fields also verify the effectiveness of the cycle consistency of
the proposed method.
\\ ( https://arxiv.org/abs/2008.05772 ,  7520kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05785
Date: Thu, 13 Aug 2020 10:01:07 GMT   (5144kb,D)

Title: Predicting Visual Overlap of Images Through Interpretable Non-Metric Box
  Embeddings
Authors: Anita Rau, Guillermo Garcia-Hernando, Danail Stoyanov, Gabriel J.
  Brostow, Daniyar Turmukhambetov
Categories: cs.CV cs.LG
Comments: ECCV 2020
\\
  To what extent are two images picturing the same 3D surfaces? Even when this
is a known scene, the answer typically requires an expensive search across
scale space, with matching and geometric verification of large sets of local
features. This expense is further multiplied when a query image is evaluated
against a gallery, e.g. in visual relocalization. While we don't obviate the
need for geometric verification, we propose an interpretable image-embedding
that cuts the search in scale space to essentially a lookup.
  Our approach measures the asymmetric relation between two images. The model
then learns a scene-specific measure of similarity, from training examples with
known 3D visible-surface overlaps. The result is that we can quickly identify,
for example, which test image is a close-up version of another, and by what
scale factor. Subsequently, local features need only be detected at that scale.
We validate our scene-specific model by showing how this embedding yields
competitive image-matching results, while being simpler, faster, and also
interpretable by humans.
\\ ( https://arxiv.org/abs/2008.05785 ,  5144kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05787
Date: Thu, 13 Aug 2020 10:02:02 GMT   (9293kb,D)

Title: Shift Equivariance in Object Detection
Authors: Marco Manfredi and Yu Wang
Categories: cs.CV
Comments: Accepted at ECCV 2020 Workshop: Beyond mAP: Reassessing the
  Evaluation of Object Detectors
\\
  Robustness to small image translations is a highly desirable property for
object detectors. However, recent works have shown that CNN-based classifiers
are not shift invariant. It is unclear to what extent this could impact object
detection, mainly because of the architectural differences between the two and
the dimensionality of the prediction space of modern detectors. To assess shift
equivariance of object detection models end-to-end, in this paper we propose an
evaluation metric, built upon a greedy search of the lower and upper bounds of
the mean average precision on a shifted image set. Our new metric shows that
modern object detection architectures, no matter if one-stage or two-stage,
anchor-based or anchor-free, are sensitive to even one pixel shift to the input
images. Furthermore, we investigate several possible solutions to this problem,
both taken from the literature and newly proposed, quantifying the
effectiveness of each one with the suggested metric. Our results indicate that
none of these methods can provide full shift equivariance. Measuring and
analyzing the extent of shift variance of different models and the
contributions of possible factors, is a first step towards being able to devise
methods that mitigate or even leverage such variabilities.
\\ ( https://arxiv.org/abs/2008.05787 ,  9293kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05826
Date: Thu, 13 Aug 2020 11:31:23 GMT   (28332kb,D)

Title: Localizing the Common Action Among a Few Videos
Authors: Pengwan Yang, Vincent Tao Hu, Pascal Mettes, Cees G. M. Snoek
Categories: cs.CV cs.LG eess.IV
Comments: ECCV 2020
\\
  This paper strives to localize the temporal extent of an action in a long
untrimmed video. Where existing work leverages many examples with their start,
their ending, and/or the class of the action during training time, we propose
few-shot common action localization. The start and end of an action in a long
untrimmed video is determined based on just a hand-full of trimmed video
examples containing the same action, without knowing their common class label.
To address this task, we introduce a new 3D convolutional network architecture
able to align representations from the support videos with the relevant query
video segments. The network contains: (\textit{i}) a mutual enhancement module
to simultaneously complement the representation of the few trimmed support
videos and the untrimmed query video; (\textit{ii}) a progressive alignment
module that iteratively fuses the support videos into the query branch; and
(\textit{iii}) a pairwise matching module to weigh the importance of different
support videos. Evaluation of few-shot common action localization in untrimmed
videos containing a single or multiple action instances demonstrates the
effectiveness and general applicability of our proposal.
\\ ( https://arxiv.org/abs/2008.05826 ,  28332kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05856
Date: Thu, 13 Aug 2020 12:22:27 GMT   (2130kb,D)

Title: Recurrent Deconvolutional Generative Adversarial Networks with
  Application to Text Guided Video Generation
Authors: Hongyuan Yu, Yan Huang, Lihong Pi, Liang Wang
Categories: cs.CV
\\
  This paper proposes a novel model for video generation and especially makes
the attempt to deal with the problem of video generation from text
descriptions, i.e., synthesizing realistic videos conditioned on given texts.
Existing video generation methods cannot be easily adapted to handle this task
well, due to the frame discontinuity issue and their text-free generation
schemes. To address these problems, we propose a recurrent deconvolutional
generative adversarial network (RD-GAN), which includes a recurrent
deconvolutional network (RDN) as the generator and a 3D convolutional neural
network (3D-CNN) as the discriminator. The RDN is a deconvolutional version of
conventional recurrent neural network, which can well model the long-range
temporal dependency of generated video frames and make good use of conditional
information. The proposed model can be jointly trained by pushing the RDN to
generate realistic videos so that the 3D-CNN cannot distinguish them from real
ones. We apply the proposed RD-GAN to a series of tasks including conventional
video generation, conditional video generation, video prediction and video
classification, and demonstrate its effectiveness by achieving well
performance.
\\ ( https://arxiv.org/abs/2008.05856 ,  2130kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05861
Date: Thu, 13 Aug 2020 12:40:24 GMT   (2705kb,D)

Title: Self-supervised Video Representation Learning by Pace Prediction
Authors: Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu
Categories: cs.CV
Comments: Accepted by ECCV 2020
\\
  This paper addresses the problem of self-supervised video representation
learning from a new perspective -- by video pace prediction. It stems from the
observation that human visual system is sensitive to video pace, e.g., slow
motion, a widely used technique in film making. Specifically, given a video
played in natural pace, we randomly sample training clips in different paces
and ask a neural network to identify the pace for each video clip. The
assumption here is that the network can only succeed in such a pace reasoning
task when it understands the underlying video content and learns representative
spatio-temporal features. In addition, we further introduce contrastive
learning to push the model towards discriminating different paces by maximizing
the agreement on similar video content. To validate the effectiveness of the
proposed method, we conduct extensive experiments on action recognition and
video retrieval tasks with several alternative network architectures.
Experimental evaluations show that our approach achieves state-of-the-art
performance for self-supervised video representation learning across different
network architectures and different benchmarks. The code and pre-trained models
are available at \url{https://github.com/laura-wang/video-pace}.
\\ ( https://arxiv.org/abs/2008.05861 ,  2705kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05865
Date: Thu, 13 Aug 2020 12:51:17 GMT   (5736kb,D)

Title: DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image
  Synthesis
Authors: Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Fei Wu, Xiao-Yuan Jing
Categories: cs.CV
\\
  Synthesizing high-resolution realistic images from text descriptions is a
challenging task. Almost all existing text-to-image methods employ stacked
generative adversarial networks as the backbone, utilize cross-modal attention
mechanisms to fuse text and image features, and use extra networks to ensure
text-image semantic consistency. The existing text-to-image models have three
problems: 1) For the backbone, there are multiple generators and discriminators
stacked for generating different scales of images making the training process
slow and inefficient. 2) For semantic consistency, the existing models employ
extra networks to ensure the semantic consistency increasing the training
complexity and bringing an additional computational cost. 3) For the text-image
feature fusion method, cross-modal attention is only applied a few times during
the generation process due to its computational cost impeding fusing the text
and image features deeply. To solve these limitations, we propose 1) a novel
simplified text-to-image backbone which is able to synthesize high-quality
images directly by one pair of generator and discriminator, 2) a novel
regularization method called Matching-Aware zero-centered Gradient Penalty
which promotes the generator to synthesize more realistic and text-image
semantic consistent images without introducing extra networks, 3) a novel
fusion module called Deep Text-Image Fusion Block which can exploit the
semantics of text descriptions effectively and fuse text and image features
deeply during the generation process. Compared with the previous text-to-image
models, our DF-GAN is simpler and more efficient and achieves better
performance. Extensive experiments and ablation studies on both Caltech-UCSD
Birds 200 and COCO datasets demonstrate the superiority of the proposed model
in comparison to state-of-the-art models.
\\ ( https://arxiv.org/abs/2008.05865 ,  5736kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05892
Date: Thu, 13 Aug 2020 13:23:18 GMT   (15273kb,D)

Title: LGNN: a Context-aware Line Segment Detector
Authors: Quan Meng, Jiakai Zhang, Qiang Hu, Xuming He, Jingyi Yu
Categories: cs.CV cs.LG eess.IV
Comments: 9 pages, 7 figures
DOI: 10.1145/3394171.3413784
\\
  We present a novel real-time line segment detection scheme called Line Graph
Neural Network (LGNN). Existing approaches require a computationally expensive
verification or postprocessing step. Our LGNN employs a deep convolutional
neural network (DCNN) for proposing line segment directly, with a graph neural
network (GNN) module for reasoning their connectivities. Specifically, LGNN
exploits a new quadruplet representation for each line segment where the GNN
module takes the predicted candidates as vertexes and constructs a sparse graph
to enforce structural context. Compared with the state-of-the-art, LGNN
achieves near real-time performance without compromising accuracy. LGNN further
enables time-sensitive 3D applications. When a 3D point cloud is accessible, we
present a multi-modal line segment classification technique for extracting a 3D
wireframe of the environment robustly and efficiently.
\\ ( https://arxiv.org/abs/2008.05892 ,  15273kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05924
Date: Thu, 13 Aug 2020 14:10:05 GMT   (4636kb)

Title: DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions
  in the Wild
Authors: Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia,
  Cheng Lu, Jiateng Liu
Categories: cs.CV cs.MM
\\
  Recently, facial expression recognition (FER) in the wild has gained a lot of
researchers' attention because it is a valuable topic to enable the FER
techniques to move from the laboratory to the real applications. In this paper,
we focus on this challenging but interesting topic and make contributions from
three aspects. First, we present a new large-scale 'in-the-wild' dynamic facial
expression database, DFEW (Dynamic Facial Expression in the Wild), consisting
of over 16,000 video clips from thousands of movies. These video clips contain
various challenging interferences in practical scenarios such as extreme
illumination, occlusions, and capricious pose changes. Second, we propose a
novel method called Expression-Clustered Spatiotemporal Feature Learning
(EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct
extensive benchmark experiments on DFEW using a lot of spatiotemporal deep
feature learning methods as well as our proposed EC-STFL. Experimental results
show that DFEW is a well-designed and challenging database, and the proposed
EC-STFL can promisingly improve the performance of existing spatiotemporal deep
neural networks in coping with the problem of dynamic FER in the wild. Our DFEW
database is publicly available and can be freely downloaded from
https://dfew-dataset.github.io/.
\\ ( https://arxiv.org/abs/2008.05924 ,  4636kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05927
Date: Thu, 13 Aug 2020 14:30:12 GMT   (761kb,D)

Title: End-to-end Contextual Perception and Prediction with Interaction
  Transformer
Authors: Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean
  Segal, Raquel Urtasun
Categories: cs.CV cs.RO
Comments: IROS 2020
\\
  In this paper, we tackle the problem of detecting objects in 3D and
forecasting their future motion in the context of self-driving. Towards this
goal, we design a novel approach that explicitly takes into account the
interactions between actors. To capture their spatial-temporal dependencies, we
propose a recurrent neural network with a novel Transformer architecture, which
we call the Interaction Transformer. Importantly, our model can be trained
end-to-end, and runs in real-time. We validate our approach on two challenging
real-world datasets: ATG4D and nuScenes. We show that our approach can
outperform the state-of-the-art on both datasets. In particular, we
significantly improve the social compliance between the estimated future
trajectories, resulting in far fewer collisions between the predicted actors.
\\ ( https://arxiv.org/abs/2008.05927 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05938
Date: Thu, 13 Aug 2020 14:47:50 GMT   (3074kb)

Title: On failures of RGB cameras and their effects in autonomous driving
  applications
Authors: Francesco Secci, Andrea Ceccarelli
Categories: cs.CV cs.LG cs.SE
Comments: preprint - accepted to the The 31st International Symposium on
  Software Reliability Engineering (ISSRE 2020)
\\
  RGB cameras are arguably one of the most relevant sensors for autonomous
driving applications. It is undeniable that failures of vehicle cameras may
compromise the autonomous driving task, possibly leading to unsafe behaviors
when images that are subsequently processed by the driving system are altered.
To support the definition of safe and robust vehicle architectures and
intelligent systems, in this paper we define the failures model of a vehicle
camera, together with an analysis of effects and known mitigations. Further, we
build a software library for the generation of the corresponding failed images
and we feed them to the trained agent of an autonomous driving simulator: the
misbehavior of the trained agent allows a better understanding of failures
effects and especially of the resulting safety risk.
\\ ( https://arxiv.org/abs/2008.05938 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05948
Date: Tue, 11 Aug 2020 18:50:38 GMT   (4072kb,D)

Title: Estimating Magnitude and Phase of Automotive Radar Signals under
  Multiple Interference Sources with Fully Convolutional Networks
Authors: Nicolae-C\u{a}t\u{a}lin Ristea, Andrei Anghel, Radu Tudor Ionescu
Categories: cs.CV
Comments: arXiv admin note: text overlap with arXiv:2007.11102
\\
  Radar sensors are gradually becoming a wide-spread equipment for road
vehicles, playing a crucial role in autonomous driving and road safety. The
broad adoption of radar sensors increases the chance of interference among
sensors from different vehicles, generating corrupted range profiles and
range-Doppler maps. In order to extract distance and velocity of multiple
targets from range-Doppler maps, the interference affecting each range profile
needs to be mitigated. In this paper, we propose a fully convolutional neural
network for automotive radar interference mitigation. In order to train our
network in a real-world scenario, we introduce a new data set of realistic
automotive radar signals with multiple targets and multiple interferers. To our
knowledge, this is the first work to mitigate interference from multiple
sources. Furthermore, we introduce a new training regime that eliminates noisy
weights, showing superior results compared to the widely-used dropout. While
some previous works successfully estimated the magnitude of automotive radar
signals, we are the first to propose a deep learning model that can accurately
estimate the phase. For instance, our novel approach reduces the phase
estimation error with respect to the commonly-adopted zeroing technique by
half, from 12.55 degrees to 6.58 degrees. Considering the lack of databases for
automotive radar interference mitigation, we release as open source our
large-scale data set that closely replicates the real-world automotive scenario
for multiple interference cases, allowing others to objectively compare their
future work in this domain. Our data set is available for download at:
http://github.com/ristea/arim-v2.
\\ ( https://arxiv.org/abs/2008.05948 ,  4072kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05955
Date: Wed, 12 Aug 2020 00:14:19 GMT   (23437kb,D)

Title: SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition with
  Distractors
Authors: Mona Jalal, Josef Spjut, Ben Boudaoud, Margrit Betke
Categories: cs.CV cs.GR cs.LG cs.RO eess.IV
Comments: 3 pages, 4 figures, 1 table, Accepted at CVPR 2019 Workshop
DOI: 10.1109/CVPRW.2019.00063
\\
  We present a new, publicly-available image dataset generated by the NVIDIA
Deep Learning Data Synthesizer intended for use in object detection, pose
estimation, and tracking applications. This dataset contains 144k stereo image
pairs that synthetically combine 18 camera viewpoints of three photorealistic
virtual environments with up to 10 objects (chosen randomly from the 21 object
models of the YCB dataset [1]) and flying distractors. Object and camera pose,
scene lighting, and quantity of objects and distractors were randomized. Each
provided view includes RGB, depth, segmentation, and surface normal images, all
pixel level. We describe our approach for domain randomization and provide
insight into the decisions that produced the dataset.
\\ ( https://arxiv.org/abs/2008.05955 ,  23437kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05977
Date: Thu, 13 Aug 2020 15:51:42 GMT   (3933kb,D)

Title: Hybrid Dynamic-static Context-aware Attention Network for Action
  Assessment in Long Videos
Authors: Ling-An Zeng, Fa-Ting Hong, Wei-Shi Zheng, Qi-Zhi Yu, Wei Zeng,
  Yao-Wei Wang, and Jian-Huang Lai
Categories: cs.CV
Comments: ACM International Conference on Multimedia 2020
ACM-class: I.2.10
DOI: 10.1145/1122445.1122456
\\
  The objective of action quality assessment is to score sports videos.
However, most existing works focus only on video dynamic information (i.e.,
motion information) but ignore the specific postures that an athlete is
performing in a video, which is important for action assessment in long videos.
In this work, we present a novel hybrid dynAmic-static Context-aware attenTION
NETwork (ACTION-NET) for action assessment in long videos. To learn more
discriminative representations for videos, we not only learn the video dynamic
information but also focus on the static postures of the detected athletes in
specific frames, which represent the action quality at certain moments, along
with the help of the proposed hybrid dynamic-static architecture. Moreover, we
leverage a context-aware attention module consisting of a temporal
instance-wise graph convolutional network unit and an attention unit for both
streams to extract more robust stream features, where the former is for
exploring the relations between instances and the latter for assigning a proper
weight to each instance. Finally, we combine the features of the two streams to
regress the final video score, supervised by ground-truth scores given by
experts. Additionally, we have collected and annotated the new Rhythmic
Gymnastics dataset, which contains videos of four different types of gymnastics
routines, for evaluation of action quality assessment in long videos. Extensive
experimental results validate the efficacy of our proposed method, which
outperforms related approaches. The codes and dataset are available at
\url{https://github.com/lingan1996/ACTION-NET}.
\\ ( https://arxiv.org/abs/2008.05977 ,  3933kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05981
Date: Thu, 13 Aug 2020 15:56:14 GMT   (2809kb,D)

Title: Black Magic in Deep Learning: How Human Skill Impacts Network Training
Authors: Kanav Anand, Ziqi Wang, Marco Loog, Jan van Gemert
Categories: cs.CV
Comments: presented at the British Machine Vision Conference, 2020
\\
  How does a user's prior experience with deep learning impact accuracy? We
present an initial study based on 31 participants with different levels of
experience. Their task is to perform hyperparameter optimization for a given
deep learning architecture. The results show a strong positive correlation
between the participant's experience and the final performance. They
additionally indicate that an experienced participant finds better solutions
using fewer resources on average. The data suggests furthermore that
participants with no prior experience follow random strategies in their pursuit
of optimal hyperparameters. Our study investigates the subjective human factor
in comparisons of state of the art results and scientific reproducibility in
deep learning.
\\ ( https://arxiv.org/abs/2008.05981 ,  2809kb)
------------------------------------------------------------------------------
\\
arXiv:2008.06020
Date: Thu, 13 Aug 2020 17:20:02 GMT   (1323kb,D)

Title: Testing the Safety of Self-driving Vehicles by Simulating Perception and
  Prediction
Authors: Kelvin Wong, Qiang Zhang, Ming Liang, Bin Yang, Renjie Liao, Abbas
  Sadat, Raquel Urtasun
Categories: cs.CV cs.LG cs.RO
Comments: ECCV 2020
\\
  We present a novel method for testing the safety of self-driving vehicles in
simulation. We propose an alternative to sensor simulation, as sensor
simulation is expensive and has large domain gaps. Instead, we directly
simulate the outputs of the self-driving vehicle's perception and prediction
system, enabling realistic motion planning testing. Specifically, we use paired
data in the form of ground truth labels and real perception and prediction
outputs to train a model that predicts what the online system will produce.
Importantly, the inputs to our system consists of high definition maps,
bounding boxes, and trajectories, which can be easily sketched by a test
engineer in a matter of minutes. This makes our approach a much more scalable
solution. Quantitative results on two large-scale datasets demonstrate that we
can realistically test motion planning using our simulations.
\\ ( https://arxiv.org/abs/2008.06020 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2008.06021
Date: Thu, 13 Aug 2020 17:22:46 GMT   (1358kb,D)

Title: BioMetricNet: deep unconstrained face verification through learning of
  metrics regularized onto Gaussian distributions
Authors: Arslan Ali, Matteo Testa, Tiziano Bianchi, Enrico Magli
Categories: cs.CV cs.LG
Comments: Accepted at ECCV20
\\
  We present BioMetricNet: a novel framework for deep unconstrained face
verification which learns a regularized metric to compare facial features.
Differently from popular methods such as FaceNet, the proposed approach does
not impose any specific metric on facial features; instead, it shapes the
decision space by learning a latent representation in which matching and
non-matching pairs are mapped onto clearly separated and well-behaved target
distributions. In particular, the network jointly learns the best feature
representation, and the best metric that follows the target distributions, to
be used to discriminate face images. In this paper we present this general
framework, first of its kind for facial verification, and tailor it to Gaussian
distributions. This choice enables the use of a simple linear decision boundary
that can be tuned to achieve the desired trade-off between false alarm and
genuine acceptance rate, and leads to a loss function that can be written in
closed form. Extensive analysis and experimentation on publicly available
datasets such as Labeled Faces in the wild (LFW), Youtube faces (YTF),
Celebrities in Frontal-Profile in the Wild (CFP), and challenging datasets like
cross-age LFW (CALFW), cross-pose LFW (CPLFW), In-the-wild Age Dataset (AgeDB)
show a significant performance improvement and confirms the effectiveness and
superiority of BioMetricNet over existing state-of-the-art methods.
\\ ( https://arxiv.org/abs/2008.06021 ,  1358kb)
------------------------------------------------------------------------------
\\
arXiv:2008.06035
Date: Thu, 13 Aug 2020 17:47:41 GMT   (2080kb,D)

Title: Towards Visually Explaining Similarity Models
Authors: Meng Zheng and Srikrishna Karanam and Terrence Chen and Richard J.
  Radke and Ziyan Wu
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: 14 pages, 6 figures, 3 tables
\\
  We consider the problem of visually explaining similarity models, i.e.,
explaining why a model predicts two images to be similar in addition to
producing a scalar score. While much recent work in visual model
interpretability has focused on gradient-based attention, these methods rely on
a classification module to generate visual explanations. Consequently, they
cannot readily explain other kinds of models that do not use or need
classification-like loss functions (e.g., similarity models trained with a
metric learning loss). In this work, we bridge this crucial gap, presenting the
first method to generate gradient-based visual explanations for image
similarity predictors. By relying solely on the learned feature embedding, we
show that our approach can be applied to any kind of CNN-based similarity
architecture, an important step towards generic visual explainability. We show
that our resulting visual explanations serve more than just interpretability;
they can be infused into the model learning process itself with new trainable
constraints based on our similarity explanations. We show that the resulting
similarity models perform, and can be visually explained, better than the
corresponding baseline models trained without our explanation constraints. We
demonstrate our approach using extensive experiments on three different kinds
of tasks: generic image retrieval, person re-identification, and low-shot
semantic segmentation.
\\ ( https://arxiv.org/abs/2008.06035 ,  2080kb)
------------------------------------------------------------------------------
\\
arXiv:2008.06041
Date: Thu, 13 Aug 2020 17:54:06 GMT   (34070kb,D)

Title: DSDNet: Deep Structured self-Driving Network
Authors: Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin Yang, Raquel
  Urtasun
Categories: cs.CV
Comments: ECCV 2020
\\
  In this paper, we propose the Deep Structured self-Driving Network (DSDNet),
which performs object detection, motion prediction, and motion planning with a
single neural network. Towards this goal, we develop a deep structured energy
based model which considers the interactions between actors and produces
socially consistent multimodal future predictions. Furthermore, DSDNet
explicitly exploits the predicted future distributions of actors to plan a safe
maneuver by using a structured planning cost. Our sample-based formulation
allows us to overcome the difficulty in probabilistic inference of continuous
random variables. Experiments on a number of large-scale self driving datasets
demonstrate that our model significantly outperforms the state-of-the-art.
\\ ( https://arxiv.org/abs/2008.06041 ,  34070kb)
------------------------------------------------------------------------------
\\
arXiv:2008.06046
Date: Thu, 13 Aug 2020 17:59:11 GMT   (3550kb,D)

Title: Full-Body Awareness from Partial Observations
Authors: Chris Rockwell, David F. Fouhey
Categories: cs.CV
Comments: In ECCV 2020
\\
  There has been great progress in human 3D mesh recovery and great interest in
learning about the world from consumer video data. Unfortunately current
methods for 3D human mesh recovery work rather poorly on consumer video data,
since on the Internet, unusual camera viewpoints and aggressive truncations are
the norm rather than a rarity. We study this problem and make a number of
contributions to address it: (i) we propose a simple but highly effective
self-training framework that adapts human 3D mesh recovery systems to consumer
videos and demonstrate its application to two recent systems; (ii) we introduce
evaluation protocols and keypoint annotations for 13K frames across four
consumer video datasets for studying this task, including evaluations on
out-of-image keypoints; and (iii) we show that our method substantially
improves PCK and human-subject judgments compared to baselines, both on test
videos from the dataset it was trained on, as well as on three other datasets
without further adaptation. Project website:
https://crockwell.github.io/partial_humans
\\ ( https://arxiv.org/abs/2008.06046 ,  3550kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05504
Date: Wed, 12 Aug 2020 18:09:29 GMT   (286kb,D)

Title: On the tree-width of even-hole-free graphs
Authors: Pierre Aboulker, Isolde Adler, Eun Jung Kim, Ni Luh Dewi Sintiari,
  Nicolas Trotignon
Categories: cs.DM cs.DS math.CO
\\
  The class of all even-hole-free graphs has unbounded tree-width, as it
contains all complete graphs. Recently, a class of (even-hole, $K_4$)-free
graphs was constructed, that still has unbounded tree-width [Sintiari and
Trotignon, 2019]. The class has unbounded degree and contains arbitrarily large
clique-minors. We ask whether this is necessary.
  We prove that for every graph $G$, if $G$ excludes a fixed graph $H$ as a
minor, then $G$ either has small tree-width, or $G$ contains a large wall or
the line graph of a large wall as induced subgraph. This can be seen as a
strengthening of Robertson and Seymour's excluded grid theorem for the case of
minor-free graphs. Our theorem implies that every class of even-hole-free
graphs excluding a fixed graph as a minor has bounded tree-width. In fact, our
theorem applies to a more general class: (theta, prism)-free graphs. This
implies the known result that planar even hole-free graph have bounded
tree-width [da Silva and Linhares Sales, Discrete Applied Mathematics 2010].
  We conjecture that even-hole-free graphs of bounded degree have bounded
tree-width. If true, this would mean that even-hole-freeness is testable in the
bounded-degree graph model of property testing. We prove the conjecture for
subcubic graphs and we give a bound on the tree-width of the class of (even
hole, pyramid)-free graphs of degree at most 4.
\\ ( https://arxiv.org/abs/2008.05504 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05801
Date: Thu, 13 Aug 2020 10:22:53 GMT   (106kb,D)

Title: On graphs of bounded degree that are far from being Hamiltonian
Authors: Isolde Adler (1) and Noleen K\"ohler (1) ((1) University of Leeds)
Categories: cs.DM cs.CC cs.DS
Comments: 15 pages, 4 figures
ACM-class: G.2.2
\\
  Hamiltonian cycles in graphs were first studied in the 1850s. Since then, an
impressive amount of research has been dedicated to identifying classes of
graphs that allow Hamiltonian cycles, and to related questions. The
corresponding decision problem, that asks whether a given graph is Hamiltonian
(i. e. admits a Hamiltonian cycle), is one of Karp's famous NP-complete
problems. It remains NP-complete on planar cubic graphs. In this paper we study
graphs of bounded degree that are far from being Hamiltonian, where a graph G
on n vertices is far from being Hamiltonian, if modifying a constant fraction
of n edges is necessary to make G Hamiltonian. We exhibit classes of graphs of
bounded degree that are locally Hamiltonian, i.e. every subgraph induced by the
neighbourhood of a small vertex set appears in some Hamiltonian graph, but that
are far from being Hamiltonian. We then use these classes to obtain a lower
bound in property testing. We show that in the bounded-degree graph model,
Hamiltonicity is not testable with one-sided error probability and query
complexity o(n). This contrasts the known fact that on planar (or minor-free)
graph classes, Hamiltonicity is testable with constant query complexity in the
bounded-degree graph model with two-sided error. Our proof is an intricate
construction that shows how to turn a d-regular graph into a graph that is far
from being Hamiltonian, and we use d-regular expander graphs to maintain local
Hamiltonicity.
\\ ( https://arxiv.org/abs/2008.05801 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05567
Date: Wed, 12 Aug 2020 20:44:56 GMT   (9345kb,D)

Title: Procedural Urban Forestry
Authors: Till Niese, S\"oren Pirk, Bedrich Benes, Oliver Deussen
Categories: cs.GR cs.CV cs.LG
Comments: 14 pages
\\
  The placement of vegetation plays a central role in the realism of virtual
scenes. We introduce procedural placement models (PPMs) for vegetation in urban
layouts. PPMs are environmentally sensitive to city geometry and allow
identifying plausible plant positions based on structural and functional zones
in an urban layout. PPMs can either be directly used by defining their
parameters or can be learned from satellite images and land register data.
Together with approaches for generating buildings and trees, this allows us to
populate urban landscapes with complex 3D vegetation. The effectiveness of our
framework is shown through examples of large-scale city scenes and close-ups of
individually grown tree models; we also validate it by a perceptual user study.
\\ ( https://arxiv.org/abs/2008.05567 ,  9345kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05872
Date: Thu, 13 Aug 2020 13:08:30 GMT   (762kb,D)

Title: Motion Similarity Modeling -- A State of the Art Report
Authors: Anna Sebernegg, Peter K\'an, Hannes Kaufmann
Categories: cs.GR cs.CV cs.HC
Report-no: VR-TR-001
\\
  The analysis of human motion opens up a wide range of possibilities, such as
realistic training simulations or authentic motions in robotics or animation.
One of the problems underlying motion analysis is the meaningful comparison of
actions based on similarity measures. Since the motion analysis is
application-dependent, it is essential to find the appropriate motion
similarity method for the particular use case. This state of the art report
provides an overview of human motion analysis and different similarity modeling
methods, while mainly focusing on approaches that work with 3D motion data. The
survey summarizes various similarity aspects and features of motion and
describes approaches to measuring the similarity between two actions.
\\ ( https://arxiv.org/abs/2008.05872 ,  762kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2008.05584 (*cross-listing*)
Date: Wed, 12 Aug 2020 21:56:51 GMT   (426kb,D)

Title: Graph Drawing via Gradient Descent, $(GD)^2$
Authors: Reyan Ahmed, Felice De Luca, Sabin Devkota, Stephen Kobourov, Mingwei
  Li
Categories: cs.DS cs.CG
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\
  Readability criteria, such as distance or neighborhood preservation, are
often used to optimize node-link representations of graphs to enable the
comprehension of the underlying data. With few exceptions, graph drawing
algorithms typically optimize one such criterion, usually at the expense of
others. We propose a layout approach, Graph Drawing via Gradient Descent,
$(GD)^2$, that can handle multiple readability criteria. $(GD)^2$ can optimize
any criterion that can be described by a smooth function. If the criterion
cannot be captured by a smooth function, a non-smooth function for the
criterion is combined with another smooth function, or auto-differentiation
tools are used for the optimization. Our approach is flexible and can be used
to optimize several criteria that have already been considered earlier (e.g.,
obtaining ideal edge lengths, stress, neighborhood preservation) as well as
other criteria which have not yet been explicitly optimized in such fashion
(e.g., vertex resolution, angular resolution, aspect ratio). We provide
quantitative and qualitative evidence of the effectiveness of $(GD)^2$ with
experimental data and a functional prototype:
\url{http://hdc.cs.arizona.edu/~mwli/graph-drawing/}.
\\ ( https://arxiv.org/abs/2008.05584 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05642 (*cross-listing*)
Date: Thu, 13 Aug 2020 01:52:40 GMT   (3360kb,D)

Title: Towards Modality Transferable Visual Information Representation with
  Optimal Model Compression
Authors: Rongqun Lin, Linwei Zhu, Shiqi Wang and Sam Kwong
Categories: eess.IV cs.CV cs.LG cs.MM
Comments: Accepted in ACM Multimedia 2020
\\
  Compactly representing the visual signals is of fundamental importance in
various image/video-centered applications. Although numerous approaches were
developed for improving the image and video coding performance by removing the
redundancies within visual signals, much less work has been dedicated to the
transformation of the visual signals to another well-established modality for
better representation capability. In this paper, we propose a new scheme for
visual signal representation that leverages the philosophy of transferable
modality. In particular, the deep learning model, which characterizes and
absorbs the statistics of the input scene with online training, could be
efficiently represented in the sense of rate-utility optimization to serve as
the enhancement layer in the bitstream. As such, the overall performance can be
further guaranteed by optimizing the new modality incorporated. The proposed
framework is implemented on the state-of-the-art video coding standard (i.e.,
versatile video coding), and significantly better representation capability has
been observed based on extensive evaluations.
\\ ( https://arxiv.org/abs/2008.05642 ,  3360kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05753 (*cross-listing*)
Date: Thu, 13 Aug 2020 08:30:23 GMT   (4956kb,D)

Title: AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT
  Denoising
Authors: Jawook Gu, Jong Chul Ye
Categories: eess.IV cs.CV cs.LG stat.ML
Comments: 12 pages, 10 figures
\\
  Recently, deep learning approaches have been extensively studied for low-dose
CT denoising thanks to its superior performance despite the fast computational
time. In particular, cycleGAN has been demonstrated as a powerful unsupervised
learning scheme to improve the low-dose CT image quality without requiring
matched high-dose reference data. Unfortunately, one of the main limitations of
the cycleGAN approach is that it requires two deep neural network generators at
the training phase, although only one of them is used at the inference phase.
The secondary auxiliary generator is needed to enforce the cycle-consistency,
but the additional memory requirement and increases of the learnable parameters
are the main huddles for cycleGAN training. To address this issue, here we
propose a novel cycleGAN architecture using a single switchable generator. In
particular, a single generator is implemented using adaptive instance
normalization (AdaIN) layers so that the baseline generator converting a
low-dose CT image to a routine-dose CT image can be switched to a generator
converting high-dose to low-dose by simply changing the AdaIN code. Thanks to
the shared baseline network, the additional memory requirement and weight
increases are minimized, and the training can be done more stably even with
small training data. Experimental results show that the proposed method
outperforms the previous cycleGAN approaches while using only about half the
parameters.
\\ ( https://arxiv.org/abs/2008.05753 ,  4956kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05765 (*cross-listing*)
Date: Thu, 13 Aug 2020 09:09:37 GMT   (16085kb,D)

Title: Revisiting Temporal Modeling for Video Super-resolution
Authors: Takashi Isobe, Fang Zhu and Shengjin Wang
Categories: eess.IV cs.CV
Comments: BMVC 2020
\\
  Video super-resolution plays an important role in surveillance video analysis
and ultra-high-definition video display, which has drawn much attention in both
the research and industrial communities. Although many deep learning-based VSR
methods have been proposed, it is hard to directly compare these methods since
the different loss functions and training datasets have a significant impact on
the super-resolution results. In this work, we carefully study and compare
three temporal modeling methods (2D CNN with early fusion, 3D CNN with slow
fusion and Recurrent Neural Network) for video super-resolution. We also
propose a novel Recurrent Residual Network (RRN) for efficient video
super-resolution, where residual learning is utilized to stabilize the training
of RNN and meanwhile to boost the super-resolution performance. Extensive
experiments show that the proposed RRN is highly computational efficiency and
produces temporal consistent VSR results with finer details than other temporal
modeling methods. Besides, the proposed method achieves state-of-the-art
results on several widely used benchmarks.
\\ ( https://arxiv.org/abs/2008.05765 ,  16085kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05767 (*cross-listing*)
Date: Thu, 13 Aug 2020 09:19:57 GMT   (498kb,D)

Title: Weight Equalizing Shift Scaler-Coupled Post-training Quantization
Authors: Jihun Oh, SangJeong Lee, Meejeong Park, Pooni Walagaurav and Kiseok
  Kwon
Categories: cs.LG cs.CV stat.ML
Comments: 9 pages, 4 figures, 4 tables
\\
  Post-training, layer-wise quantization is preferable because it is free from
retraining and is hardware-friendly. Nevertheless, accuracy degradation has
occurred when a neural network model has a big difference of per-out-channel
weight ranges. In particular, the MobileNet family has a tragedy drop in top-1
accuracy from 70.60% ~ 71.87% to 0.1% on the ImageNet dataset after 8-bit
weight quantization. To mitigate this significant accuracy reduction, we
propose a new weight equalizing shift scaler, i.e. rescaling the weight range
per channel by a 4-bit binary shift, prior to a layer-wise quantization. To
recover the original output range, inverse binary shifting is efficiently fused
to the existing per-layer scale compounding in the fixed-computing
convolutional operator of the custom neural processing unit. The binary shift
is a key feature of our algorithm, which significantly improved the accuracy
performance without impeding the memory footprint. As a result, our proposed
method achieved a top-1 accuracy of 69.78% ~ 70.96% in MobileNets and showed
robust performance in varying network models and tasks, which is competitive to
channel-wise quantization results.
\\ ( https://arxiv.org/abs/2008.05767 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05780 (*cross-listing*)
Date: Thu, 13 Aug 2020 09:57:04 GMT   (844kb,D)

Title: Multi-Modality Pathology Segmentation Framework: Application to Cardiac
  Magnetic Resonance Images
Authors: Zhen Zhang, Chenyu Liu, Wangbin Ding, Sihan Wang, Chenhao Pei,
  Mingjing Yang, Liqin Huang
Categories: eess.IV cs.CV
Comments: 12 pages,MyoPS 2020
\\
  Multi-sequence of cardiac magnetic resonance (CMR) images can provide
complementary information for myocardial pathology (scar and edema). However,
it is still challenging to fuse these underlying information for pathology
segmentation effectively. This work presents an automatic cascade pathology
segmentation framework based on multi-modality CMR images. It mainly consists
of two neural networks: an anatomical structure segmentation network (ASSN) and
a pathological region segmentation network (PRSN). Specifically, the ASSN aims
to segment the anatomical structure where the pathology may exist, and it can
provide a spatial prior for the pathological region segmentation. In addition,
we integrate a denoising auto-encoder (DAE) into the ASSN to generate
segmentation results with plausible shapes. The PRSN is designed to segment
pathological region based on the result of ASSN, in which a fusion block based
on channel attention is proposed to better aggregate multi-modality information
from multi-modality CMR images. Experiments from the MyoPS2020 challenge
dataset show that our framework can achieve promising performance for
myocardial scar and edema segmentation.
\\ ( https://arxiv.org/abs/2008.05780 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05789 (*cross-listing*)
Date: Thu, 13 Aug 2020 10:08:12 GMT   (2036kb,D)

Title: Look, Listen, and Attend: Co-Attention Network for Self-Supervised
  Audio-Visual Representation Learning
Authors: Ying Cheng, Ruize Wang, Zhihao Pan, Rui Feng, Yuejie Zhang
Categories: cs.MM cs.AI cs.CV cs.LG
Comments: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020)
\\
  When watching videos, the occurrence of a visual event is often accompanied
by an audio event, e.g., the voice of lip motion, the music of playing
instruments. There is an underlying correlation between audio and visual
events, which can be utilized as free supervised information to train a neural
network by solving the pretext task of audio-visual synchronization. In this
paper, we propose a novel self-supervised framework with co-attention mechanism
to learn generic cross-modal representations from unlabelled videos in the
wild, and further benefit downstream tasks. Specifically, we explore three
different co-attention modules to focus on discriminative visual regions
correlated to the sounds and introduce the interactions between them.
Experiments show that our model achieves state-of-the-art performance on the
pretext task while having fewer parameters compared with existing methods. To
further evaluate the generalizability and transferability of our approach, we
apply the pre-trained model on two downstream tasks, i.e., sound source
localization and action recognition. Extensive experiments demonstrate that our
model provides competitive results with other self-supervised methods, and also
indicate that our approach can tackle the challenging scenes which contain
multiple sound sources.
\\ ( https://arxiv.org/abs/2008.05789 ,  2036kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05930 (*cross-listing*)
Date: Thu, 13 Aug 2020 14:40:46 GMT   (36963kb,D)

Title: Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable
  Semantic Representations
Authors: Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan,
  Raquel Urtasun
Categories: cs.RO cs.AI cs.CV cs.LG stat.ML
Comments: European Conference on Computer Vision (ECCV) 2020
\\
  In this paper we propose a novel end-to-end learnable network that performs
joint perception, prediction and motion planning for self-driving vehicles and
produces interpretable intermediate representations. Unlike existing neural
motion planners, our motion planning costs are consistent with our perception
and prediction estimates. This is achieved by a novel differentiable semantic
occupancy representation that is explicitly used as cost by the motion planning
process. Our network is learned end-to-end from human demonstrations. The
experiments in a large-scale manual-driving dataset and closed-loop simulation
show that the proposed model significantly outperforms state-of-the-art
planners in imitating the human behaviors while producing much safer
trajectories.
\\ ( https://arxiv.org/abs/2008.05930 ,  36963kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05975 (*cross-listing*)
Date: Thu, 13 Aug 2020 15:45:44 GMT   (1994kb)

Title: Deep Learning to Quantify Pulmonary Edema in Chest Radiographs
Authors: Steven Horng, Ruizhi Liao, Xin Wang, Sandeep Dalal, Polina Golland,
  Seth J Berkowitz
Categories: eess.IV cs.CV
Comments: The two first authors contributed equally
\\
  Background: Clinical management decisions for acutely decompensated CHF
patients are often based on grades of pulmonary edema severity, rather than its
mere absence or presence. The grading of pulmonary edema on chest radiographs
is based on well-known radiologic findings.
  Purpose: We develop a clinical machine learning task to grade pulmonary edema
severity and release both the underlying data and code to serve as a benchmark
for future algorithmic developments in machine vision.
  Materials and Methods: We collected 369,071 chest radiographs and their
associated radiology reports from 64,581 patients from the MIMIC-CXR chest
radiograph dataset. We extracted pulmonary edema severity labels from the
associated radiology reports as 4 ordinal levels: no edema (0), vascular
congestion (1), interstitial edema (2), and alveolar edema (3). We developed
machine learning models using two standard approaches: 1) a semi-supervised
model using a variational autoencoder and 2) a pre-trained supervised learning
model using a dense neural network.
  Results: We measured the area under the receiver operating characteristic
curve (AUROC) from the semi-supervised model and the pre-trained model. AUROC
for differentiating alveolar edema from no edema was 0.99 and 0.87
(semi-supervised and pre-trained models). Performance of the algorithm was
inversely related to the difficulty in categorizing milder states of pulmonary
edema: 2 vs 0 (0.88, 0.81), 1 vs 0 (0.79, 0.66), 3 vs 1 (0.93, 0.82), 2 vs 1
(0.69, 0.73), 3 vs 2 (0.88, 0.63).
  Conclusion: Accurate grading of pulmonary edema on chest radiographs is a
clinically important task. Application of state-of-the-art machine learning
techniques can produce a novel quantitative imaging biomarker from one of the
oldest and most widely available imaging modalities.
\\ ( https://arxiv.org/abs/2008.05975 ,  1994kb)
------------------------------------------------------------------------------
\\
arXiv:2008.06029 (*cross-listing*)
Date: Thu, 13 Aug 2020 17:36:59 GMT   (1612kb)

Title: Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks
  in Highly Accelerated MRI
Authors: Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta
  Ellermann, K\^amil U\u{g}urbil, Mehmet Ak\c{c}akaya
Categories: eess.IV cs.CV cs.LG eess.SP physics.med-ph
\\
  Purpose: To develop an improved self-supervised learning strategy that
efficiently uses the acquired data for training a physics-guided reconstruction
network without a database of fully-sampled data.
  Methods: Currently self-supervised learning for physics-guided reconstruction
networks splits acquired undersampled data into two disjoint sets, where one is
used for data consistency (DC) in the unrolled network and the other to define
the training loss. The proposed multi-mask self-supervised learning via data
undersampling (SSDU) splits acquired measurements into multiple pairs of
disjoint sets for each training sample, while using one of these sets for DC
units and the other for defining loss, thereby more efficiently using the
undersampled data. Multi-mask SSDU is applied on fully-sampled 3D knee and
prospectively undersampled 3D brain MRI datasets, which are retrospectively
subsampled to acceleration rate (R)=8, and compared to CG-SENSE and single-mask
SSDU DL-MRI, as well as supervised DL-MRI when fully-sampled data is available.
  Results: Results on knee MRI show that the proposed multi-mask SSDU
outperforms SSDU and performs closely with supervised DL-MRI, while
significantly outperforming CG-SENSE. A clinical reader study further ranks the
multi-mask SSDU higher than supervised DL-MRI in terms of SNR and aliasing
artifacts. Results on brain MRI show that multi-mask SSDU achieves better
reconstruction quality compared to SSDU and CG-SENSE. Reader study demonstrates
that multi-mask SSDU at R=8 significantly improves reconstruction compared to
single-mask SSDU at R=8, as well as CG-SENSE at R=2.
  Conclusion: The proposed multi-mask SSDU approach enables improved training
of physics-guided neural networks without fully-sampled data, by enabling
efficient use of the undersampled data with multiple masks.
\\ ( https://arxiv.org/abs/2008.06029 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05648 (*cross-listing*)
Date: Thu, 13 Aug 2020 02:18:34 GMT   (30kb)

Title: Cut Sparsification of the Clique Beyond the Ramanujan Bound
Authors: Antares Chen, Jonathan Shi, Luca Trevisan
Categories: cs.DS cs.DM math.CO math.PR
ACM-class: G.2.1; G.2.2; F.2.0
\\
  We prove that a random $d$-regular graph, with high probability, is a cut
sparsifier of the clique with approximation error at most $\left(2\sqrt{\frac 2
\pi} + o_{n,d}(1)\right)/\sqrt d$, where $2\sqrt{\frac 2 \pi} = 1.595\ldots$
and $o_{n,d}(1)$ denotes an error term that depends on $n$ and $d$ and goes to
zero if we first take the limit $n\rightarrow \infty$ and then the limit $d
\rightarrow \infty$.
  This is established by analyzing linear-size cuts using techniques of
Jagannath and Sen \cite{jagannath2017unbalanced} derived from ideas from
statistical physics and analyzing small cuts via martingale inequalities.
  We also prove that every spectral sparsifier of the clique having average
degree $d$ and a certain high "pseudo-girth" property has an approximation
error that is at least the "Ramanujan bound" $(2-o_{n,d}(1))/\sqrt d$, which is
met by $d$-regular Ramanujan graphs, generalizing a lower bound of Srivastava
and Trevisan \cite{ST18}.
  Together, these results imply a separation between spectral sparsification
and cut sparsification. If $G$ is a random $\log n$-regular graph on $n$
vertices, we show that, with high probability, $G$ admits a (weighted subgraph)
cut sparsifier of average degree $d$ and approximation error at most
$(1.595\ldots + o_{n,d}(1))/\sqrt d$, while every (weighted subgraph) spectral
sparsifier of $G$ having average degree $d$ has approximation error at least
$(2-o_{n,d}(1))/\sqrt d$.
\\ ( https://arxiv.org/abs/2008.05648 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05800 (*cross-listing*)
Date: Thu, 13 Aug 2020 10:21:46 GMT   (66kb)

Title: On Testability of First-Order Properties in Bounded-Degree Graphs
Authors: Isolde Adler (1), Noleen K\"ohler (1) and Pan Peng (2) ((1) University
  of Leeds, (2) University of Sheffield)
Categories: cs.LO cs.CC cs.DM cs.DS math.CO
Comments: 37 pages, 4 figures
ACM-class: F.4.1; G.2.2
\\
  We study property testing of properties that are definable in first-order
logic (FO) in the bounded-degree graph and relational structure models. We show
that any FO property that is defined by a formula with quantifier prefix
$\exists^*\forall^*$ is testable (i.e., testable with constant query
complexity), while there exists an FO property that is expressible by a formula
with quantifier prefix $\forall^*\exists^*$ that is not testable. In the dense
graph model, a similar picture is long known (Alon, Fisher, Krivelevich,
Szegedy, Combinatoria 2000), despite the very different nature of the two
models. In particular, we obtain our lower bound by a first-order formula that
defines a class of bounded-degree expanders, based on zig-zag products of
graphs. We expect this to be of independent interest. We then prove testability
of some first-order properties that speak about isomorphism types of
neighbourhoods, including testability of $1$-neighbourhood-freeness, and
$r$-neighbourhood-freeness under a mild assumption on the degrees.
\\ ( https://arxiv.org/abs/2008.05800 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05911 (*cross-listing*)
Date: Thu, 13 Aug 2020 13:44:49 GMT   (274kb)

Title: On the Bipartiteness Constant and Expansion of Cayley Graphs
Authors: Nina Moorman, Peter Ralli, Prasad Tetali
Categories: math.CO cs.DM
Comments: 14 pages, 2 figures
\\
  Let $G$ be a finite, undirected $d$-regular graph and $A(G)$ its normalized
adjacency matrix, with eigenvalues $1 = \lambda_1(A)\geq \dots \ge \lambda_n
\ge -1$. It is a classical fact that $\lambda_n = -1$ if and only if $G$ is
bipartite. Our main result provides a quantitative separation of $\lambda_n$
from $-1$ in the case of Cayley graphs, in terms of their expansion. Denoting
$h_{out}$ by the (outer boundary) vertex expansion of $G$, we show that if $G$
is a non-bipartite Cayley graph (constructed using a group and a symmetric
generating set of size $d$) then $\lambda_n \ge -1 + ch_{out}^2/d^2\,,$ for $c$
an absolute constant. We exhibit graphs for which this result is tight up to a
factor depending on $d$. This improves upon a recent result by Biswas and Saha
who showed $\lambda_n \ge -1 + h_{out}^4/(2^9d^8)\,.$ We also note that such a
result could not be true for general non-bipartite graphs.
\\ ( https://arxiv.org/abs/2008.05911 ,  274kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1810.08310
replaced with revised version Thu, 13 Aug 2020 02:47:37 GMT   (9713kb,D)

Title: An Efficient Data Retrieval Parallel Reeb Graph Algorithm
Authors: Mustafa Hajij, Paul Rosen
Categories: cs.CG cs.CV
\\ ( https://arxiv.org/abs/1810.08310 ,  9713kb)
------------------------------------------------------------------------------
\\
arXiv:1611.07596
replaced with revised version Thu, 13 Aug 2020 17:14:28 GMT   (3771kb,D)

Title: Fast Fourier Color Constancy
Authors: Jonathan T. Barron, Yun-Ta Tsai
Categories: cs.CV
Comments: CVPR 2017
\\ ( https://arxiv.org/abs/1611.07596 ,  3771kb)
------------------------------------------------------------------------------
\\
arXiv:1904.02920
replaced with revised version Thu, 13 Aug 2020 06:44:45 GMT   (4852kb,D)

Title: Branched Multi-Task Networks: Deciding What Layers To Share
Authors: Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere and Luc
  Van Gool
Categories: cs.CV
Comments: Accepted at BMVC 2020
\\ ( https://arxiv.org/abs/1904.02920 ,  4852kb)
------------------------------------------------------------------------------
\\
arXiv:1909.08153
replaced with revised version Thu, 13 Aug 2020 15:15:53 GMT   (2851kb,D)

Title: CAMAL: Context-Aware Multi-layer Attention framework for Lightweight
  Environment Invariant Visual Place Recognition
Authors: Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Klaus McDonald-Maier
Categories: cs.CV
Comments: under-review
\\ ( https://arxiv.org/abs/1909.08153 ,  2851kb)
------------------------------------------------------------------------------
\\
arXiv:1912.00439
replaced with revised version Thu, 13 Aug 2020 14:47:16 GMT   (8402kb,D)

Title: DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo
  Reconstruction
Authors: Andreas Kuhn (1), Christian Sormann (2), Mattia Rossi (1,3), Oliver
  Erdler (1), Friedrich Fraundorfer (2) ((1) Sony Europe B.V., (2) Graz
  University of Technology, (3) \'Ecole Polytechnique F\'ed\'erale de Lausanne)
Categories: cs.CV
Comments: changes in V3: re-worked confidence prediction scheme, re-organized
  text, updated experiments; changes in V2: a reference was updated
\\ ( https://arxiv.org/abs/1912.00439 ,  8402kb)
------------------------------------------------------------------------------
\\
arXiv:2001.04529
replaced with revised version Thu, 13 Aug 2020 16:00:02 GMT   (2335kb,D)

Title: Rethinking Curriculum Learning with Incremental Labels and Adaptive
  Compensation
Authors: Madan Ravi Ganesh and Jason J. Corso
Categories: cs.CV cs.AI
Comments: 15 pages
\\ ( https://arxiv.org/abs/2001.04529 ,  2335kb)
------------------------------------------------------------------------------
\\
arXiv:2003.00217
replaced with revised version Thu, 13 Aug 2020 03:54:01 GMT   (14320kb)

Title: NAS-Count: Counting-by-Density with Neural Architecture Search
Authors: Yutao Hu, Xiaolong Jiang, Xuhui Liu, Baochang Zhang, Jungong Han,
  Xianbin Cao, David Doermann
Categories: cs.CV
Comments: Accepted to European Conference on Computer Vision(ECCV) 2020
\\ ( https://arxiv.org/abs/2003.00217 ,  14320kb)
------------------------------------------------------------------------------
\\
arXiv:2004.03452
replaced with revised version Thu, 13 Aug 2020 16:50:35 GMT   (4159kb,D)

Title: Strategies for Robust Image Classification
Authors: Jason Stock, Andy Dolan, and Tom Cavey
Categories: cs.CV cs.LG stat.ML
Comments: 15 pages, and 39 figure (with Appendix)
\\ ( https://arxiv.org/abs/2004.03452 ,  4159kb)
------------------------------------------------------------------------------
\\
arXiv:2004.07999
replaced with revised version Thu, 13 Aug 2020 15:54:37 GMT   (9302kb,D)

Title: REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets
Authors: Angelina Wang and Arvind Narayanan and Olga Russakovsky
Categories: cs.CV
Comments: ECCV 2020 Spotlight. Tool available at:
  https://github.com/princetonvisualai/revise-tool
\\ ( https://arxiv.org/abs/2004.07999 ,  9302kb)
------------------------------------------------------------------------------
\\
arXiv:2004.08656
replaced with revised version Thu, 13 Aug 2020 13:41:24 GMT   (5456kb,D)

Title: Occluded Prohibited Items Detection: an X-ray Security Inspection
  Benchmark and De-occlusion Attention Module
Authors: Yanlu Wei, Renshuai Tao, Zhangjie Wu, Yuqing Ma, Libo Zhang, Xianglong
  Liu
Categories: cs.CV
Comments: 9 pages, 7 figures, accepted by ACM Multimedia 2020, data and code
  are available at https://github.com/OPIXray-author/OPIXray
\\ ( https://arxiv.org/abs/2004.08656 ,  5456kb)
------------------------------------------------------------------------------
\\
arXiv:2005.04437
replaced with revised version Thu, 13 Aug 2020 11:20:01 GMT   (8096kb,D)

Title: Understanding Dynamic Scenes using Graph Convolution Networks
Authors: Sravan Mylavarapu, Mahtab Sandhu, Priyesh Vijayan, K Madhava Krishna,
  Balaraman Ravindran, Anoop Namboodiri
Categories: cs.CV
Comments: To appear at IROS 2020
\\ ( https://arxiv.org/abs/2005.04437 ,  8096kb)
------------------------------------------------------------------------------
\\
arXiv:2006.00202
replaced with revised version Thu, 13 Aug 2020 04:58:22 GMT   (1236kb,D)

Title: Attention-Guided Discriminative Region Localization and Label
  Distribution Learning for Bone Age Assessment
Authors: Chao Chen, Zhihong Chen, Xinyu Jin, Lanjuan Li, William Speier, Corey
  W. Arnold
Categories: cs.CV
Comments: codes are available at
  https://github.com/chenchao666/Bone-Age-Assessment
\\ ( https://arxiv.org/abs/2006.00202 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2006.01250
replaced with revised version Wed, 12 Aug 2020 20:01:43 GMT   (993kb,D)

Title: Learning to Detect 3D Objects from Point Clouds in Real Time
Authors: Abhinav Sagar
Categories: cs.CV cs.LG eess.IV
Comments: 10 pages
\\ ( https://arxiv.org/abs/2006.01250 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05597
replaced with revised version Thu, 13 Aug 2020 01:08:50 GMT   (2062kb,D)

Title: Condensing Two-stage Detection with Automatic Object Key Part Discovery
Authors: Zhe Chen, Jing Zhang, Dacheng Tao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.05597 ,  2062kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11487
replaced with revised version Wed, 12 Aug 2020 19:15:43 GMT   (801kb,D)

Title: Paying more attention to snapshots of Iterative Pruning: Improving Model
  Compression via Ensemble Distillation
Authors: Duong H. Le, Vo Trung Nhan, Nam Thoai
Categories: cs.CV cs.LG
Comments: BMVC 2020 - Camera ready
\\ ( https://arxiv.org/abs/2006.11487 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2006.16241
replaced with revised version Thu, 13 Aug 2020 15:43:39 GMT   (2977kb,D)

Title: The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution
  Generalization
Authors: Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang,
  Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song,
  Jacob Steinhardt, Justin Gilmer
Categories: cs.CV cs.LG stat.ML
Comments: Datasets, code, and models available at
  https://github.com/hendrycks/imagenet-r
\\ ( https://arxiv.org/abs/2006.16241 ,  2977kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01151
replaced with revised version Thu, 13 Aug 2020 14:11:15 GMT   (3080kb,D)

Title: JUMPS: Joints Upsampling Method for Pose Sequences
Authors: Lucas Mourot, Fran\c{c}ois Le Clerc, C\'edric Th\'ebault and Pierre
  Hellier
Categories: cs.CV
Comments: 7 pages, 7 figures
\\ ( https://arxiv.org/abs/2007.01151 ,  3080kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02749
replaced with revised version Thu, 13 Aug 2020 15:14:07 GMT   (825kb,D)

Title: Multi-Objective Neural Architecture Search Based on Diverse Structures
  and Adaptive Recommendation
Authors: Chunnan Wang, Hongzhi Wang, Guosheng Feng, Fei Geng
Categories: cs.CV cs.LG
Comments: 11pages
\\ ( https://arxiv.org/abs/2007.02749 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09833
replaced with revised version Thu, 13 Aug 2020 05:42:05 GMT   (8292kb,D)

Title: MINI-Net: Multiple Instance Ranking Network for Video Highlight
  Detection
Authors: Fa-Ting Hong, Xuanteng Huang, Wei-Hong Li, and Wei-Shi Zheng
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2007.09833 ,  8292kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12685
replaced with revised version Wed, 12 Aug 2020 19:53:31 GMT   (420kb,D)

Title: Semantic Segmentation With Multi Scale Spatial Attention For Self
  Driving Cars
Authors: Abhinav Sagar, RajKumar Soundrapandiyan
Categories: cs.CV cs.LG
Comments: 10 pages
\\ ( https://arxiv.org/abs/2007.12685 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13632
replaced with revised version Thu, 13 Aug 2020 08:29:49 GMT   (909kb,D)

Title: Towards Accuracy-Fairness Paradox: Adversarial Example-based Data
  Augmentation for Visual Debiasing
Authors: Yi Zhang, Jitao Sang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2007.13632 ,  909kb)
------------------------------------------------------------------------------
\\
arXiv:2007.14245
replaced with revised version Wed, 12 Aug 2020 19:57:51 GMT   (503kb,D)

Title: Bayesian Multi Scale Neural Network for Crowd Counting
Authors: Abhinav Sagar
Categories: cs.CV cs.LG stat.ML
Comments: 10 pages
\\ ( https://arxiv.org/abs/2007.14245 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2007.16100
replaced with revised version Thu, 13 Aug 2020 13:53:20 GMT   (9814kb,D)

Title: Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution
Authors: Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui
  Wang, Song Han
Categories: cs.CV
Comments: ECCV 2020. The first two authors contributed equally to this work.
  Project page: http://spvnas.mit.edu/
\\ ( https://arxiv.org/abs/2007.16100 ,  9814kb)
------------------------------------------------------------------------------
\\
arXiv:2008.00992
replaced with revised version Thu, 13 Aug 2020 14:17:19 GMT   (4816kb,D)

Title: An Exploration of Target-Conditioned Segmentation Methods for Visual
  Object Trackers
Authors: Matteo Dunnhofer, Niki Martinel, Christian Micheloni
Categories: cs.CV
Comments: European Conference on Computer Vision (ECCV) 2020, Visual Object
  Tracking Challenge VOT2020 workshop
\\ ( https://arxiv.org/abs/2008.00992 ,  4816kb)
------------------------------------------------------------------------------
\\
arXiv:2008.01403
replaced with revised version Thu, 13 Aug 2020 01:56:06 GMT   (11765kb,D)

Title: Jointly Cross- and Self-Modal Graph Attention Network for Query-Based
  Moment Localization
Authors: Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou,
  Zichuan Xu
Categories: cs.CV cs.IR
Comments: Accepted by ACM MM 2020
\\ ( https://arxiv.org/abs/2008.01403 ,  11765kb)
------------------------------------------------------------------------------
\\
arXiv:2008.02268
replaced with revised version Thu, 13 Aug 2020 11:02:36 GMT   (46267kb,D)

Title: NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo
  Collections
Authors: Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T.
  Barron, Alexey Dosovitskiy, Daniel Duckworth
Categories: cs.CV cs.GR cs.LG
Comments: Project website: https://nerf-w.github.io. Ricardo Martin-Brualla,
  Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work.
  Updated affiliations
\\ ( https://arxiv.org/abs/2008.02268 ,  46267kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04451
replaced with revised version Wed, 12 Aug 2020 21:24:21 GMT   (41250kb,D)

Title: Grasping Field: Learning Implicit Representations for Human Grasps
Authors: Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black,
  Krikamol Muandet, Siyu Tang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2008.04451 ,  41250kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04965
replaced with revised version Thu, 13 Aug 2020 00:37:47 GMT   (2787kb,D)

Title: Image segmentation via Cellular Automata
Authors: Mark Sandler, Andrey Zhmoginov, Liangcheng Luo, Alexander Mordvintsev,
  Ettore Randazzo, Blaise Ag\'uera y Arcas
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2008.04965 ,  2787kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05090
replaced with revised version Thu, 13 Aug 2020 06:58:02 GMT   (16088kb,D)

Title: Learning to Caricature via Semantic Shape Transform
Authors: Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Yu-Ting Chang, Yijun Li,
  Deng Cai, Ming-Hsuan Yang
Categories: cs.CV
Comments: Submitted to IJCV, code and model are available at
  https://github.com/wenqingchu/Semantic-CariGANs/
\\ ( https://arxiv.org/abs/2008.05090 ,  16088kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05373
replaced with revised version Thu, 13 Aug 2020 09:54:20 GMT   (3076kb,D)

Title: Attention-based Fully Gated Conventional Recurrent Neural Network for
  Russian Handwritten Text
Authors: Abdelrahman Abdallah, Mohamed Hamada and Daniyar Nurseitov
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2008.05373 ,  3076kb)
------------------------------------------------------------------------------
\\
arXiv:1908.08681
replaced with revised version Thu, 13 Aug 2020 05:42:12 GMT   (3039kb,D)

Title: Mish: A Self Regularized Non-Monotonic Activation Function
Authors: Diganta Misra
Categories: cs.LG cs.CV cs.NE stat.ML
Comments: Accepted to BMVC 2020
\\ ( https://arxiv.org/abs/1908.08681 ,  3039kb)
------------------------------------------------------------------------------
\\
arXiv:1909.13374
replaced with revised version Thu, 13 Aug 2020 05:47:23 GMT   (1508kb,D)

Title: Deep k-NN Defense against Clean-label Data Poisoning Attacks
Authors: Neehar Peri, Neal Gupta, W. Ronny Huang, Liam Fowl, Chen Zhu, Soheil
  Feizi, Tom Goldstein, John P. Dickerson
Categories: cs.LG cs.CV cs.NE
Comments: Accepted to ECCV 2020 Workshop - Adversarial Robustness in the Real
  World (AROW). First three authors contributed equally
\\ ( https://arxiv.org/abs/1909.13374 ,  1508kb)
------------------------------------------------------------------------------
\\
arXiv:1912.00336
replaced with revised version Thu, 13 Aug 2020 02:59:54 GMT   (8816kb,D)

Title: Semi-supervised Visual Feature Integration for Pre-trained Language
  Models
Authors: Lisai Zhang, Qingcai Chen, Dongfang Li, Buzhou Tang
Categories: cs.CL cs.CV
Comments: 12 pages, 6 figures, 5 tables
\\ ( https://arxiv.org/abs/1912.00336 ,  8816kb)
------------------------------------------------------------------------------
\\
arXiv:2001.00425 (*cross-listing*)
replaced with revised version Wed, 12 Aug 2020 21:52:20 GMT   (778kb,D)

Title: Kalman Filtering and Expectation Maximization for Multitemporal Spectral
  Unmixing
Authors: Ricardo Augusto Borsoi, Tales Imbiriba, Pau Closas, Jos\'e Carlos
  Moreira Bermudez, C\'edric Richard
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2001.00425 ,  778kb)
------------------------------------------------------------------------------
\\
arXiv:2004.05298
replaced with revised version Thu, 13 Aug 2020 12:13:54 GMT   (1192kb,D)

Title: Exploit Where Optimizer Explores via Residuals
Authors: An Xu, Zhouyuan Huo, Heng Huang
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/2004.05298 ,  1192kb)
------------------------------------------------------------------------------
\\
arXiv:2006.10304
replaced with revised version Thu, 13 Aug 2020 06:46:34 GMT   (45kb)

Title: Automatic Speech Recognition Benchmark for Air-Traffic Communications
Authors: Juan Zuluaga-Gomez and Petr Motlicek and Qingran Zhan and Karel Vesely
  and Rudolf Braun
Categories: cs.CL cs.CV cs.LG cs.SD eess.AS
Comments: Accepted to: 21st INTERSPEECH conference (Shanghai, October 25-29)
\\ ( https://arxiv.org/abs/2006.10304 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2007.14267 (*cross-listing*)
replaced with revised version Thu, 13 Aug 2020 09:07:25 GMT   (140kb,D)

Title: Efficient Adaptation of Neural Network Filter for Video Compression
Authors: Yat-Hong Lam, Alireza Zare, Francesco Cricri, Jani Lainema, Miska
  Hannuksela
Categories: eess.IV cs.CV cs.LG cs.MM
Comments: Accepted in ACM Multimedia 2020
\\ ( https://arxiv.org/abs/2007.14267 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2008.01410 (*cross-listing*)
replaced with revised version Wed, 12 Aug 2020 22:15:30 GMT   (3294kb,D)

Title: Deep Parallel MRI Reconstruction Network Without Coil Sensitivities
Authors: Wanyu Bian, Yunmei Chen, Xiaojing Ye
Categories: eess.IV cs.CV
Comments: Accepted by MICCAI international workshop MLMIR 2020
\\ ( https://arxiv.org/abs/2008.01410 ,  3294kb)
------------------------------------------------------------------------------
\\
arXiv:2008.02725 (*cross-listing*)
replaced with revised version Thu, 13 Aug 2020 07:05:13 GMT   (1157kb,D)

Title: A Sensitivity Analysis Approach for Evaluating a Radar Simulation for
  Virtual Testing of Autonomous Driving Functions
Authors: Anthony Ngo, Max Paul Bauer, Michael Resch
Categories: eess.SP cs.CV cs.RO
Comments: IEEE 2020 Asia-Pacific Conference on Intelligent Robot Systems (ACIRS
  2020)
\\ ( https://arxiv.org/abs/2008.02725 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05221
replaced with revised version Thu, 13 Aug 2020 10:41:02 GMT   (1282kb,D)

Title: Compression of Deep Learning Models for Text: A Survey
Authors: Manish Gupta, Puneet Agrawal
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: Under Submission. 21 pages
\\ ( https://arxiv.org/abs/2008.05221 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:1802.05905
replaced with revised version Thu, 13 Aug 2020 15:39:19 GMT   (60kb,D)

Title: Assigning times to minimise reachability in temporal graphs
Authors: Jessica Enright, Kitty Meeks and Fiona Skerman
Categories: cs.CC cs.DM cs.DS
Comments: Author final version, to appear in Journal of Computer and System
  Sciences. Material from the previous version has been reorganised
  substantially, and some results have been strengthened
\\ ( https://arxiv.org/abs/1802.05905 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:1906.06111 (*cross-listing*)
replaced with revised version Thu, 13 Aug 2020 08:49:31 GMT   (66kb,D)

Title: On the Djokovi\'c-Winkler relation and its closure in subdivisions of
  fullerenes, triangulations, and chordal graphs
Authors: Sandi Klav\v{z}ar, Kolja Knauer, Tilen Marc
Categories: math.CO cs.DM
Comments: 13 pages, 5 figures, fixed a bug in Lemma 4.1
\\ ( https://arxiv.org/abs/1906.06111 ,  66kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
