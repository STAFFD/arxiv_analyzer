Delivered-To: brucelu2013@gmail.com
Received: by 2002:a54:2e8d:0:0:0:0:0 with SMTP id s13csp1171244ecp;
        Thu, 13 Aug 2020 02:44:27 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJygktmCfXp6WwySmQO7Cs5prLSaRqvt31MfB6haxZBUzxeYnsPEiqIv3YyKQKaJpDwT9iaO
X-Received: by 2002:a37:6193:: with SMTP id v141mr3855954qkb.94.1597311867078;
        Thu, 13 Aug 2020 02:44:27 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1597311867; cv=none;
        d=google.com; s=arc-20160816;
        b=MXu8S7mK5QOyxO7iWkf589we6jKGhAgQHyMumdY9GW6b61Qx7Qf3XaLMKNWxCuEavZ
         RBogRAIQCIeWgeiWmN4sMDBGz3L6s0HQSvNH1ApCN169YK/s1tfZ2Gtb1lXMgmQWbc6C
         NpfnXLQKVKldfE7AaXVHL3J4zB9jifjIGwovZBarM08u/noTdW4xCW9VRnT6ckB5qZXx
         kKEXFzrcz8zIID9ET+jLa2IwxYIndjBh8MjiaB061qgPfMA7REPkBi+YzJiwIh3cHdSN
         VgoPwfrj13iVFF2opAPdR1j0P8UE5rGeEIlsK6Yx13TqJknE7iEev1kAllMt7bXNkcPv
         7PUg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=TVMIQLaNv+GWwDtEx9jY197ULqN/jtoFw1518echuhk=;
        b=PBLwIll4WMnJ+BAMDYLQM+6KlludJzZifBN/aCXL4ezg9PiXJbvI8MIOhfcVefRsyW
         wnD1EGy6AGkQXmLoTuzqKEw7YKSsmK7H4lFDkWqHfoSiL8eQroEveM4yC2E8TOZd+M1g
         Rc417ylAi5sOz0b5nllP3vidIpLLzRI6sr6gnjDE5XDF+GddKdy82sHvezdBYnXTFHsy
         ETDrr7nPEIs4A3bwpMZYdQnamBsZrfTdS3SKudQbd7pIq/W4DLiLs5tVhIrWbrKMnn3z
         b8e7CtqfsRSpYVeTttT2psOs22o+vq4hjg5+OYHUypUY4o9+nuF88wtPlMWsP9ZK2/c0
         unTg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id j197si2896114qke.351.2020.08.13.02.44.26
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 13 Aug 2020 02:44:27 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07D9iQGO005726;
	Thu, 13 Aug 2020 05:44:26 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07D9iQFN034872;
	Thu, 13 Aug 2020 05:44:26 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 07D9iQkK034871;
	Thu, 13 Aug 2020 05:44:26 -0400
Date: Thu, 13 Aug 2020 05:44:26 -0400
Message-Id: <202008130944.07D9iQkK034871@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 4848 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Graphics
 received from  Tue 11 Aug 20 18:00:00 GMT  to  Wed 12 Aug 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2008.04933
Date: Tue, 11 Aug 2020 18:03:13 GMT   (13264kb,D)

Title: PX-NET: Simple, Efficient Pixel-Wise Training of Photometric Stereo
  Networks
Authors: Fotios Logothetis, Ignas Budvytis, Roberto Mecca, Roberto Cipolla
Categories: cs.CV
\\
  Retrieving accurate 3D reconstructions of objects from the way they reflect
light is a very challenging task in computer vision. Despite more than four
decades since the definition of the Photometric Stereo problem, most of the
literature has had limited success when global illumination effects such as
cast shadows, self-reflections and ambient light come into play, especially for
specular surfaces.
  Recent approaches have leveraged the power of deep learning in conjunction
with computer graphics in order to cope with the need of a vast number of
training data in order to invert the image irradiance equation and retrieve the
geometry of the object. However, rendering global illumination effects is a
slow process which can limit the amount of training data that can be generated.
  In this work we propose a novel pixel-wise training procedure for normal
prediction by replacing the training data of globally rendered images with
independent per-pixel renderings. We show that robustness to global physical
effects can be achieved via data-augmentation which greatly simplifies and
speeds up the data creation procedure. Our network, PX-NET, achieves the
state-of-the-art performance on synthetic datasets, as well as the Diligent
real dataset.
\\ ( https://arxiv.org/abs/2008.04933 ,  13264kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04946
Date: Sat, 1 Aug 2020 15:35:55 GMT   (3407kb,D)

Title: Little Motion, Big Results: Using Motion Magnification to Reveal Subtle
  Tremors in Infants
Authors: Girik Malik and Ish K. Gulati
Categories: cs.CV cs.LG eess.IV
\\
  Detecting tremors is challenging for both humans and machines. Infants
exposed to opioids during pregnancy often show signs and symptoms of withdrawal
after birth, which are easy to miss with the human eye. The constellation of
clinical features, termed as Neonatal Abstinence Syndrome (NAS), include
tremors, seizures, irritability, etc. The current standard of care uses
Finnegan Neonatal Abstinence Syndrome Scoring System (FNASS), based on
subjective evaluations. Monitoring with FNASS requires highly skilled nursing
staff, making continuous monitoring difficult. In this paper we propose an
automated tremor detection system using amplified motion signals. We
demonstrate its applicability on bedside video of infant exhibiting signs of
NAS. Further, we test different modes of deep convolutional network based
motion magnification, and identify that dynamic mode works best in the clinical
setting, being invariant to common orientational changes. We propose a strategy
for discharge and follow up for NAS patients, using motion magnification to
supplement the existing protocols. Overall our study suggests methods for
bridging the gap in current practices, training and resource utilization.
\\ ( https://arxiv.org/abs/2008.04946 ,  3407kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04965
Date: Tue, 11 Aug 2020 19:04:09 GMT   (2787kb,D)

Title: Image segmentation via Cellular Automata
Authors: Mark Sandler, Andrey Zhmoginov, Liangcheng Luo, Alexander Mordvintsev,
  Ettore Randazzo, Blaise Ag\'uera y Arcas
Categories: cs.CV cs.LG
\\
  In this paper, we propose a new approach for building cellular automata to
solve real-world segmentation problems.
  We design and train a cellular automaton that can successfully segment
high-resolution images.
  We consider a colony that densely inhabits the pixel grid, and all cells are
governed by a randomized update that uses the current state, the color, and the
state of the $3\times 3$ neighborhood.
  The space of possible rules is defined by a small neural network.
  The update rule is applied repeatedly in parallel to a large random subset of
cells and after convergence is used to produce segmentation masks that are then
back-propagated to learn the optimal update rules using standard gradient
descent methods.
  We demonstrate that such models can be learned efficiently with only limited
trajectory length and that they show remarkable ability to organize the
information to produce a globally consistent segmentation result, using only
local information exchange.
  From a practical perspective, our approach allows us to build very efficient
models -- our smallest automata use less than 10,000 paramaters to solve
complex segmentation tasks.
\\ ( https://arxiv.org/abs/2008.04965 ,  2787kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04968
Date: Tue, 11 Aug 2020 19:10:32 GMT   (2998kb,D)

Title: Campus3D: A Photogrammetry Point Cloud Benchmark for Hierarchical
  Understanding of Outdoor Scene
Authors: Xinke Li, Chongshou Li, Zekun Tong, Andrew Lim, Junsong Yuan, Yuwei
  Wu, Jing Tang, Raymond Huang
Categories: cs.CV cs.AI
Comments: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020)
Journal-ref: Proceedings of the 28th ACM International Conference on Multimedia
  2020
DOI: 10.1145/3394171.3413661
\\
  Learning on 3D scene-based point cloud has received extensive attention as
its promising application in many fields, and well-annotated and multisource
datasets can catalyze the development of those data-driven approaches. To
facilitate the research of this area, we present a richly-annotated 3D point
cloud dataset for multiple outdoor scene understanding tasks and also an
effective learning framework for its hierarchical segmentation task. The
dataset was generated via the photogrammetric processing on unmanned aerial
vehicle (UAV) images of the National University of Singapore (NUS) campus, and
has been point-wisely annotated with both hierarchical and instance-based
labels. Based on it, we formulate a hierarchical learning problem for 3D point
cloud segmentation and propose a measurement evaluating consistency across
various hierarchies. To solve this problem, a two-stage method including
multi-task (MT) learning and hierarchical ensemble (HE) with consistency
consideration is proposed. Experimental results demonstrate the superiority of
the proposed method and potential advantages of our hierarchical annotations.
In addition, we benchmark results of semantic and instance segmentation, which
is accessible online at https://3d.dataset.site with the dataset and all source
codes.
\\ ( https://arxiv.org/abs/2008.04968 ,  2998kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04991
Date: Tue, 11 Aug 2020 20:11:53 GMT   (8239kb,D)

Title: Retrieval Guided Unsupervised Multi-domain Image-to-Image Translation
Authors: Raul Gomez, Yahui Liu, Marco De Nadai, Dimosthenis Karatzas, Bruno
  Lepri and Nicu Sebe
Categories: cs.CV
Comments: Submitted to ACM MM '20, October 12-16, 2020, Seattle, WA, USA
\\
  Image to image translation aims to learn a mapping that transforms an image
from one visual domain to another. Recent works assume that images descriptors
can be disentangled into a domain-invariant content representation and a
domain-specific style representation. Thus, translation models seek to preserve
the content of source images while changing the style to a target visual
domain. However, synthesizing new images is extremely challenging especially in
multi-domain translations, as the network has to compose content and style to
generate reliable and diverse images in multiple domains. In this paper we
propose the use of an image retrieval system to assist the image-to-image
translation task. First, we train an image-to-image translation model to map
images to multiple domains. Then, we train an image retrieval model using real
and generated images to find images similar to a query one in content but in a
different domain. Finally, we exploit the image retrieval system to fine-tune
the image-to-image translation model and generate higher quality images. Our
experiments show the effectiveness of the proposed solution and highlight the
contribution of the retrieval network, which can benefit from additional
unlabeled data and help image-to-image translation models in the presence of
scarce data.
\\ ( https://arxiv.org/abs/2008.04991 ,  8239kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04999
Date: Tue, 11 Aug 2020 20:51:42 GMT   (40014kb)

Title: VI-Net: View-Invariant Quality of Human Movement Assessment
Authors: Faegheh Sardari, Adeline Paiement, Sion Hannuna, and Majid Mirmehdi
Categories: cs.CV
Comments: 13 pages, 6 figures, 7 tables
\\
  We propose a view-invariant method towards the assessment of the quality of
human movements which does not rely on skeleton data. Our end-to-end
convolutional neural network consists of two stages, where at first a
view-invariant trajectory descriptor for each body joint is generated from RGB
images, and then the collection of trajectories for all joints are processed by
an adapted, pre-trained 2D CNN (e.g. VGG-19 or ResNeXt-50) to learn the
relationship amongst the different body parts and deliver a score for the
movement quality. We release the only publicly-available, multi-view,
non-skeleton, non-mocap, rehabilitation movement dataset (QMAR), and provide
results for both cross-subject and cross-view scenarios on this dataset. We
show that VI-Net achieves average rank correlation of 0.66 on cross-subject and
0.65 on unseen views when trained on only two views. We also evaluate the
proposed method on the single-view rehabilitation dataset KIMORE and obtain
0.66 rank correlation against a baseline of 0.62.
\\ ( https://arxiv.org/abs/2008.04999 ,  40014kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05023
Date: Tue, 11 Aug 2020 22:28:48 GMT   (3070kb,D)

Title: Audio- and Gaze-driven Facial Animation of Codec Avatars
Authors: Alexander Richard, Colin Lea, Shugao Ma, Juergen Gall, Fernando de la
  Torre, Yaser Sheikh
Categories: cs.CV
\\
  Codec Avatars are a recent class of learned, photorealistic face models that
accurately represent the geometry and texture of a person in 3D (i.e., for
virtual reality), and are almost indistinguishable from video. In this paper we
describe the first approach to animate these parametric models in real-time
which could be deployed on commodity virtual reality hardware using audio
and/or eye tracking. Our goal is to display expressive conversations between
individuals that exhibit important social signals such as laughter and
excitement solely from latent cues in our lossy input signals. To this end we
collected over 5 hours of high frame rate 3D face scans across three
participants including traditional neutral speech as well as expressive and
conversational speech. We investigate a multimodal fusion approach that
dynamically identifies which sensor encoding should animate which parts of the
face at any time. See the supplemental video which demonstrates our ability to
generate full face motion far beyond the typically neutral lip articulations
seen in competing work:
https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/
\\ ( https://arxiv.org/abs/2008.05023 ,  3070kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05058
Date: Wed, 12 Aug 2020 01:23:21 GMT   (7816kb,D)

Title: Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via
  Geometry-Aware Adversarial Learning
Authors: Borna Be\v{s}i\'c and Abhinav Valada
Categories: cs.CV cs.LG cs.RO
Comments: Dataset, code and models are available at
  http://rl.uni-freiburg.de/research/rgbd-inpainting
\\
  Dynamic objects have a significant impact on the robot's perception of the
environment which degrades the performance of essential tasks such as
localization and mapping. In this work, we address this problem by synthesizing
plausible color, texture and geometry in regions occluded by dynamic objects.
We propose the novel geometry-aware DynaFill architecture that follows a
coarse-to-fine topology and incorporates our gated recurrent feedback mechanism
to adaptively fuse information from previous timesteps. We optimize our
architecture using adversarial training to synthesize fine realistic textures
which enables it to hallucinate color and depth structure in occluded regions
online in a spatially and temporally coherent manner, without relying on future
frame information. Casting our inpainting problem as an image-to-image
translation task, our model also corrects regions correlated with the presence
of dynamic objects in the scene, such as shadows or reflections. We introduce a
large-scale hyperrealistic dataset with RGB-D images, semantic segmentation
labels, camera poses as well as ground truth RGB-D information of occluded
regions. Extensive quantitative and qualitative evaluations show that our
approach achieves state-of-the-art performance, even in challenging weather
conditions. Furthermore, we present results for retrieval-based visual
localization with the synthesized images that demonstrate the utility of our
approach.
\\ ( https://arxiv.org/abs/2008.05058 ,  7816kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05060
Date: Wed, 12 Aug 2020 01:34:21 GMT   (10778kb,D)

Title: Online Graph Completion: Multivariate Signal Recovery in Computer Vision
Authors: Won Hwa Kim, Mona Jalal, Seongjae Hwang, Sterling C. Johnson, Vikas
  Singh
Categories: cs.CV cs.LG eess.SP stat.ML
Comments: 9 pages, 7 figures, CVPR 2017 Conference
DOI: 10.1109/CVPR.2017.533
\\
  The adoption of "human-in-the-loop" paradigms in computer vision and machine
learning is leading to various applications where the actual data acquisition
(e.g., human supervision) and the underlying inference algorithms are closely
interwined. While classical work in active learning provides effective
solutions when the learning module involves classification and regression
tasks, many practical issues such as partially observed measurements, financial
constraints and even additional distributional or structural aspects of the
data typically fall outside the scope of this treatment. For instance, with
sequential acquisition of partial measurements of data that manifest as a
matrix (or tensor), novel strategies for completion (or collaborative
filtering) of the remaining entries have only been studied recently. Motivated
by vision problems where we seek to annotate a large dataset of images via a
crowdsourced platform or alternatively, complement results from a
state-of-the-art object detector using human feedback, we study the
"completion" problem defined on graphs, where requests for additional
measurements must be made sequentially. We design the optimization model in the
Fourier domain of the graph describing how ideas based on adaptive
submodularity provide algorithms that work well in practice. On a large set of
images collected from Imgur, we see promising results on images that are
otherwise difficult to categorize. We also show applications to an experimental
design problem in neuroimaging.
\\ ( https://arxiv.org/abs/2008.05060 ,  10778kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05065
Date: Wed, 12 Aug 2020 01:58:35 GMT   (9707kb)

Title: Select Good Regions for Deblurring based on Convolutional Neural
  Networks
Authors: Hang Yang and Xiaotian Wu and Xinglong Sun
Categories: cs.CV
\\
  The goal of blind image deblurring is to recover sharp image from one input
blurred image with an unknown blur kernel. Most of image deblurring approaches
focus on developing image priors, however, there is not enough attention to the
influence of image details and structures on the blur kernel estimation. What
is the useful image structure and how to choose a good deblurring region? In
this work, we propose a deep neural network model method for selecting good
regions to estimate blur kernel. First we construct image patches with labels
and train a deep neural networks, then the learned model is applied to
determine which region of the image is most suitable to deblur. Experimental
results illustrate that the proposed approach is effective, and could be able
to select good regions for image deblurring.
\\ ( https://arxiv.org/abs/2008.05065 ,  9707kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05079
Date: Wed, 12 Aug 2020 03:13:17 GMT   (10980kb,D)

Title: BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass
  Networks
Authors: Lixin Yang, Jiasen Li, Wenqiang Xu, Yiqun Diao, Cewu Lu
Categories: cs.CV
Comments: To appear on BMVC2020
\\
  3D hand estimation has been a long-standing research topic in computer
vision. A recent trend aims not only to estimate the 3D hand joint locations
but also to recover the mesh model. However, achieving those goals from a
single RGB image remains challenging. In this paper, we introduce an end-to-end
learnable model, BiHand, which consists of three cascaded stages, namely 2D
seeding stage, 3D lifting stage, and mesh generation stage. At the output of
BiHand, the full hand mesh will be recovered using the joint rotations and
shape parameters predicted from the network. Inside each stage, BiHand adopts a
novel bisecting design which allows the networks to encapsulate two closely
related information (e.g. 2D keypoints and silhouette in 2D seeding stage, 3D
joints, and depth map in 3D lifting stage, joint rotations and shape parameters
in the mesh generation stage) in a single forward pass. As the information
represents different geometry or structure details, bisecting the data flow can
facilitate optimization and increase robustness. For quantitative evaluation,
we conduct experiments on two public benchmarks, namely the Rendered Hand
Dataset (RHD) and the Stereo Hand Pose Tracking Benchmark (STB). Extensive
experiments show that our model can achieve superior accuracy in comparison
with state-of-the-art methods, and can produce appealing 3D hand meshes in
several severe conditions.
\\ ( https://arxiv.org/abs/2008.05079 ,  10980kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05090
Date: Wed, 12 Aug 2020 03:41:49 GMT   (16088kb,D)

Title: Learning to Caricature via Semantic Shape Transform
Authors: Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Yu-Ting Chang, Yijun Li,
  Deng Cai, Ming-Hsuan Yang
Categories: cs.CV
Comments: submitted to IJCV
\\
  Caricature is an artistic drawing created to abstract or exaggerate facial
features of a person. Rendering visually pleasing caricatures is a difficult
task that requires professional skills, and thus it is of great interest to
design a method to automatically generate such drawings. To deal with large
shape changes, we propose an algorithm based on a semantic shape transform to
produce diverse and plausible shape exaggerations. Specifically, we predict
pixel-wise semantic correspondences and perform image warping on the input
photo to achieve dense shape transformation. We show that the proposed
framework is able to render visually pleasing shape exaggerations while
maintaining their facial structures. In addition, our model allows users to
manipulate the shape via the semantic map. We demonstrate the effectiveness of
our approach on a large photograph-caricature benchmark dataset with
comparisons to the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2008.05090 ,  16088kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05096
Date: Wed, 12 Aug 2020 04:14:11 GMT   (1157kb,D)

Title: Inter-Image Communication for Weakly Supervised Localization
Authors: Xiaolin Zhang, Yunchao Wei, Yi Yang
Categories: cs.CV
\\
  Weakly supervised localization aims at finding target object regions using
only image-level supervision. However, localization maps extracted from
classification networks are often not accurate due to the lack of fine
pixel-level supervision. In this paper, we propose to leverage pixel-level
similarities across different objects for learning more accurate object
locations in a complementary way. Particularly, two kinds of constraints are
proposed to prompt the consistency of object features within the same
categories. The first constraint is to learn the stochastic feature consistency
among discriminative pixels that are randomly sampled from different images
within a batch. The discriminative information embedded in one image can be
leveraged to benefit its counterpart with inter-image communication. The second
constraint is to learn the global consistency of object features throughout the
entire dataset. We learn a feature center for each category and realize the
global feature consistency by forcing the object features to approach
class-specific centers. The global centers are actively updated with the
training process. The two constraints can benefit each other to learn
consistent pixel-level features within the same categories, and finally improve
the quality of localization maps. We conduct extensive experiments on two
popular benchmarks, i.e., ILSVRC and CUB-200-2011. Our method achieves the
Top-1 localization error rate of 45.17% on the ILSVRC validation set,
surpassing the current state-of-the-art method by a large margin. The code is
available at https://github.com/xiaomengyc/I2C.
\\ ( https://arxiv.org/abs/2008.05096 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05105
Date: Wed, 12 Aug 2020 04:39:32 GMT   (3024kb,D)

Title: Local Temperature Scaling for Probability Calibration
Authors: Zhipeng Ding, Xu Han, Peirong Liu, Marc Niethammer
Categories: cs.CV
\\
  For semantic segmentation, label probabilities are often uncalibrated as they
are typically only the by-product of a segmentation task. Intersection over
Union (IoU) and Dice score are often used as criteria for segmentation success,
while metrics related to label probabilities are rarely explored. On the other
hand, probability calibration approaches have been studied, which aim at
matching probability outputs with experimentally observed errors, but they
mainly focus on classification tasks, not on semantic segmentation. Thus, we
propose a learning-based calibration method that focuses on multi-label
semantic segmentation. Specifically, we adopt a tree-like convolution neural
network to predict local temperature values for probability calibration. One
advantage of our approach is that it does not change prediction accuracy, hence
allowing for calibration as a post-processing step. Experiments on the COCO and
LPBA40 datasets demonstrate improved calibration performance over different
metrics. We also demonstrate the performance of our method for multi-atlas
brain segmentation from magnetic resonance images.
\\ ( https://arxiv.org/abs/2008.05105 ,  3024kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05110
Date: Wed, 12 Aug 2020 04:55:54 GMT   (19927kb,D)

Title: Facial Expression Retargeting from Human to Avatar Made Easy
Authors: Juyong Zhang, Keyu Chen, Jianmin Zheng
Categories: cs.CV cs.GR
Comments: IEEE Transactions on Visualization and Computer Graphics (TVCG), to
  appear
\\
  Facial expression retargeting from humans to virtual characters is a useful
technique in computer graphics and animation. Traditional methods use markers
or blendshapes to construct a mapping between the human and avatar faces.
However, these approaches require a tedious 3D modeling process, and the
performance relies on the modelers' experience. In this paper, we propose a
brand-new solution to this cross-domain expression transfer problem via
nonlinear expression embedding and expression domain translation. We first
build low-dimensional latent spaces for the human and avatar facial expressions
with variational autoencoder. Then we construct correspondences between the two
latent spaces guided by geometric and perceptual constraints. Specifically, we
design geometric correspondences to reflect geometric matching and utilize a
triplet data structure to express users' perceptual preference of avatar
expressions. A user-friendly method is proposed to automatically generate
triplets for a system allowing users to easily and efficiently annotate the
correspondences. Using both geometric and perceptual correspondences, we
trained a network for expression domain translation from human to avatar.
Extensive experimental results and user studies demonstrate that even
nonprofessional users can apply our method to generate high-quality facial
expression retargeting results with less time and effort.
\\ ( https://arxiv.org/abs/2008.05110 ,  19927kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05129
Date: Wed, 12 Aug 2020 06:23:49 GMT   (2822kb,D)

Title: Open Set Recognition with Conditional Probabilistic Generative Models
Authors: Xin Sun, Chi Zhang, Guosheng Lin and Keck-Voon Ling
Categories: cs.CV cs.LG stat.ML
Comments: Extended version of CGDL arXiv:2003.08823 in CVPR2020
\\
  Deep neural networks have made breakthroughs in a wide range of visual
understanding tasks. A typical challenge that hinders their real-world
applications is that unknown samples may be fed into the system during the
testing phase, but traditional deep neural networks will wrongly recognize
these unknown samples as one of the known classes. Open set recognition (OSR)
is a potential solution to overcome this problem, where the open set classifier
should have the flexibility to reject unknown samples and meanwhile maintain
high classification accuracy in known classes. Probabilistic generative models,
such as Variational Autoencoders (VAE) and Adversarial Autoencoders (AAE), are
popular methods to detect unknowns, but they cannot provide discriminative
representations for known classification. In this paper, we propose a novel
framework, called Conditional Probabilistic Generative Models (CPGM), for open
set recognition. The core insight of our work is to add discriminative
information into the probabilistic generative models, such that the proposed
models can not only detect unknown samples but also classify known classes by
forcing different latent features to approximate conditional Gaussian
distributions. We discuss many model variants and provide comprehensive
experiments to study their characteristics. Experiment results on multiple
benchmark datasets reveal that the proposed method significantly outperforms
the baselines and achieves new state-of-the-art performance.
\\ ( https://arxiv.org/abs/2008.05129 ,  2822kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05132
Date: Wed, 12 Aug 2020 06:36:33 GMT   (51864kb,D)

Title: Object Detection for Graphical User Interface: Old Fashioned or Deep
  Learning or a Combination?
Authors: Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu, and
  Liming Zhu
Categories: cs.CV cs.HC cs.LG cs.SE
Comments: 13 pages, accepted to ESEC/FSE '20
DOI: 10.1145/3368089.3409691
\\
  Detecting Graphical User Interface (GUI) elements in GUI images is a
domain-specific object detection task. It supports many software engineering
tasks, such as GUI animation and testing, GUI search and code generation.
Existing studies for GUI element detection directly borrow the mature methods
from computer vision (CV) domain, including old fashioned ones that rely on
traditional image processing features (e.g., canny edge, contours), and deep
learning models that learn to detect from large-scale GUI data. Unfortunately,
these CV methods are not originally designed with the awareness of the unique
characteristics of GUIs and GUI elements and the high localization accuracy of
the GUI element detection task. We conduct the first large-scale empirical
study of seven representative GUI element detection methods on over 50k GUI
images to understand the capabilities, limitations and effective designs of
these methods. This study not only sheds the light on the technical challenges
to be addressed but also informs the design of new GUI element detection
methods. We accordingly design a new GUI-specific old-fashioned method for
non-text GUI element detection which adopts a novel top-down coarse-to-fine
strategy, and incorporate it with the mature deep learning model for GUI text
detection.Our evaluation on 25,000 GUI images shows that our method
significantly advances the start-of-the-art performance in GUI element
detection.
\\ ( https://arxiv.org/abs/2008.05132 ,  51864kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05149
Date: Wed, 12 Aug 2020 07:37:16 GMT   (2310kb,D)

Title: ASAP-Net: Attention and Structure Aware Point Cloud Sequence
  Segmentation
Authors: Hanwen Cao, Yongyi Lu, Cewu Lu, Bo Pang, Gongshen Liu, Alan Yuille
Categories: cs.CV
Comments: The British Machine Vision Conference (BMVC)
ACM-class: I.4; I.5
\\
  Recent works of point clouds show that mulit-frame spatio-temporal modeling
outperforms single-frame versions by utilizing cross-frame information. In this
paper, we further improve spatio-temporal point cloud feature learning with a
flexible module called ASAP considering both attention and structure
information across frames, which we find as two important factors for
successful segmentation in dynamic point clouds. Firstly, our ASAP module
contains a novel attentive temporal embedding layer to fuse the relatively
informative local features across frames in a recurrent fashion. Secondly, an
efficient spatio-temporal correlation method is proposed to exploit more local
structure for embedding, meanwhile enforcing temporal consistency and reducing
computation complexity. Finally, we show the generalization ability of the
proposed ASAP module with different backbone networks for point cloud sequence
segmentation. Our ASAP-Net (backbone plus ASAP module) outperforms baselines
and previous methods on both Synthia and SemanticKITTI datasets (+3.4 to +15.2
mIoU points with different backbones). Code is availabe at
https://github.com/intrepidChw/ASAP-Net
\\ ( https://arxiv.org/abs/2008.05149 ,  2310kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05156
Date: Wed, 12 Aug 2020 07:58:13 GMT   (1416kb,D)

Title: HOSE-Net: Higher Order Structure Embedded Network for Scene Graph
  Generation
Authors: Meng Wei, Chun Yuan, Xiaoyu Yue, Kuo Zhong
Categories: cs.CV
Comments: Accepted to ACM MM 2020
\\
  Scene graph generation aims to produce structured representations for images,
which requires to understand the relations between objects. Due to the
continuous nature of deep neural networks, the prediction of scene graphs is
divided into object detection and relation classification. However, the
independent relation classes cannot separate the visual features well. Although
some methods organize the visual features into graph structures and use message
passing to learn contextual information, they still suffer from drastic
intra-class variations and unbalanced data distributions. One important factor
is that they learn an unstructured output space that ignores the inherent
structures of scene graphs. Accordingly, in this paper, we propose a Higher
Order Structure Embedded Network (HOSE-Net) to mitigate this issue. First, we
propose a novel structure-aware embedding-to-classifier(SEC) module to
incorporate both local and global structural information of relationships into
the output space. Specifically, a set of context embeddings are learned via
local graph based message passing and then mapped to a global structure based
classification space. Second, since learning too many context-specific
classification subspaces can suffer from data sparsity issues, we propose a
hierarchical semantic aggregation(HSA) module to reduces the number of
subspaces by introducing higher order structural information. HSA is also a
fast and flexible tool to automatically search a semantic object hierarchy
based on relational knowledge graphs. Extensive experiments show that the
proposed HOSE-Net achieves the state-of-the-art performance on two popular
benchmarks of Visual Genome and VRD.
\\ ( https://arxiv.org/abs/2008.05156 ,  1416kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05157
Date: Wed, 12 Aug 2020 08:03:28 GMT   (8684kb,D)

Title: Towards Geometry Guided Neural Relighting with Flash Photography
Authors: Di Qiu, Jin Zeng, Zhanghan Ke, Wenxiu Sun, Chengxi Yang
Categories: cs.CV cs.GR cs.LG
\\
  Previous image based relighting methods require capturing multiple images to
acquire high frequency lighting effect under different lighting conditions,
which needs nontrivial effort and may be unrealistic in certain practical use
scenarios. While such approaches rely entirely on cleverly sampling the color
images under different lighting conditions, little has been done to utilize
geometric information that crucially influences the high-frequency features in
the images, such as glossy highlight and cast shadow. We therefore propose a
framework for image relighting from a single flash photograph with its
corresponding depth map using deep learning. By incorporating the depth map,
our approach is able to extrapolate realistic high-frequency effects under
novel lighting via geometry guided image decomposition from the flashlight
image, and predict the cast shadow map from the shadow-encoding transformed
depth map. Moreover, the single-image based setup greatly simplifies the data
capture process. We experimentally validate the advantage of our geometry
guided approach over state-of-the-art image-based approaches in intrinsic image
decomposition and image relighting, and also demonstrate our performance on
real mobile phone photo examples.
\\ ( https://arxiv.org/abs/2008.05157 ,  8684kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05158
Date: Wed, 12 Aug 2020 08:07:55 GMT   (8706kb,D)

Title: Balanced Depth Completion between Dense Depth Inference and Sparse Range
  Measurements via KISS-GP
Authors: Sungho Yoon and Ayoung Kim
Categories: cs.CV cs.RO
Comments: accepted to IROS 2020. 8 pages, 9 figures, 2 tables. Video at this
  https://www.youtube.com/watch?v=x8n0lvjvorg&t=33s
\\
  Estimating a dense and accurate depth map is the key requirement for
autonomous driving and robotics. Recent advances in deep learning have allowed
depth estimation in full resolution from a single image. Despite this
impressive result, many deep-learning-based monocular depth estimation (MDE)
algorithms have failed to keep their accuracy yielding a meter-level estimation
error. In many robotics applications, accurate but sparse measurements are
readily available from Light Detection and Ranging (LiDAR). Although they are
highly accurate, the sparsity limits full resolution depth map reconstruction.
Targeting the problem of dense and accurate depth map recovery, this paper
introduces the fusion of these two modalities as a depth completion (DC)
problem by dividing the role of depth inference and depth regression. Utilizing
the state-of-the-art MDE and our Gaussian process (GP) based depth-regression
method, we propose a general solution that can flexibly work with various MDE
modules by enhancing its depth with sparse range measurements. To overcome the
major limitation of GP, we adopt Kernel Interpolation for Scalable Structured
(KISS)-GP and mitigate the computational complexity from O(N^3) to O(N). Our
experiments demonstrate that the accuracy and robustness of our method
outperform state-of-the-art unsupervised methods for sparse and biased
measurements.
\\ ( https://arxiv.org/abs/2008.05158 ,  8706kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05196
Date: Wed, 12 Aug 2020 09:29:16 GMT   (533kb,D)

Title: RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion
  Judgement and Objective AU Annotations
Authors: Wenjing Yan, Shan Li, Chengtao Que, JiQuan Pei, Weihong Deng
Categories: cs.CV
\\
  Much of the work on automatic facial expression recognition relies on
databases containing a certain number of emotion classes and their exaggerated
facial configurations (generally six prototypical facial expressions), based on
Ekman's Basic Emotion Theory. However, recent studies have revealed that facial
expressions in our human life can be blended with multiple basic emotions. And
the emotion labels for these in-the-wild facial expressions cannot easily be
annotated solely on pre-defined AU patterns. How to analyze the action units
for such complex expressions is still an open question. To address this issue,
we develop a RAF-AU database that employs a sign-based (i.e., AUs) and
judgement-based (i.e., perceived emotion) approach to annotating blended facial
expressions in the wild. We first reviewed the annotation methods in existing
databases and identified crowdsourcing as a promising strategy for labeling
in-the-wild facial expressions. Then, RAF-AU was finely annotated by
experienced coders, on which we also conducted a preliminary investigation of
which key AUs contribute most to a perceived emotion, and the relationship
between AUs and facial expressions. Finally, we provided a baseline for AU
recognition in RAF-AU using popular features and multi-label learning methods.
\\ ( https://arxiv.org/abs/2008.05196 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05202
Date: Wed, 12 Aug 2020 09:46:52 GMT   (1867kb,D)

Title: Representative Graph Neural Network
Authors: Changqian Yu, Yifan Liu, Changxin Gao, Chunhua Shen, Nong Sang
Categories: cs.CV
Comments: Accepted to ECCV 2020. Code is available at https://git.io/RepGraph
Journal-ref: European Conference on Computer Vision 2020
\\
  Non-local operation is widely explored to model the long-range dependencies.
However, the redundant computation in this operation leads to a prohibitive
complexity. In this paper, we present a Representative Graph (RepGraph) layer
to dynamically sample a few representative features, which dramatically reduces
redundancy. Instead of propagating the messages from all positions, our
RepGraph layer computes the response of one node merely with a few
representative nodes. The locations of representative nodes come from a learned
spatial offset matrix. The RepGraph layer is flexible to integrate into many
visual architectures and combine with other operations. With the application of
semantic segmentation, without any bells and whistles, our RepGraph network can
compete or perform favourably against the state-of-the-art methods on three
challenging benchmarks: ADE20K, Cityscapes, and PASCAL-Context datasets. In the
task of object detection, our RepGraph layer can also improve the performance
on the COCO dataset compared to the non-local operation. Code is available at
https://git.io/RepGraph.
\\ ( https://arxiv.org/abs/2008.05202 ,  1867kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05204
Date: Wed, 12 Aug 2020 09:54:17 GMT   (7895kb,D)

Title: Pixel-level Corrosion Detection on Metal Constructions by Fusion of Deep
  Learning Semantic and Contour Segmentation
Authors: Iason Katsamenis, Eftychios Protopapadakis, Anastasios Doulamis,
  Nikolaos Doulamis, Athanasios Voulodimos
Categories: cs.CV cs.LG eess.IV
Comments: 6 pages, 6 figures
MSC-class: 68T07 (Primary) 68T45 (Secondary)
ACM-class: I.2.10; I.4.6
\\
  Corrosion detection on metal constructions is a major challenge in civil
engineering for quick, safe and effective inspection. Existing image analysis
approaches tend to place bounding boxes around the defected region which is not
adequate both for structural analysis and pre-fabrication, an innovative
construction concept which reduces maintenance cost, time and improves safety.
In this paper, we apply three semantic segmentation-oriented deep learning
models (FCN, U-Net and Mask R-CNN) for corrosion detection, which perform
better in terms of accuracy and time and require a smaller number of annotated
samples compared to other deep models, e.g. CNN. However, the final images
derived are still not sufficiently accurate for structural analysis and
pre-fabrication. Thus, we adopt a novel data projection scheme that fuses the
results of color segmentation, yielding accurate but over-segmented contours of
a region, with a processed area of the deep masks, resulting in high-confidence
corroded pixels.
\\ ( https://arxiv.org/abs/2008.05204 ,  7895kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05225
Date: Wed, 12 Aug 2020 10:51:24 GMT   (4895kb,D)

Title: A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote
  Sensing Images
Authors: Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, Mihai Datcu
Categories: cs.CV
\\
  Conventional existing retrieval methods in remote sensing (RS) are often
based on a uni-modal data retrieval framework. In this work, we propose a novel
inter-modal triplet-based zero-shot retrieval scheme utilizing a sketch-based
representation of RS data. The proposed scheme performs efficiently even when
the sketch representations are marginally prototypical of the image. We
conducted experiments on a new bi-modal image-sketch dataset called Earth on
Canvas (EoC) conceived during this study. We perform a thorough bench-marking
of this dataset and demonstrate that the proposed network outperforms other
state-of-the-art methods for zero-shot sketch-based retrieval framework in
remote sensing.
\\ ( https://arxiv.org/abs/2008.05225 ,  4895kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05230
Date: Wed, 12 Aug 2020 11:02:01 GMT   (1690kb,D)

Title: Defending Adversarial Examples via DNN Bottleneck Reinforcement
Authors: Wenqing Liu, Miaojing Shi, Teddy Furon, Li Li
Categories: cs.CV
Comments: ACM MM 2020 - Full Paper
DOI: 10.1145/3394171.3413604
\\
  This paper presents a DNN bottleneck reinforcement scheme to alleviate the
vulnerability of Deep Neural Networks (DNN) against adversarial attacks.
Typical DNN classifiers encode the input image into a compressed latent
representation more suitable for inference. This information bottleneck makes a
trade-off between the image-specific structure and class-specific information
in an image. By reinforcing the former while maintaining the latter, any
redundant information, be it adversarial or not, should be removed from the
latent representation. Hence, this paper proposes to jointly train an
auto-encoder (AE) sharing the same encoding weights with the visual classifier.
In order to reinforce the information bottleneck, we introduce the multi-scale
low-pass objective and multi-scale high-frequency communication for better
frequency steering in the network. Unlike existing approaches, our scheme is
the first reforming defense per se which keeps the classifier structure
untouched without appending any pre-processing head and is trained with clean
images only. Extensive experiments on MNIST, CIFAR-10 and ImageNet demonstrate
the strong defense of our method against various adversarial attacks.
\\ ( https://arxiv.org/abs/2008.05230 ,  1690kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05231
Date: Wed, 12 Aug 2020 11:02:40 GMT   (13307kb,D)

Title: Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using
  Transformer Encoders
Authors: Nicola Messina, Giuseppe Amato, Andrea Esuli, Fabrizio Falchi, Claudio
  Gennaro, St\'ephane Marchand-Maillet
Categories: cs.CV
Comments: Submitted to ACM Transactions on Multimedia Computing,
  Communications, and Applications (TOMM)
\\
  Despite the evolution of deep-learning-based visual-textual processing
systems, precise multi-modal matching remains a challenging task. In this work,
we tackle the problem of accurate cross-media retrieval through image-sentence
matching based on word-region alignments using supervision only at the global
image-sentence level. In particular, we present an approach called Transformer
Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained
match between the underlying components of images and sentences, i.e., image
regions and words, respectively, in order to preserve the informative richness
of both modalities. The proposed approach obtains state-of-the-art results on
the image retrieval task on both MS-COCO and Flickr30k. Moreover, on MS-COCO,
it defeats current approaches also on the sentence retrieval task. Given our
long-term interest in scalable cross-modal information retrieval, TERAN is
designed to keep the visual and textual data pipelines well separated. In fact,
cross-attention links invalidate any chance to separately extract visual and
textual features needed for the online search and the offline indexing steps in
large-scale retrieval systems. In this respect, TERAN merges the information
from the two domains only during the final alignment phase, immediately before
the loss computation. We argue that the fine-grained alignments produced by
TERAN pave the way towards the research for effective and efficient methods for
large-scale cross-modal information retrieval. We compare the effectiveness of
our approach against the best eight methods in this research area. On the
MS-COCO 1K test set, we obtain an improvement of 3.5% and 1.2% respectively on
the image and the sentence retrieval tasks on the Recall@1 metric. The code
used for the experiments is publicly available on GitHub at
https://github.com/mesnico/TERAN.
\\ ( https://arxiv.org/abs/2008.05231 ,  13307kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05242
Date: Wed, 12 Aug 2020 11:29:48 GMT   (5741kb,D)

Title: PAM:Point-wise Attention Module for 6D Object Pose Estimation
Authors: Myoungha Song, Jeongho Lee, Donghwan Kim
Categories: cs.CV
Comments: 11 pages, 5figures
\\
  6D pose estimation refers to object recognition and estimation of 3D rotation
and 3D translation. The key technology for estimating 6D pose is to estimate
pose by extracting enough features to find pose in any environment. Previous
methods utilized depth information in the refinement process or were designed
as a heterogeneous architecture for each data space to extract feature.
However, these methods are limited in that they cannot extract sufficient
feature. Therefore, this paper proposes a Point Attention Module that can
efficiently extract powerful feature from RGB-D. In our Module, attention map
is formed through a Geometric Attention Path(GAP) and Channel Attention
Path(CAP). In GAP, it is designed to pay attention to important information in
geometric information, and CAP is designed to pay attention to important
information in Channel information. We show that the attention module
efficiently creates feature representations without significantly increasing
computational complexity. Experimental results show that the proposed method
outperforms the existing methods in benchmarks, YCB Video and LineMod. In
addition, the attention module was applied to the classification task, and it
was confirmed that the performance significantly improved compared to the
existing model.
\\ ( https://arxiv.org/abs/2008.05242 ,  5741kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05255
Date: Wed, 12 Aug 2020 12:03:27 GMT   (3416kb,D)

Title: Identity-Aware Attribute Recognition via Real-Time Distributed Inference
  in Mobile Edge Clouds
Authors: Zichuan Xu, Jiangkai Wu, Qiufen Xia, Pan Zhou, Jiankang Ren, Huizhi
  Liang
Categories: cs.CV cs.DC cs.MM cs.NI
Comments: 9 pages, 8 figures, Proceedings of the 28th ACM International
  Conference on Multimedia (ACM MM'20), Seattle, WA, USA
\\
  With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods.
\\ ( https://arxiv.org/abs/2008.05255 ,  3416kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05258
Date: Wed, 12 Aug 2020 12:08:25 GMT   (10817kb,D)

Title: Guided Collaborative Training for Pixel-wise Semi-Supervised Learning
Authors: Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, Rynson W.H. Lau
Categories: cs.CV cs.LG eess.IV
Comments: 16th European Conference on Computer Vision (ECCV 2020)
\\
  We investigate the generalization of semi-supervised learning (SSL) to
diverse pixel-wise tasks. Although SSL methods have achieved impressive results
in image classification, the performances of applying them to pixel-wise tasks
are unsatisfactory due to their need for dense outputs. In addition, existing
pixel-wise SSL approaches are only suitable for certain tasks as they usually
require to use task-specific properties. In this paper, we present a new SSL
framework, named Guided Collaborative Training (GCT), for pixel-wise tasks,
with two main technical contributions. First, GCT addresses the issues caused
by the dense outputs through a novel flaw detector. Second, the modules in GCT
learn from unlabeled data collaboratively through two newly proposed
constraints that are independent of task-specific properties. As a result, GCT
can be applied to a wide range of pixel-wise tasks without structural
adaptation. Our extensive experiments on four challenging vision tasks,
including semantic segmentation, real image denoising, portrait image matting,
and night image enhancement, show that GCT outperforms state-of-the-art SSL
methods by a large margin. Our code available at:
https://github.com/ZHKKKe/PixelSSL.
\\ ( https://arxiv.org/abs/2008.05258 ,  10817kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05309
Date: Wed, 12 Aug 2020 13:34:46 GMT   (3164kb,D)

Title: Factor Graph based 3D Multi-Object Tracking in Point Clouds
Authors: Johannes P\"oschmann, Tim Pfeifer and Peter Protzel
Categories: cs.CV cs.RO
Comments: 8 pages, 4 figures, accepted by IEEE Intl. Conf. on Intelligent
  Robots and Systems (IROS) 2020, visualization of the results of our offline
  tracker available at https://www.youtube.com/watch?v=mvZmli4jrZQ
\\
  Accurate and reliable tracking of multiple moving objects in 3D space is an
essential component of urban scene understanding. This is a challenging task
because it requires the assignment of detections in the current frame to the
predicted objects from the previous one. Existing filter-based approaches tend
to struggle if this initial assignment is not correct, which can happen easily.
We propose a novel optimization-based approach that does not rely on explicit
and fixed assignments. Instead, we represent the result of an off-the-shelf 3D
object detector as Gaussian mixture model, which is incorporated in a factor
graph framework. This gives us the flexibility to assign all detections to all
objects simultaneously. As a result, the assignment problem is solved
implicitly and jointly with the 3D spatial multi-object state estimation using
non-linear least squares optimization. Despite its simplicity, the proposed
algorithm achieves robust and reliable tracking results and can be applied for
offline as well as online tracking. We demonstrate its performance on the real
world KITTI tracking dataset and achieve better results than many
state-of-the-art algorithms. Especially the consistency of the estimated tracks
is superior offline as well as online.
\\ ( https://arxiv.org/abs/2008.05309 ,  3164kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05314
Date: Wed, 12 Aug 2020 13:44:20 GMT   (354kb,D)

Title: TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained
  Differentiable Neural Architecture Search
Authors: Yibo Hu, Xiang Wu, Ran He
Categories: cs.CV
Comments: Accepted by ECCV2020. Code is available at
  https://github.com/AberHu/TF-NAS
\\
  With the flourish of differentiable neural architecture search (NAS),
automatically searching latency-constrained architectures gives a new
perspective to reduce human labor and expertise. However, the searched
architectures are usually suboptimal in accuracy and may have large jitters
around the target latency. In this paper, we rethink three freedoms of
differentiable NAS, i.e. operation-level, depth-level and width-level, and
propose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good
classification accuracy and precise latency constraint. For the
operation-level, we present a bi-sampling search algorithm to moderate the
operation collapse. For the depth-level, we introduce a sink-connecting search
space to ensure the mutual exclusion between skip and other candidate
operations, as well as eliminate the architecture redundancy. For the
width-level, we propose an elasticity-scaling strategy that achieves precise
latency constraint in a progressively fine-grained manner. Experiments on
ImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched
TF-NAS-A obtains 76.9% top-1 accuracy, achieving state-of-the-art results with
less latency. The total search time is only 1.8 days on 1 Titan RTX GPU. Code
is available at https://github.com/AberHu/TF-NAS.
\\ ( https://arxiv.org/abs/2008.05314 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05336
Date: Wed, 12 Aug 2020 14:13:53 GMT   (10348kb,D)

Title: Image-based Portrait Engraving
Authors: Paul L. Rosin and Yu-Kun Lai
Categories: cs.CV
Comments: 9 pages, 8 figures
\\
  This paper describes a simple image-based method that applies engraving
stylisation to portraits using ordered dithering. Face detection is used to
estimate a rough proxy geometry of the head consisting of a cylinder, which is
used to warp the dither matrix, causing the engraving lines to curve around the
face for better stylisation. Finally, an application of the approach to colour
engraving is demonstrated.
\\ ( https://arxiv.org/abs/2008.05336 ,  10348kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05359
Date: Wed, 12 Aug 2020 14:57:53 GMT   (15369kb,D)

Title: LogoDet-3K: A Large-Scale Image Dataset for Logo Detection
Authors: Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng,
  Shuqiang Jiang
Categories: cs.CV cs.MM
\\
  Logo detection has been gaining considerable attention because of its wide
range of applications in the multimedia field, such as copyright infringement
detection, brand visibility monitoring, and product brand management on social
media. In this paper, we introduce LogoDet-3K, the largest logo detection
dataset with full annotation, which has 3,000 logo categories, about 200,000
manually annotated logo objects and 158,652 images. LogoDet-3K creates a more
challenging benchmark for logo detection, for its higher comprehensive coverage
and wider variety in both logo categories and annotated objects compared with
existing datasets. We describe the collection and annotation process of our
dataset, analyze its scale and diversity in comparison to other datasets for
logo detection. We further propose a strong baseline method Logo-Yolo, which
incorporates Focal loss and CIoU loss into the state-of-the-art YOLOv3
framework for large-scale logo detection. Logo-Yolo can solve the problems of
multi-scale objects, logo sample imbalance and inconsistent bounding-box
regression. It obtains about 4% improvement on the average performance compared
with YOLOv3, and greater improvements compared with reported several deep
detection models on LogoDet-3K. The evaluations on other three existing
datasets further verify the effectiveness of our method, and demonstrate better
generalization ability of LogoDet-3K on logo detection and retrieval tasks. The
LogoDet-3K dataset is used to promote large-scale logo-related research and it
can be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.
\\ ( https://arxiv.org/abs/2008.05359 ,  15369kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05369
Date: Wed, 12 Aug 2020 15:09:13 GMT   (29648kb,D)

Title: Anomaly localization by modeling perceptual features
Authors: David Dehaene, Pierre Eline
Categories: cs.CV eess.IV
\\
  Although unsupervised generative modeling of an image dataset using a
Variational AutoEncoder (VAE) has been used to detect anomalous images, or
anomalous regions in images, recent works have shown that this method often
identifies images or regions that do not concur with human perception, even
questioning the usability of generative models for robust anomaly detection.
Here, we argue that those issues can emerge from having a simplistic model of
the anomaly distribution and we propose a new VAE-based model expressing a more
complex anomaly model that is also closer to human perception. This
Feature-Augmented VAE is trained by not only reconstructing the input image in
pixel space, but also in several different feature spaces, which are computed
by a convolutional neural network trained beforehand on a large image dataset.
It achieves clear improvement over state-of-the-art methods on the MVTec
anomaly detection and localization datasets.
\\ ( https://arxiv.org/abs/2008.05369 ,  29648kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05373
Date: Wed, 12 Aug 2020 15:14:47 GMT   (3079kb,D)

Title: Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text
Authors: Abdelrahman Abdallah, Mohamed Hamada and Daniyar Nurseitov
Categories: cs.CV cs.AI
\\
  This research approaches the task of handwritten text with attention
encoder-decoder networks that are trained on Kazakh and Russian language. We
developed a novel deep neural network model based on Fully Gated CNN, supported
by Multiple bidirectional GRU and Attention mechanisms to manipulate
sophisticated features that achieve 0.045 Character Error Rate (CER), 0.192
Word Error Rate (WER) and 0.253 Sequence Error Rate (SER) for the first test
dataset and 0.064 CER, 0.24 WER and 0.361 SER for the second test dataset.
Also, we propose fully gated layers by taking the advantage of multiple the
output feature from Tahn and input feature, this proposed work achieves better
results and We experimented with our model on the Handwritten Kazakh \& Russian
Database (HKR). Our research is the first work on the HKR dataset and
demonstrates state-of-the-art results to most of the other existing models.
\\ ( https://arxiv.org/abs/2008.05373 ,  3079kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05381
Date: Wed, 12 Aug 2020 15:29:11 GMT   (14797kb,D)

Title: Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation
Authors: Shashank Manjunath, Aitzaz Nathaniel, Jeff Druce, Stan German
Categories: cs.CV cs.LG
\\
  Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely "black box," offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.
\\ ( https://arxiv.org/abs/2008.05381 ,  14797kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05383
Date: Wed, 12 Aug 2020 15:29:30 GMT   (22096kb,D)

Title: Towards Unsupervised Crowd Counting via Regression-Detection
  Bi-knowledge Transfer
Authors: Yuting Liu, Zheng Wang, Miaojing Shi, Shin'ichi Satoh, Qijun Zhao,
  Hongyu Yang
Categories: cs.CV
Comments: This paper has been accepted by ACM MM 2020
\\
  Unsupervised crowd counting is a challenging yet not largely explored task.
In this paper, we explore it in a transfer learning setting where we learn to
detect and count persons in an unlabeled target set by transferring
bi-knowledge learnt from regression- and detection-based models in a labeled
source set. The dual source knowledge of the two models is heterogeneous and
complementary as they capture different modalities of the crowd distribution.
We formulate the mutual transformations between the outputs of regression- and
detection-based models as two scene-agnostic transformers which enable
knowledge distillation between the two models. Given the regression- and
detection-based models and their mutual transformers learnt in the source, we
introduce an iterative self-supervised learning scheme with
regression-detection bi-knowledge transfer in the target. Extensive experiments
on standard crowd counting benchmarks, ShanghaiTech, UCF\_CC\_50, and UCF\_QNRF
demonstrate a substantial improvement of our method over other
state-of-the-arts in the transfer learning setting.
\\ ( https://arxiv.org/abs/2008.05383 ,  22096kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05396
Date: Fri, 7 Aug 2020 10:20:25 GMT   (2403kb,D)

Title: Full Reference Screen Content Image Quality Assessment by Fusing
  Multi-level Structure Similarity
Authors: Chenglizhao Chen, Hongmeng Zhao, Huan Yang, Chong Peng, Teng Yu
Categories: cs.CV cs.LG
\\
  The screen content images (SCIs) usually comprise various content types with
sharp edges, in which the artifacts or distortions can be well sensed by the
vanilla structure similarity measurement in a full reference manner.
Nonetheless, almost all of the current SOTA structure similarity metrics are
"locally" formulated in a single-level manner, while the true human visual
system (HVS) follows the multi-level manner, and such mismatch could eventually
prevent these metrics from achieving trustworthy quality assessment. To
ameliorate, this paper advocates a novel solution to measure structure
similarity "globally" from the perspective of sparse representation. To perform
multi-level quality assessment in accordance with the real HVS, the
above-mentioned global metric will be integrated with the conventional local
ones by resorting to the newly devised selective deep fusion network. To
validate its efficacy and effectiveness, we have compared our method with 12
SOTA methods over two widely-used large-scale public SCI datasets, and the
quantitative results indicate that our method yields significantly higher
consistency with subjective quality score than the currently leading works.
Both the source code and data are also publicly available to gain widespread
acceptance and facilitate new advancement and its validation.
\\ ( https://arxiv.org/abs/2008.05396 ,  2403kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05397
Date: Mon, 10 Aug 2020 07:12:43 GMT   (619kb,D)

Title: Rethinking of the Image Salient Object Detection: Object-level Semantic
  Saliency Re-ranking First, Pixel-wise Saliency Refinement Latter
Authors: Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin
Categories: cs.CV cs.LG
\\
  The real human attention is an interactive activity between our visual system
and our brain, using both low-level visual stimulus and high-level semantic
information. Previous image salient object detection (SOD) works conduct their
saliency predictions in a multi-task manner, i.e., performing pixel-wise
saliency regression and segmentation-like saliency refinement at the same time,
which degenerates their feature backbones in revealing semantic information.
However, given an image, we tend to pay more attention to those regions which
are semantically salient even in the case that these regions are perceptually
not the most salient ones at first glance. In this paper, we divide the SOD
problem into two sequential tasks: 1) we propose a lightweight, weakly
supervised deep network to coarsely locate those semantically salient regions
first; 2) then, as a post-processing procedure, we selectively fuse multiple
off-the-shelf deep models on these semantically salient regions as the
pixel-wise saliency refinement. In sharp contrast to the state-of-the-art
(SOTA) methods that focus on learning pixel-wise saliency in "single image"
using perceptual clues mainly, our method has investigated the "object-level
semantic ranks between multiple images", of which the methodology is more
consistent with the real human attention mechanism. Our method is simple yet
effective, which is the first attempt to consider the salient object detection
mainly as an object-level semantic re-ranking problem.
\\ ( https://arxiv.org/abs/2008.05397 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05402
Date: Wed, 12 Aug 2020 15:48:49 GMT   (2332kb,D)

Title: DAWN: Vehicle Detection in Adverse Weather Nature Dataset
Authors: Mourad A. Kenk, Mahmoud Hassaballah
Categories: cs.CV
Comments: Available at https://data.mendeley.com/datasets/766ygrbt8y/3 ,IEEE
  Transactions on Intelligent Transportation Systems
DOI: 10.17632/766ygrbt8y.3
\\
  Recently, self-driving vehicles have been introduced with several automated
features including lane-keep assistance, queuing assistance in traffic-jam,
parking assistance and crash avoidance. These self-driving vehicles and
intelligent visual traffic surveillance systems mainly depend on cameras and
sensors fusion systems. Adverse weather conditions such as heavy fog, rain,
snow, and sandstorms are considered dangerous restrictions of the functionality
of cameras impacting seriously the performance of adopted computer vision
algorithms for scene understanding (i.e., vehicle detection, tracking, and
recognition in traffic scenes). For example, reflection coming from rain flow
and ice over roads could cause massive detection errors which will affect the
performance of intelligent visual traffic systems. Additionally, scene
understanding and vehicle detection algorithms are mostly evaluated using
datasets contain certain types of synthetic images plus a few real-world
images. Thus, it is uncertain how these algorithms would perform on unclear
images acquired in the wild and how the progress of these algorithms is
standardized in the field. To this end, we present a new dataset (benchmark)
consisting of real-world images collected under various adverse weather
conditions called DAWN. This dataset emphasizes a diverse traffic environment
(urban, highway and freeway) as well as a rich variety of traffic flow. The
DAWN dataset comprises a collection of 1000 images from real-traffic
environments, which are divided into four sets of weather conditions: fog,
snow, rain and sandstorms. The dataset is annotated with object bounding boxes
for autonomous driving and video surveillance scenarios. This data helps
interpreting effects caused by the adverse weather conditions on the
performance of vehicle detection systems.
\\ ( https://arxiv.org/abs/2008.05402 ,  2332kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05413
Date: Wed, 12 Aug 2020 16:08:36 GMT   (52617kb,D)

Title: Look here! A parametric learning based approach to redirect visual
  attention
Authors: Youssef Alami Mejjati and Celso F. Gomez and Kwang In Kim and Eli
  Shechtman and Zoya Bylinskii
Categories: cs.CV
Comments: To appear in ECCV 2020
\\
  Across photography, marketing, and website design, being able to direct the
viewer's attention is a powerful tool. Motivated by professional workflows, we
introduce an automatic method to make an image region more attention-capturing
via subtle image edits that maintain realism and fidelity to the original. From
an input image and a user-provided mask, our GazeShiftNet model predicts a
distinct set of global parametric transformations to be applied to the
foreground and background image regions separately. We present the results of
quantitative and qualitative experiments that demonstrate improvements over
prior state-of-the-art. In contrast to existing attention shifting algorithms,
our global parametric approach better preserves image semantics and avoids
typical generative artifacts. Our edits enable inference at interactive rates
on any image size, and easily generalize to videos. Extensions of our model
allow for multi-style edits and the ability to both increase and attenuate
attention in an image region. Furthermore, users can customize the edited
images by dialing the edits up or down via interpolations in parameter space.
This paper presents a practical tool that can simplify future image editing
pipelines.
\\ ( https://arxiv.org/abs/2008.05413 ,  52617kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05416
Date: Wed, 12 Aug 2020 16:14:46 GMT   (1151kb,D)

Title: DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features
Authors: Dongjiang Li, Xuesong Shi, Qiwei Long, Shenghui Liu, Wei Yang, Fangshi
  Wang, Qi Wei, Fei Qiao
Categories: cs.CV cs.RO
Comments: 8 pages, 5 figures, to be published in IROS 2020
\\
  A robust and efficient Simultaneous Localization and Mapping (SLAM) system is
essential for robot autonomy. For visual SLAM algorithms, though the
theoretical framework has been well established for most aspects, feature
extraction and association is still empirically designed in most cases, and can
be vulnerable in complex environments. This paper shows that feature extraction
with deep convolutional neural networks (CNNs) can be seamlessly incorporated
into a modern SLAM framework. The proposed SLAM system utilizes a
state-of-the-art CNN to detect keypoints in each image frame, and to give not
only keypoint descriptors, but also a global descriptor of the whole image.
These local and global features are then used by different SLAM modules,
resulting in much more robustness against environmental changes and viewpoint
changes compared with using hand-crafted features. We also train a visual
vocabulary of local features with a Bag of Words (BoW) method. Based on the
local features, global features, and the vocabulary, a highly reliable loop
closure detection method is built. Experimental results show that all the
proposed modules significantly outperforms the baseline, and the full system
achieves much lower trajectory errors and much higher correct rates on all
evaluated data. Furthermore, by optimizing the CNN with Intel OpenVINO toolkit
and utilizing the Fast BoW library, the system benefits greatly from the SIMD
(single-instruction-multiple-data) techniques in modern CPUs. The full system
can run in real-time without any GPU or other accelerators. The code is public
at https://github.com/ivipsourcecode/dxslam.
\\ ( https://arxiv.org/abs/2008.05416 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05441
Date: Wed, 12 Aug 2020 17:10:12 GMT   (537kb,D)

Title: Stable Low-rank Tensor Decomposition for Compression of Convolutional
  Neural Network
Authors: Anh-Huy Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Ermilov,
  Julia Gusak, Petr Tichavsky, Valeriy Glukhov, Ivan Oseledets, and Andrzej
  Cichocki
Categories: cs.CV
Comments: This paper is accepted to ECCV2020
\\
  Most state of the art deep neural networks are overparameterized and exhibit
a high computational cost. A straightforward approach to this problem is to
replace convolutional kernels with its low-rank tensor approximations, whereas
the Canonical Polyadic tensor Decomposition is one of the most suited models.
However, fitting the convolutional tensors by numerical optimization algorithms
often encounters diverging components, i.e., extremely large rank-one tensors
but canceling each other. Such degeneracy often causes the non-interpretable
result and numerical instability for the neural network fine-tuning. This paper
is the first study on degeneracy in the tensor decomposition of convolutional
kernels. We present a novel method, which can stabilize the low-rank
approximation of convolutional kernels and ensure efficient compression while
preserving the high-quality performance of the neural networks. We evaluate our
approach on popular CNN architectures for image classification and show that
our method results in much lower accuracy degradation and provides consistent
performance.
\\ ( https://arxiv.org/abs/2008.05441 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05457
Date: Wed, 12 Aug 2020 17:45:25 GMT   (8498kb,D)

Title: More Diverse Means Better: Multimodal Deep Learning Meets Remote Sensing
  Imagery Classification
Authors: Danfeng Hong and Lianru Gao and Naoto Yokoya and Jing Yao and Jocelyn
  Chanussot and Qian Du and Bing Zhang
Categories: cs.CV eess.IV
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing, 2020
DOI: 10.1109/TGRS.2020.3016820
\\
  Classification and identification of the materials lying over or beneath the
Earth's surface have long been a fundamental but challenging research topic in
geoscience and remote sensing (RS) and have garnered a growing concern owing to
the recent advancements of deep learning techniques. Although deep networks
have been successfully applied in single-modality-dominated classification
tasks, yet their performance inevitably meets the bottleneck in complex scenes
that need to be finely classified, due to the limitation of information
diversity. In this work, we provide a baseline solution to the aforementioned
difficulty by developing a general multimodal deep learning (MDL) framework. In
particular, we also investigate a special case of multi-modality learning (MML)
-- cross-modality learning (CML) that exists widely in RS image classification
applications. By focusing on "what", "where", and "how" to fuse, we show
different fusion strategies as well as how to train deep networks and build the
network architecture. Specifically, five fusion architectures are introduced
and developed, further being unified in our MDL framework. More significantly,
our framework is not only limited to pixel-wise classification tasks but also
applicable to spatial information modeling with convolutional neural networks
(CNNs). To validate the effectiveness and superiority of the MDL framework,
extensive experiments related to the settings of MML and CML are conducted on
two different multimodal RS datasets. Furthermore, the codes and datasets will
be available at https://github.com/danfenghong/IEEE_TGRS_MDL-RS, contributing
to the RS community.
\\ ( https://arxiv.org/abs/2008.05457 ,  8498kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05140
Date: Wed, 12 Aug 2020 07:22:16 GMT   (9kb)

Title: The Italian bondage and reinforcement numbers of digraphs
Authors: Kijung Kim
Categories: cs.DM math.CO
\\
  An \textit{Italian dominating function} on a digraph $D$ with vertex set
$V(D)$ is defined as a function $f : V(D) \rightarrow \{0, 1, 2\}$ such that
every vertex $v \in V(D)$ with $f(v) = 0$ has at least two in-neighbors
assigned $1$ under $f$ or one in-neighbor $w$ with $f(w) = 2$. The
\textit{weight} of an Italian dominating function $f$ is the value $\omega(f) =
f(V(D)) = \sum_{u \in V(D)} f(u)$. The \textit{Italian domination number} of a
digraph $D$, denoted by $\gamma_I(D)$, is the minimum taken over the weights of
all Italian dominating functions on $D$. The \textit{Italian bondage number} of
a digraph $D$, denoted by $b_I(D)$, is the minimum number of arcs of $A(D)$
whose removal in $D$ results in a digraph $D'$ with $\gamma_I(D') >
\gamma_I(D)$. The \textit{Italian reinforcement number} of a digraph $D$,
denoted by $r_I(D)$, is the minimum number of extra arcs whose addition to $D$
results in a digraph $D'$ with $\gamma_I(D') < \gamma_I(D)$. In this paper, we
initiate the study of Italian bondage and reinforcement numbers in digraphs and
present some bounds for $b_I(D)$ and $r_I(D)$. We also determine the Italian
bondage and reinforcement numbers of some classes of digraphs.
\\ ( https://arxiv.org/abs/2008.05140 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05262
Date: Wed, 12 Aug 2020 12:15:41 GMT   (2639kb,D)

Title: The Topology of Shapes Made with Points
Authors: Alexandros Haridis
Categories: cs.GR math.GN
Comments: 12 pages, 5 figures. Keywords: Shape with Points, Finite Order
  Topology, T0-space, Structural Description, Mathematics of Shapes. Preprint
  of journal article. Article first published online: February 11, 2019.
  Environment and Planning B: Urban Analytics and City Science (2019)
MSC-class: 54-02, 57N25, 54A05, 54H99, 00A66, 00A67, 00A06
DOI: 10.1177/2399808319827015
\\
  In architecture, city planning, visual arts, and other design areas, shapes
are often made with points, or with structural representations based on
point-sets. Shapes made with points can be understood more generally as finite
arrangements formed with elements (i.e. points) of the algebra of shapes $U_i$,
for $i = 0$. This paper examines the kind of topology that is applicable to
such shapes. From a mathematical standpoint, any "shape made with points" is
equivalent to a finite space, so that topology on a shape made with points is
no different than topology on a finite space: the study of topological
structure naturally coincides with the study of preorder relations on the
points of the shape. After establishing this fact, some connections between the
topology of shapes made with points and the topology of "point-free" pictorial
shapes (when $i > 0$) are discussed and the main differences between the two
are summarized.
\\ ( https://arxiv.org/abs/2008.05262 ,  2639kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05440
Date: Wed, 12 Aug 2020 17:06:51 GMT   (8903kb,D)

Title: DSM-Net: Disentangled Structured Mesh Net for Controllable Generation of
  Fine Geometry
Authors: Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas J. Guibas, Lin Gao
Categories: cs.GR
\\
  3D shape generation is a fundamental operation in computer graphics. While
significant progress has been made, especially with recent deep generative
models, it remains a challenge to synthesize high-quality geometric shapes with
rich detail and complex structure, in a controllable manner. To tackle this, we
introduce DSM-Net, a deep neural network that learns a disentangled structured
mesh representation for 3D shapes, where two key aspects of shapes, geometry
and structure, are encoded in a synergistic manner to ensure plausibility of
the generated shapes, while also being disentangled as much as possible. This
supports a range of novel shape generation applications with intuitive control,
such as interpolation of structure (geometry) while keeping geometry
(structure) unchanged. To achieve this, we simultaneously learn structure and
geometry through variational autoencoders (VAEs) in a hierarchical manner for
both, with bijective mappings at each level. In this manner we effectively
encode geometry and structure in separate latent spaces, while ensuring their
compatibility: the structure is used to guide the geometry and vice versa. At
the leaf level, the part geometry is represented using a conditional part VAE,
to encode high-quality geometric details, guided by the structure context as
the condition. Our method not only supports controllable generation
applications, but also produces high-quality synthesized shapes, outperforming
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2008.05440 ,  8903kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2008.05186 (*cross-listing*)
Date: Wed, 12 Aug 2020 09:09:03 GMT   (104kb,D)

Title: A short proof of the non-biplanarity of $K_9$
Authors: Ahmad Biniaz
Categories: math.CO cs.CG cs.DM
\\
  Battle, Harary, and Kodama (1962) and independently Tutte (1963) proved that
the complete graph with nine vertices is not biplanar. Aiming towards
simplicity and brevity, in this note we provide a short proof of this claim.
\\ ( https://arxiv.org/abs/2008.05186 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04903 (*cross-listing*)
Date: Wed, 12 Aug 2020 17:06:39 GMT   (576kb,D)

Title: Automatic assembly of aero engine low pressure turbine shaft based on 3D
  vision measurement
Authors: Jiaxiang Wang and Kunyong Chen
Categories: cs.RO cs.CV
Comments: 5pages,12figures
\\
  In order to solve the problem of low automation of Aero-engine Turbine shaft
assembly and the difficulty of non-contact high-precision measurement, a
structured light binocular measurement technology for key components of
aero-engine is proposed in this paper. Combined with three-dimensional point
cloud data processing and assembly position matching algorithm, the
high-precision measurement of shaft hole assembly posture in the process of
turbine shaft docking is realized. Firstly, the screw thread curve on the bolt
surface is segmented based on PCA projection and edge point cloud clustering,
and Hough transform is used to model fit the three-dimensional thread curve.
Then the preprocessed two-dimensional convex hull is constructed to segment the
key hole location features, and the mounting surface and hole location obtained
by segmentation are fitted based on RANSAC method. Finally, the geometric
feature matching is used the evaluation index of turbine shaft assembly is
established to optimize the pose. The final measurement accuracy of mounting
surface matching is less than 0.05mm, and the measurement accuracy of mounting
hole matching based on minimum ance optimization is less than 0.1 degree. The
measurement algorithm is implemented on the automatic assembly test-bed of a
certain type of aero-engine low-pressure turbine rotor. In the narrow
installation space, the assembly process of the turbine shaft assembly, such as
the automatic alignment and docking of the shaft hole, the automatic heating
and temperature measurement of the installation seam, and the automatic
tightening of the two guns, are realized in the narrow installation space
Guidance, real-time inspection and assembly result evaluation.
\\ ( https://arxiv.org/abs/2008.04903 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05024 (*cross-listing*)
Date: Tue, 11 Aug 2020 22:35:24 GMT   (2422kb,D)

Title: Learned Proximal Networks for Quantitative Susceptibility Mapping
Authors: Kuo-Wei Lai, Manisha Aggarwal, Peter van Zijl, Xu Li, Jeremias Sulam
Categories: eess.IV cs.CV cs.LG
Comments: 11 pages
\\
  Quantitative Susceptibility Mapping (QSM) estimates tissue magnetic
susceptibility distributions from Magnetic Resonance (MR) phase measurements by
solving an ill-posed dipole inversion problem. Conventional single orientation
QSM methods usually employ regularization strategies to stabilize such
inversion, but may suffer from streaking artifacts or over-smoothing. Multiple
orientation QSM such as calculation of susceptibility through multiple
orientation sampling (COSMOS) can give well-conditioned inversion and an
artifact free solution but has expensive acquisition costs. On the other hand,
Convolutional Neural Networks (CNN) show great potential for medical image
reconstruction, albeit often with limited interpretability. Here, we present a
Learned Proximal Convolutional Neural Network (LP-CNN) for solving the
ill-posed QSM dipole inversion problem in an iterative proximal gradient
descent fashion. This approach combines the strengths of data-driven
restoration priors and the clear interpretability of iterative solvers that can
take into account the physical model of dipole convolution. During training,
our LP-CNN learns an implicit regularizer via its proximal, enabling the
decoupling between the forward operator and the data-driven parameters in the
reconstruction algorithm. More importantly, this framework is believed to be
the first deep learning QSM approach that can naturally handle an arbitrary
number of phase input measurements without the need for any ad-hoc rotation or
re-training. We demonstrate that the LP-CNN provides state-of-the-art
reconstruction results compared to both traditional and deep learning methods
while allowing for more flexibility in the reconstruction process.
\\ ( https://arxiv.org/abs/2008.05024 ,  2422kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05028 (*cross-listing*)
Date: Tue, 11 Aug 2020 22:50:06 GMT   (9856kb,D)

Title: End-to-End Rate-Distortion Optimization for Bi-Directional Learned Video
  Compression
Authors: M. Akin Yilmaz and A. Murat Tekalp
Categories: eess.IV cs.CV
Comments: This work is accepted for publication at IEEE ICIP 2020
\\
  Conventional video compression methods employ a linear transform and block
motion model, and the steps of motion estimation, mode and quantization
parameter selection, and entropy coding are optimized individually due to
combinatorial nature of the end-to-end optimization problem. Learned video
compression allows end-to-end rate-distortion optimized training of all
nonlinear modules, quantization parameter and entropy model simultaneously.
While previous work on learned video compression considered training a
sequential video codec based on end-to-end optimization of cost averaged over
pairs of successive frames, it is well-known in conventional video compression
that hierarchical, bi-directional coding outperforms sequential compression. In
this paper, we propose for the first time end-to-end optimization of a
hierarchical, bi-directional motion compensated learned codec by accumulating
cost function over fixed-size groups of pictures (GOP). Experimental results
show that the rate-distortion performance of our proposed learned
bi-directional {\it GOP coder} outperforms the state-of-the-art end-to-end
optimized learned sequential compression as expected.
\\ ( https://arxiv.org/abs/2008.05028 ,  9856kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05084 (*cross-listing*)
Date: Wed, 12 Aug 2020 03:20:19 GMT   (16790kb,D)

Title: Self-supervised Light Field View Synthesis Using Cycle Consistency
Authors: Yang Chen, Martin Alain, Aljosa Smolic
Categories: eess.IV cs.CV
Comments: Accepted at MMSP 2020
\\
  High angular resolution is advantageous for practical applications of light
fields. In order to enhance the angular resolution of light fields, view
synthesis methods can be utilized to generate dense intermediate views from
sparse light field input. Most successful view synthesis methods are
learning-based approaches which require a large amount of training data paired
with ground truth. However, collecting such large datasets for light fields is
challenging compared to natural images or videos. To tackle this problem, we
propose a self-supervised light field view synthesis framework with cycle
consistency. The proposed method aims to transfer prior knowledge learned from
high quality natural video datasets to the light field view synthesis task,
which reduces the need for labeled light field data. A cycle consistency
constraint is used to build bidirectional mapping enforcing the generated views
to be consistent with the input views. Derived from this key concept, two loss
functions, cycle loss and reconstruction loss, are used to fine-tune the
pre-trained model of a state-of-the-art video interpolation method. The
proposed method is evaluated on various datasets to validate its robustness,
and results show it not only achieves competitive performance compared to
supervised fine-tuning, but also outperforms state-of-the-art light field view
synthesis methods, especially when generating multiple intermediate views.
Besides, our generic light field view synthesis framework can be adopted to any
pre-trained model for advanced video interpolation.
\\ ( https://arxiv.org/abs/2008.05084 ,  16790kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05101 (*cross-listing*)
Date: Wed, 12 Aug 2020 04:26:18 GMT   (346kb,D)

Title: FATNN: Fast and Accurate Ternary Neural Networks
Authors: Peng Chen, Bohan Zhuang, Chunhua Shen
Categories: cs.LG cs.CV
Comments: 15 pages
\\
  Ternary Neural Networks (TNNs) have received much attention due to being
potentially orders of magnitude faster in inference, as well as more power
efficient, than full-precision counterparts. However, 2 bits are required to
encode the ternary representation with only 3 quantization levels leveraged. As
a result, conventional TNNs have similar memory consumption and speed compared
with the standard 2-bit models, but have worse representational capability.
Moreover, there is still a significant gap in accuracy between TNNs and
full-precision networks, hampering their deployment to real applications. To
tackle these two challenges, in this work, we first show that, under some mild
constraints, the computational complexity of ternary inner product can be
reduced by 2x. Second, to mitigate the performance gap, we elaborately design
an implementation-dependent ternary quantization algorithm. The proposed
framework is termed Fast and Accurate Ternary Neural Networks (FATNN).
Experiments on image classification demonstrate that our FATNN surpasses the
state-of-the-arts by a significant margin in accuracy. More importantly,
speedup evaluation comparing with various precisions is analyzed on several
platforms, which serves as a strong benchmark for further research.
\\ ( https://arxiv.org/abs/2008.05101 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05117 (*cross-listing*)
Date: Wed, 12 Aug 2020 05:43:59 GMT   (2884kb,D)

Title: A Longitudinal Method for Simultaneous Whole-Brain and Lesion
  Segmentation in Multiple Sclerosis
Authors: Stefano Cerri, Andrew Hoopes, Douglas N. Greve, Mark M\"uhlau, Koen
  Van Leemput
Categories: eess.IV cs.CV cs.LG
\\
  In this paper we propose a novel method for the segmentation of longitudinal
brain MRI scans of patients suffering from Multiple Sclerosis. The method
builds upon an existing cross-sectional method for simultaneous whole-brain and
lesion segmentation, introducing subject-specific latent variables to encourage
temporal consistency between longitudinal scans. It is very generally
applicable, as it does not make any prior assumptions on the scanner, the MRI
protocol, or the number and timing of longitudinal follow-up scans. Preliminary
experiments on three longitudinal datasets indicate that the proposed method
produces more reliable segmentations and detects disease effects better than
the cross-sectional method it is based upon.
\\ ( https://arxiv.org/abs/2008.05117 ,  2884kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05133 (*cross-listing*)
Date: Wed, 12 Aug 2020 06:38:15 GMT   (494kb)

Title: An Inter- and Intra-Band Loss for Pansharpening Convolutional Neural
  Networks
Authors: Jiajun Cai and Bo Huang
Categories: eess.IV cs.CV
Comments: 4 pages, 2 figures
\\
  Pansharpening aims to fuse panchromatic and multispectral images from the
satellite to generate images with both high spatial and spectral resolution.
With the successful applications of deep learning in the computer vision field,
a lot of scholars have proposed many convolutional neural networks (CNNs) to
solve the pansharpening task. These pansharpening networks focused on various
distinctive structures of CNNs, and most of them are trained by L2 loss between
fused images and simulated desired multispectral images. However, L2 loss is
designed to directly minimize the difference of spectral information of each
band, which does not consider the inter-band relations in the training process.
In this letter, we propose a novel inter- and intra-band (IIB) loss to overcome
the drawback of original L2 loss. Our proposed IIB loss can effectively
preserve both inter- and intra-band relations and can be directly applied to
different pansharpening CNNs.
\\ ( https://arxiv.org/abs/2008.05133 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05217 (*cross-listing*)
Date: Wed, 12 Aug 2020 10:28:39 GMT   (3606kb,D)

Title: Large-Scale Analysis of Iliopsoas Muscle Volumes in the UK Biobank
Authors: Julie Fitzpatrick, Nicolas Basty, Madeleine Cule, Yi Liu, Jimmy D.
  Bell, E. LouiseThomas, Brandon Whitcher
Categories: eess.IV cs.CV q-bio.QM
Comments: Julie Fitzpatrick and Nicolas Basty are joint first authors
\\
  Psoas muscle measurements are frequently used as markers of sarcopenia and
predictors of health. Manually measured cross-sectional areas are most commonly
used, but there is a lack of consistency regarding the position of the
measurementand manual annotations are not practical for large population
studies. We have developed a fully automated method to measure iliopsoas muscle
volume (comprised of the psoas and iliacus muscles) using a convolutional
neural network. Magnetic resonance images were obtained from the UK Biobank for
5,000 male and female participants, balanced for age, gender and BMI. Ninety
manual annotations were available for model training and validation. The model
showed excellent performance against out-of-sample data (dice score coefficient
of 0.912 +/- 0.018). Iliopsoas muscle volumes were successfully measured in all
5,000 participants. Iliopsoas volume was greater in male compared with female
subjects. There was a small but significant asymmetry between left and right
iliopsoas muscle volumes. We also found that iliopsoas volume was significantly
related to height, BMI and age, and that there was an acceleration in muscle
volume decrease in men with age. Our method provides a robust technique for
measuring iliopsoas muscle volume that can be applied to large cohorts.
\\ ( https://arxiv.org/abs/2008.05217 ,  3606kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05221 (*cross-listing*)
Date: Wed, 12 Aug 2020 10:42:14 GMT   (1884kb,D)

Title: Compression of Deep Learning Models for Text: A Survey
Authors: Manish Gupta, Puneet Agrawal
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: Under Submission. 33 pages, ACMArt, single column format
\\
  In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanks to deep
learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units
(GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer based
models like Bidirectional Encoder Representations from Transformers (BERT). But
these models are humongous in size. On the other hand, real world applications
demand small model size, low response times and low computational power
wattage. In this survey, we discuss six different types of methods (Pruning,
Quantization, Knowledge Distillation, Parameter Sharing, Tensor Decomposition,
and Linear Transformer based methods) for compression of such models to enable
their deployment in real industry NLP projects. Given the critical need of
building applications with efficient and small models, and the large amount of
recently published work in this area, we believe that this survey organizes the
plethora of work done by the 'deep learning for NLP' community in the past few
years and presents it as a coherent story.
\\ ( https://arxiv.org/abs/2008.05221 ,  1884kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05247 (*cross-listing*)
Date: Wed, 12 Aug 2020 11:44:01 GMT   (2799kb,D)

Title: Learning to Learn from Mistakes: Robust Optimization for Adversarial
  Noise
Authors: Alex Serban, Erik Poll, Joost Visser
Categories: cs.LG cs.CV stat.ML
Comments: Published at ICANN 2020
\\
  Sensitivity to adversarial noise hinders deployment of machine learning
algorithms in security-critical applications. Although many adversarial
defenses have been proposed, robustness to adversarial noise remains an open
problem. The most compelling defense, adversarial training, requires a
substantial increase in processing time and it has been shown to overfit on the
training data. In this paper, we aim to overcome these limitations by training
robust models in low data regimes and transfer adversarial knowledge between
different models. We train a meta-optimizer which learns to robustly optimize a
model using adversarial examples and is able to transfer the knowledge learned
to new models, without the need to generate new adversarial examples.
Experimental results show the meta-optimizer is consistent across different
architectures and data sets, suggesting it is possible to automatically patch
adversarial vulnerabilities.
\\ ( https://arxiv.org/abs/2008.05247 ,  2799kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05332 (*cross-listing*)
Date: Wed, 12 Aug 2020 14:12:07 GMT   (21113kb,D)

Title: Renal Cell Carcinoma Detection and Subtyping with Minimal Point-Based
  Annotation in Whole-Slide Images
Authors: Zeyu Gao, Pargorn Puttapirat, Jiangbo Shi, Chen Li
Categories: eess.IV cs.CV cs.LG q-bio.QM
Comments: 10 pages, 5 figure, 3 tables, accepted at MICCAI 2020
\\
  Obtaining a large amount of labeled data in medical imaging is laborious and
time-consuming, especially for histopathology. However, it is much easier and
cheaper to get unlabeled data from whole-slide images (WSIs). Semi-supervised
learning (SSL) is an effective way to utilize unlabeled data and alleviate the
need for labeled data. For this reason, we proposed a framework that employs an
SSL method to accurately detect cancerous regions with a novel annotation
method called Minimal Point-Based annotation, and then utilize the predicted
results with an innovative hybrid loss to train a classification model for
subtyping. The annotator only needs to mark a few points and label them are
cancer or not in each WSI. Experiments on three significant subtypes of renal
cell carcinoma (RCC) proved that the performance of the classifier trained with
the Min-Point annotated dataset is comparable to a classifier trained with the
segmentation annotated dataset for cancer region detection. And the subtyping
model outperforms a model trained with only diagnostic labels by 12% in terms
of f1-score for testing WSIs.
\\ ( https://arxiv.org/abs/2008.05332 ,  21113kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05192 (*cross-listing*)
Date: Wed, 12 Aug 2020 09:16:22 GMT   (6kb)

Title: Lower-bounds on the growth of power-free languages over large alphabets
Authors: Matthieu Rosenfeld
Categories: math.CO cs.DM
\\
  We study the growth rate of some power-free languages. For any integer $k$
and real $\beta>1$, we let $\alpha(k,\beta)$ be the growth rate of the number
of $\beta$-free words of a given length over the alphabet $\{1,2,\ldots, k\}$.
Shur studied the asymptotic behavior of $\alpha(k,\beta)$ for $\beta\ge2$ as
$k$ goes to infinity. He suggested a conjecture regarding the asymptotic
behavior of $\alpha(k,\beta)$ as $k$ goes to infinity when $1<\beta<2$. He
showed that for $\frac{9}{8}\le\beta<2$ the asymptotic upper-bound holds of his
conjecture holds.
  We show that the asymptotic lower-bound of his conjecture holds. This implies
that the conjecture is true for $\frac{9}{8}\le\beta<2$.
\\ ( https://arxiv.org/abs/2008.05192 ,  6kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05391 (*cross-listing*)
Date: Wed, 12 Aug 2020 15:40:21 GMT   (941kb,D)

Title: Revisiting Modified Greedy Algorithm for Monotone Submodular
  Maximization with a Knapsack Constraint
Authors: Jing Tang, Xueyan Tang, Andrew Lim, Kai Han, Chongshou Li, Junsong
  Yuan
Categories: cs.DS cs.AI cs.DM
\\
  Monotone submodular maximization with a knapsack constraint is NP-hard.
Various approximation algorithms have been devised to address this optimization
problem. In this paper, we revisit the widely known modified greedy algorithm.
First, we show that this algorithm can achieve an approximation factor of
$0.405$, which significantly improves the known factor of $0.357$ given by
Wolsey or $(1-1/\mathrm{e})/2\approx 0.316$ given by Khuller et al. More
importantly, our analysis uncovers a gap in Khuller et al.'s proof for the
extensively mentioned approximation factor of $(1-1/\sqrt{\mathrm{e}})\approx
0.393$ in the literature to clarify a long time of misunderstanding on this
issue. Second, we enhance the modified greedy algorithm to derive a
data-dependent upper bound on the optimum. We empirically demonstrate the
tightness of our upper bound with a real-world application. The bound enables
us to obtain a data-dependent ratio typically much higher than $0.405$ between
the solution value of the modified greedy algorithm and the optimum. It can
also be used to significantly improve the efficiency of algorithms such as
branch and bound.
\\ ( https://arxiv.org/abs/2008.05391 ,  941kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05392 (*cross-listing*)
Date: Wed, 12 Aug 2020 15:42:22 GMT   (205kb,D)

Title: The Local Queue Number of Graphs with Bounded Treewidth
Authors: Laura Merker and Torsten Ueckerdt
Categories: math.CO cs.DM
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\
  A queue layout of a graph $G$ consists of a vertex ordering of $G$ and a
partition of the edges into so-called queues such that no two edges in the same
queue nest, i.e., have their endpoints ordered in an ABBA-pattern. Continuing
the research on local ordered covering numbers, we introduce the local queue
number of a graph $G$ as the minimum $\ell$ such that $G$ admits a queue layout
with each vertex having incident edges in no more than $\ell$ queues. Similarly
to the local page number [Merker, Ueckerdt, GD'19], the local queue number is
closely related to the graph's density and can be arbitrarily far from the
classical queue number.
  We present tools to bound the local queue number of graphs from above and
below, focusing on graphs of treewidth $k$. Using these, we show that every
graph of treewidth $k$ has local queue number at most $k+1$ and that this bound
is tight for $k=2$, while a general lower bound is $\lceil k/2\rceil+1$. Our
results imply, inter alia, that the maximum local queue number among planar
graphs is either 3 or 4.
\\ ( https://arxiv.org/abs/2008.05392 ,  205kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1904.02920
replaced with revised version Wed, 12 Aug 2020 08:32:29 GMT   (4858kb,D)

Title: Branched Multi-Task Networks: Deciding What Layers To Share
Authors: Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere and Luc
  Van Gool
Categories: cs.CV
Comments: Accepted at BMVC 2020
\\ ( https://arxiv.org/abs/1904.02920 ,  4858kb)
------------------------------------------------------------------------------
\\
arXiv:1904.05034
replaced with revised version Wed, 12 Aug 2020 09:01:45 GMT   (4456kb,D)

Title: ThumbNet: One Thumbnail Image Contains All You Need for Recognition
Authors: Chen Zhao and Bernard Ghanem
Categories: cs.CV
\\ ( https://arxiv.org/abs/1904.05034 ,  4456kb)
------------------------------------------------------------------------------
\\
arXiv:1909.10837
replaced with revised version Wed, 12 Aug 2020 05:31:43 GMT   (3687kb,D)

Title: Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust
  Performance
Authors: Shibo Zhou, Xiaohua LI, Ying Chen, Sanjeev T. Chandrasekaran, Arindam
  Sanyal
Categories: cs.CV cs.NE eess.IV
\\ ( https://arxiv.org/abs/1909.10837 ,  3687kb)
------------------------------------------------------------------------------
\\
arXiv:2002.00461
replaced with revised version Tue, 11 Aug 2020 18:21:46 GMT   (563kb,D)

Title: Effect of Analysis Window and Feature Selection on Classification of
  Hand Movements Using EMG Signal
Authors: Asad Ullah, Sarwan Ali, Imdadullah Khan, Muhammad Asad Khan, Safiullah
  Faizullah
Categories: cs.CV eess.SP
Comments: Accepted to Intelligent Systems Conference (IntelliSys) 2020
\\ ( https://arxiv.org/abs/2002.00461 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2003.02059
replaced with revised version Wed, 12 Aug 2020 04:10:05 GMT   (31721kb,D)

Title: Vehicle-Human Interactive Behaviors in Emergency: Data Extraction from
  Traffic Accident Videos
Authors: Wansong Liu, Danyang Luo, Changxu Wu, Minghui Zheng
Categories: cs.CV cs.SY eess.IV eess.SY
Comments: ACC 2020 final version
\\ ( https://arxiv.org/abs/2003.02059 ,  31721kb)
------------------------------------------------------------------------------
\\
arXiv:2004.03212
replaced with revised version Wed, 12 Aug 2020 12:23:53 GMT   (8496kb,D)

Title: Text-Guided Neural Image Inpainting
Authors: Lisai Zhang, Qingcai Chen, Baotian Hu, and Shuoran Jiang
Categories: cs.CV cs.CL
Comments: Camera-ready version at ACM MM'2020. 9 pages, 4 tables, 7 figures
DOI: 10.1145/3394171.3414017
\\ ( https://arxiv.org/abs/2004.03212 ,  8496kb)
------------------------------------------------------------------------------
\\
arXiv:2004.13324
replaced with revised version Wed, 12 Aug 2020 07:23:23 GMT   (12020kb,D)

Title: Learning Feature Descriptors using Camera Pose Supervision
Authors: Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, Noah Snavely
Categories: cs.CV
Comments: ECCV 2020
\\ ( https://arxiv.org/abs/2004.13324 ,  12020kb)
------------------------------------------------------------------------------
\\
arXiv:2005.03948
replaced with revised version Wed, 12 Aug 2020 14:21:22 GMT   (5155kb,D)

Title: Layer-wise training convolutional neural networks with smaller filters
  for human activity recognition using wearable sensors
Authors: Yin Tang, Qi Teng, Lei Zhang, Fuhong Min and Jun He
Categories: cs.CV
Comments: 11 pages, 11 figures
\\ ( https://arxiv.org/abs/2005.03948 ,  5155kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04203
replaced with revised version Tue, 11 Aug 2020 21:47:33 GMT   (3149kb,D)

Title: Thoracic Disease Identification and Localization using Distance Learning
  and Region Verification
Authors: Cheng Zhang, Francine Chen, Yan-Ying Chen
Categories: cs.CV
Comments: British Machine Vision Conference (BMVC) 2020
\\ ( https://arxiv.org/abs/2006.04203 ,  3149kb)
------------------------------------------------------------------------------
\\
arXiv:2006.11693
replaced with revised version Wed, 12 Aug 2020 03:44:21 GMT   (399kb,D)

Title: Dense-Captioning Events in Videos: SYSU Submission to ActivityNet
  Challenge 2020
Authors: Teng Wang, Huicheng Zheng, Mingjing Yu
Categories: cs.CV
Comments: Second-place solution to TASK 2 (Dense video captioning) in
  ActivityNet Challenge 2020. Code is available at
  https://github.com/ttengwang/dense-video-captioning-pytorch
\\ ( https://arxiv.org/abs/2006.11693 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2006.14090
replaced with revised version Tue, 11 Aug 2020 22:54:26 GMT   (876kb,D)

Title: Neural Architecture Design for GPU-Efficient Networks
Authors: Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, Rong Jin
Categories: cs.CV
Comments: update training setting
\\ ( https://arxiv.org/abs/2006.14090 ,  876kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09529
replaced with revised version Tue, 11 Aug 2020 22:49:13 GMT   (5732kb,D)

Title: Single View Metrology in the Wild
Authors: Rui Zhu, Xingyi Yang, Yannick Hold-Geoffroy, Federico Perazzi,
  Jonathan Eisenmann, Kalyan Sunkavalli, Manmohan Chandraker
Categories: cs.CV
Comments: ECCV 2020, camera-ready version
\\ ( https://arxiv.org/abs/2007.09529 ,  5732kb)
------------------------------------------------------------------------------
\\
arXiv:2007.15176
replaced with revised version Wed, 12 Aug 2020 10:05:48 GMT   (22117kb,D)

Title: Domain Adaptive Semantic Segmentation Using Weak Labels
Authors: Sujoy Paul, Yi-Hsuan Tsai, Samuel Schulter, Amit K. Roy-Chowdhury,
  Manmohan Chandraker
Categories: cs.CV
Comments: ECCV 2020
\\ ( https://arxiv.org/abs/2007.15176 ,  22117kb)
------------------------------------------------------------------------------
\\
arXiv:2008.02531
replaced with revised version Wed, 12 Aug 2020 07:28:38 GMT   (2315kb,D)

Title: Self-supervised Video Representation Learning Using Inter-intra
  Contrastive Framework
Authors: Li Tao, Xueting Wang, Toshihiko Yamasaki
Categories: cs.CV
Comments: Accepted by ACMMM 2020. Our project page is at
  https://bestjuly.github.io/Inter-intra-video-contrastive-learning/
DOI: 10.1145/3394171.3413694
\\ ( https://arxiv.org/abs/2008.02531 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2008.03064
replaced with revised version Wed, 12 Aug 2020 13:50:29 GMT   (1637kb,D)

Title: A Surgery of the Neural Architecture Evaluators
Authors: Xuefei Ning, Wenshuo Li, Zixuan Zhou, Tianchen Zhao, Yin Zheng, Shuang
  Liang, Huazhong Yang, Yu Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2008.03064 ,  1637kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04556
replaced with revised version Wed, 12 Aug 2020 02:35:11 GMT   (2758kb,D)

Title: Text as Neural Operator: Image Manipulation by Text Instruction
Authors: Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Honglak Lee, Irfan Essa,
  Weilong Yang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2008.04556 ,  2758kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04851
replaced with revised version Wed, 12 Aug 2020 07:29:25 GMT   (7853kb,D)

Title: TextRay: Contour-based Geometric Modeling for Arbitrary-shaped Scene
  Text Detection
Authors: Fangfang Wang, Yifeng Chen, Fei Wu, and Xi Li
Categories: cs.CV cs.LG
Comments: Accepted to ACM MM 2020
\\ ( https://arxiv.org/abs/2008.04851 ,  7853kb)
------------------------------------------------------------------------------
\\
arXiv:2002.09533
replaced with revised version Wed, 12 Aug 2020 15:26:48 GMT   (10317kb,D)

Title: Real-Time Visualization in Non-Isotropic Geometries
Authors: Eryk Kopczy\'nski and Dorota Celi\'nska-Kopczy\'nska
Categories: cs.GR math.DG
MSC-class: 53A35 Non-Euclidean differential geometry
ACM-class: I.3.7
\\ ( https://arxiv.org/abs/2002.09533 ,  10317kb)
------------------------------------------------------------------------------
\\
arXiv:2003.01944 (*cross-listing*)
replaced with revised version Wed, 12 Aug 2020 09:44:29 GMT   (1181kb,D)

Title: Semixup: In- and Out-of-Manifold Regularization for Deep Semi-Supervised
  Knee Osteoarthritis Severity Grading from Plain Radiographs
Authors: Huy Hoang Nguyen, Simo Saarakkala, Matthew Blaschko, Aleksei Tiulpin
Categories: eess.IV cs.CV cs.LG
Comments: 11 main, 03 supplementary pages. The manuscript was accepted to IEEE
  Transactions on Medical Imaging in August 2020
\\ ( https://arxiv.org/abs/2003.01944 ,  1181kb)
------------------------------------------------------------------------------
\\
arXiv:2004.00871 (*cross-listing*)
replaced with revised version Wed, 12 Aug 2020 17:20:56 GMT   (21195kb,D)

Title: End-To-End Convolutional Neural Network for 3D Reconstruction of Knee
  Bones From Bi-Planar X-Ray Images
Authors: Yoni Kasten, Daniel Doktofsky and Ilya Kovler
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2004.00871 ,  21195kb)
------------------------------------------------------------------------------
\\
arXiv:2005.03684
replaced with revised version Wed, 12 Aug 2020 03:21:27 GMT   (7905kb,D)

Title: Learning to Segment Actions from Observation and Narration
Authors: Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen
  Clark, Aida Nematzadeh
Categories: cs.CL cs.CV
Comments: ACL 2020
\\ ( https://arxiv.org/abs/2005.03684 ,  7905kb)
------------------------------------------------------------------------------
\\
arXiv:2006.14822 (*cross-listing*)
replaced with revised version Wed, 12 Aug 2020 06:43:50 GMT   (1132kb,D)

Title: A survey of loss functions for semantic segmentation
Authors: Shruti Jadon
Categories: eess.IV cs.CV cs.LG
Comments: 5 pages, 5 figures, 2 tables
Journal-ref: 2020 IEEE International Conference on Computational Intelligence
  in Bioinformatics and Computational Biology
\\ ( https://arxiv.org/abs/2006.14822 ,  1132kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12911
replaced with revised version Wed, 12 Aug 2020 11:09:09 GMT   (815kb,D)

Title: Tighter risk certificates for neural networks
Authors: Mar\'ia P\'erez-Ortiz and Omar Rivasplata and John Shawe-Taylor and
  Csaba Szepesv\'ari
Categories: cs.LG cs.CV stat.ML
Comments: Preprint under review
\\ ( https://arxiv.org/abs/2007.12911 ,  815kb)
------------------------------------------------------------------------------
\\
arXiv:2008.03008 (*cross-listing*)
replaced with revised version Wed, 12 Aug 2020 03:15:53 GMT   (1802kb,D)

Title: The Ensemble Method for Thorax Diseases Classification
Authors: Bayu A. Nugroho
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2008.03008 ,  1802kb)
------------------------------------------------------------------------------
\\
arXiv:2005.09524
replaced with revised version Wed, 12 Aug 2020 04:49:36 GMT   (53kb,D)

Title: On repetitiveness measures of Thue-Morse words
Authors: Kanaru Kutsukake, Takuya Matsumoto, Yuto Nakashima, Shunsuke Inenaga,
  Hideo Bannai, Masayuki Takeda
Categories: cs.DS cs.DM
Comments: accepted to SPIRE 2020
\\ ( https://arxiv.org/abs/2005.09524 ,  53kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
