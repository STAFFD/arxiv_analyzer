Delivered-To: brucelu2013@gmail.com
Received: by 2002:a67:e3b6:0:0:0:0:0 with SMTP id j22csp1104079vsm;
        Thu, 11 Jun 2020 01:56:27 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJym5b+0oNH7t7+8tSSuhjZ3cTVLuYX077JO13bemdv1+HSlIThs2Hmsl8uQmMO1PfbTEQtJ
X-Received: by 2002:ae9:c10d:: with SMTP id z13mr6829735qki.3.1591865787296;
        Thu, 11 Jun 2020 01:56:27 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1591865787; cv=none;
        d=google.com; s=arc-20160816;
        b=S89yJFHQB4JScjyEycYNFGdUrngb4bwlI3eKsknn09iU8ZosTkUnavpkz/w4CrCqf/
         T5SdBIAvKOtEaHo8CPzvlsirOBoRVUZFcBzWQr0zxOT0O+mnX/fEfz+K/EzUjXb8+lU0
         8bM2VLMDcZaP2V3hBfYhlI2KGftyiY4qNpYs77IpdCsiVeESBkE89g5b3qZjHbIs4L1T
         q3ffxnNtXYHb4tzT4R/epuoQ+8faMYcciXssGGHuiaCqh0QeNwifzcO5KNe1CxE5me6G
         a37J6TRinoBQZrW+5T4tqwzdSfAHnEHOmnSTVoLeBvu2W7s/OBY2ZdajAWn7n2+kwkMI
         8c3w==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=NYOOKIChM00Ci3paLRjgP7epEqMHle6KRRqrenP0shI=;
        b=eqX/uJqfe5HMDb/8wDgw5rGY3r0L6YitXwyPC6W51OZepjVU+f3upUQw1R4kS9tKIS
         K2ZZwUm3tR/6KqLnqB2k8cbTICOyQbL7iMhhDjTpQPY+fPl6oJfDoPWqIQdcFC0VE0n1
         bIjqDz6xy4tJfuOaafgVzE6NBPV0Ue0CTCF4UJl52bIFM1Er1moKDoSq6bJmYTj4dKGD
         CEmgItcssD9QD3miq5ykk0gelkHnrlXd49R5IjC9RCYkvtRGuyCULrlwka5T5KwDG6+j
         eBbcwpA3et2J6qEO6XVgR3zF0LnCYhwvkZ8wWSkRnRHipHVUm9jLfBYVx/cGPKAQZC9F
         LmYQ==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id j11si1418797qke.306.2020.06.11.01.56.26
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 11 Jun 2020 01:56:27 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 05B8uQLl004888;
	Thu, 11 Jun 2020 04:56:26 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 05B8uQ1R008460;
	Thu, 11 Jun 2020 04:56:26 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 05B8uQ2V008459;
	Thu, 11 Jun 2020 04:56:26 -0400
Date: Thu, 11 Jun 2020 04:56:26 -0400
Message-Id: <202006110856.05B8uQ2V008459@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 15090 1
Content-Type: text/plain
MIME-Version: 1.0

This week, arXiv staff paused business-as-usual and
joined scientists participating in the #strike4blacklives and
#shutdownSTEM. For more information, read our statement here:
https://bit.ly/arXivStrike4BlackLives
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Emerging Technologies
Graphics
 received from  Mon  8 Jun 20 18:00:00 GMT  to  Wed 10 Jun 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2006.04859
Date: Mon, 8 Jun 2020 18:21:40 GMT   (1316kb,D)

Title: Novel Perception Algorithmic Framework For Object Identification and
  Tracking In Autonomous Navigation
Authors: Suryansh Saxena and Isaac K Isukapati
Categories: cs.CV cs.RO
\\
  This paper introduces a novel perception framework that has the ability to
identify and track objects in autonomous vehicle's field of view. The proposed
algorithms don't require any training for achieving this goal. The framework
makes use of ego-vehicle's pose estimation and a KD-Tree-based segmentation
algorithm to generate object clusters. In turn, using a VFH technique, the
geometry of each identified object cluster is translated into a multi-modal PDF
and a motion model is initiated with every new object cluster for the purpose
of robust spatio-temporal tracking. The methodology further uses statistical
properties of high-dimensional probability density functions and Bayesian
motion model estimates to identify and track objects from frame to frame. The
effectiveness of the methodology is tested on a KITTI dataset. The results show
that the median tracking accuracy is around 91% with an end-to-end
computational time of 153 milliseconds
\\ ( https://arxiv.org/abs/2006.04859 ,  1316kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04868
Date: Mon, 8 Jun 2020 18:38:24 GMT   (4083kb,D)

Title: DoubleU-Net: A Deep Convolutional Neural Network for Medical Image
  Segmentation
Authors: Debesh Jha, Michael A. Riegler, Dag Johansen, P{\aa}l Halvorsen,
  H{\aa}vard D. Johansen
Categories: cs.CV
\\
  Semantic image segmentation is the process of labeling each pixel of an image
with its corresponding class. An encoder-decoder based approach, like U-Net and
its variants, is a popular strategy for solving medical image segmentation
tasks. To improve the performance of U-Net on various segmentation tasks, we
propose a novel architecture called DoubleU-Net, which is a combination of two
U-Net architectures stacked on top of each other. The first U-Net uses a
pre-trained VGG-19 as the encoder, which has already learned features from
ImageNet and can be transferred to another task easily. To capture more
semantic information efficiently, we added another U-Net at the bottom. We also
adopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information
within the network. We have evaluated DoubleU-Net using four medical
segmentation datasets, covering various imaging modalities such as colonoscopy,
dermoscopy, and microscopy. Experiments on the MICCAI 2015 segmentation
challenge, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the
Lesion boundary segmentation datasets demonstrate that the DoubleU-Net
outperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more
accurate segmentation masks, especially in the case of the CVC-ClinicDB and
MICCAI 2015 segmentation challenge datasets, which have challenging images such
as smaller and flat polyps. These results show the improvement over the
existing U-Net model. The encouraging results, produced on various medical
image segmentation datasets, show that DoubleU-Net can be used as a strong
baseline for both medical image segmentation and cross-dataset evaluation
testing to measure the generalizability of Deep Learning (DL) models.
\\ ( https://arxiv.org/abs/2006.04868 ,  4083kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04874
Date: Mon, 8 Jun 2020 18:53:03 GMT   (9001kb,D)

Title: Skinning a Parameterization of Three-Dimensional Space for Neural
  Network Cloth
Authors: Jane Wu, Zhenglin Geng, Hui Zhou, Ronald Fedkiw
Categories: cs.CV
\\
  We present a novel learning framework for cloth deformation by embedding
virtual cloth into a tetrahedral mesh that parametrizes the volumetric region
of air surrounding the underlying body. In order to maintain this volumetric
parameterization during character animation, the tetrahedral mesh is
constrained to follow the body surface as it deforms. We embed the cloth mesh
vertices into this parameterization of three-dimensional space in order to
automatically capture much of the nonlinear deformation due to both joint
rotations and collisions. We then train a convolutional neural network to
recover ground truth deformation by learning cloth embedding offsets for each
skeletal pose. Our experiments show significant improvement over learning cloth
offsets from body surface parameterizations, both quantitatively and visually,
with prior state of the art having a mean error five standard deviations higher
than ours. Moreover, our results demonstrate the efficacy of a general learning
paradigm where high-frequency details can be embedded into low-frequency
parameterizations.
\\ ( https://arxiv.org/abs/2006.04874 ,  9001kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04894
Date: Mon, 8 Jun 2020 19:29:09 GMT   (8336kb,D)

Title: Probabilistic Semantic Mapping for Urban Autonomous Driving Applications
Authors: David Paz, Hengyuan Zhang, Qinru Li, Hao Xiang, Henrik Christensen
Categories: cs.CV
Comments: 6 pages, 10 figures, submitted to IROS 2020
\\
  Recent advancement in statistical learning and computational ability has
enabled autonomous vehicle technology to develop at a much faster rate and
become widely adopted. While many of the architectures previously introduced
are capable of operating under highly dynamic environments, many of these are
constrained to smaller-scale deployments and require constant maintenance due
to the associated scalability cost with high-definition (HD) maps. HD maps
provide critical information for self-driving cars to drive safely. However,
traditional approaches for creating HD maps involves tedious manual labeling.
As an attempt to tackle this problem, we fuse 2D image semantic segmentation
with pre-built point cloud maps collected from a relatively inexpensive 16
channel LiDAR sensor to construct a local probabilistic semantic map in bird's
eye view that encodes static landmarks such as roads, sidewalks, crosswalks,
and lanes in the driving environment. Experiments from data collected in an
urban environment show that this model can be extended for automatically
incorporating road features into HD maps with potential future work directions.
\\ ( https://arxiv.org/abs/2006.04894 ,  8336kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04898
Date: Mon, 8 Jun 2020 19:31:02 GMT   (7417kb,D)

Title: Reposing Humans by Warping 3D Features
Authors: Markus Knoche, Istv\'an S\'ar\'andi, Bastian Leibe
Categories: cs.CV
Comments: Accepted at CVPR 2020 Workshop on Human-Centric Image/Video Synthesis
\\
  We address the problem of reposing an image of a human into any desired novel
pose. This conditional image-generation task requires reasoning about the 3D
structure of the human, including self-occluded body parts. Most prior works
are either based on 2D representations or require fitting and manipulating an
explicit 3D body mesh. Based on the recent success in deep learning-based
volumetric representations, we propose to implicitly learn a dense feature
volume from human images, which lends itself to simple and intuitive
manipulation through explicit geometric warping. Once the latent feature volume
is warped according to the desired pose change, the volume is mapped back to
RGB space by a convolutional decoder. Our state-of-the-art results on the
DeepFashion and the iPER benchmarks indicate that dense volumetric human
representations are worth investigating in more detail.
\\ ( https://arxiv.org/abs/2006.04898 ,  7417kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04902
Date: Mon, 8 Jun 2020 19:36:26 GMT   (14688kb,D)

Title: What Matters in Unsupervised Optical Flow
Authors: Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon,
  Kurt Konolige, Anelia Angelova
Categories: cs.CV cs.LG eess.IV
Comments: Source code is available at
  https://github.com/google-research/google-research/tree/master/uflow
\\
  We systematically compare and analyze a set of key components in unsupervised
optical flow to identify which photometric loss, occlusion handling, and
smoothness regularization is most effective. Alongside this investigation we
construct a number of novel improvements to unsupervised flow models, such as
cost volume normalization, stopping the gradient at the occlusion mask,
encouraging smoothness before upsampling the flow field, and continual
self-supervision with image resizing. By combining the results of our
investigation with our improved model components, we are able to present a new
unsupervised flow technique that significantly outperforms the previous
unsupervised state-of-the-art and performs on par with supervised FlowNet2 on
the KITTI 2015 dataset, while also being significantly simpler than related
approaches.
\\ ( https://arxiv.org/abs/2006.04902 ,  14688kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04924
Date: Mon, 8 Jun 2020 20:42:39 GMT   (9398kb,D)

Title: A Self-supervised Approach for Adversarial Robustness
Authors: Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih
  Porikli
Categories: cs.CV
Comments: CVPR-2020 (Oral). Code this http
  https://github.com/Muzammal-Naseer/NRP}
\\
  Adversarial examples can cause catastrophic mistakes in Deep Neural Network
(DNNs) based vision systems e.g., for classification, segmentation and object
detection. The vulnerability of DNNs against such attacks can prove a major
roadblock towards their real-world deployment. Transferability of adversarial
examples demand generalizable defenses that can provide cross-task protection.
Adversarial training that enhances robustness by modifying target model's
parameters lacks such generalizability. On the other hand, different input
processing based defenses fall short in the face of continuously evolving
attacks. In this paper, we take the first step to combine the benefits of both
approaches and propose a self-supervised adversarial training mechanism in the
input space. By design, our defense is a generalizable approach and provides
significant robustness against the \textbf{unseen} adversarial attacks (\eg by
reducing the success rate of translation-invariant \textbf{ensemble} attack
from 82.6\% to 31.9\% in comparison to previous state-of-the-art). It can be
deployed as a plug-and-play solution to protect a variety of vision systems, as
we demonstrate for the case of classification, segmentation and detection. Code
is available at: {\small\url{https://github.com/Muzammal-Naseer/NRP}}.
\\ ( https://arxiv.org/abs/2006.04924 ,  9398kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04973
Date: Mon, 8 Jun 2020 22:35:12 GMT   (3026kb,D)

Title: Pixel-Wise Motion Deblurring of Thermal Videos
Authors: Manikandasriram Srinivasan Ramanagopal, Zixu Zhang, Ram Vasudevan,
  Matthew Johnson-Roberson
Categories: cs.CV cs.RO
Comments: 10 pages, 8 figures, Accepted to Robotics: Science and Systems 2020
\\
  Uncooled microbolometers can enable robots to see in the absence of visible
illumination by imaging the "heat" radiated from the scene. Despite this
ability to see in the dark, these sensors suffer from significant motion blur.
This has limited their application on robotic systems. As described in this
paper, this motion blur arises due to the thermal inertia of each pixel. This
has meant that traditional motion deblurring techniques, which rely on
identifying an appropriate spatial blur kernel to perform spatial
deconvolution, are unable to reliably perform motion deblurring on thermal
camera images. To address this problem, this paper formulates reversing the
effect of thermal inertia at a single pixel as a Least Absolute Shrinkage and
Selection Operator (LASSO) problem which we can solve rapidly using a quadratic
programming solver. By leveraging sparsity and a high frame rate, this
pixel-wise LASSO formulation is able to recover motion deblurred frames of
thermal videos without using any spatial information. To compare its quality
against state-of-the-art visible camera based deblurring methods, this paper
evaluated the performance of a family of pre-trained object detectors on a set
of images restored by different deblurring algorithms. All evaluated object
detectors performed systematically better on images restored by the proposed
algorithm rather than any other tested, state-of-the-art methods.
\\ ( https://arxiv.org/abs/2006.04973 ,  3026kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04991
Date: Mon, 8 Jun 2020 23:34:08 GMT   (320kb,D)

Title: Rethinking Classification Loss Designs for Person Re-identification with
  a Unified View
Authors: Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Shif-Fu Chang
Categories: cs.CV
\\
  Person Re-identification (ReID) aims at matching a person of interest across
images. In convolutional neural networks (CNNs) based approaches, loss design
plays a role of metric learning which guides the feature learning process to
pull closer features of the same identity and to push far apart features of
different identities. In recent years, the combination of classification loss
and triplet loss achieves superior performance and is predominant in ReID. In
this paper, we rethink these loss functions within a generalized formulation
and argue that triplet-based optimization can be viewed as a two-class
subsampling classification, which performs classification over two sampled
categories based on instance similarities. Furthermore, we present a case study
which demonstrates that increasing the number of simultaneously considered
instance classes significantly improves the ReID performance, since it is
aligned better with the ReID test/inference process. With the multi-class
subsampling classification incorporated, we provide a strong baseline which
achieves the state-of-the-art performance on the benchmark person ReID
datasets. Finally, we propose a new meta prototypical N-tuple loss for more
efficient multi-class subsampling classification. We aim to inspire more new
loss designs in the person ReID field.
\\ ( https://arxiv.org/abs/2006.04991 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05011
Date: Tue, 9 Jun 2020 01:55:48 GMT   (6483kb,D)

Title: RGB-D-E: Event Camera Calibration for Fast 6-DOF Object Tracking
Authors: Etienne Dubeau, Mathieu Garon, Benoit Debaque, Raoul de Charette,
  Jean-Fran\c{c}ois Lalonde
Categories: cs.CV
Comments: 8 pages, 8 figures
\\
  Augmented reality devices require multiple sensors to perform various tasks
such as localization and tracking. Currently, popular cameras are mostly
frame-based (e.g. RGB and Depth) which impose a high data bandwidth and power
usage. With the necessity for low power and more responsive augmented reality
systems, using solely frame-based sensors imposes limits to the various
algorithms that needs high frequency data from the environement. As such,
event-based sensors have become increasingly popular due to their low power,
bandwidth and latency, as well as their very high frequency data acquisition
capabilities. In this paper, we propose, for the first time, to use an
event-based camera to increase the speed of 3D object tracking in 6 degrees of
freedom. This application requires handling very high object speed to convey
compelling AR experiences. To this end, we propose a new system which combines
a recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We
develop a deep learning approach, which combines an existing RGB-D network
along with a novel event-based network in a cascade fashion, and demonstrate
that our approach significantly improves the robustness of a state-of-the-art
frame-based 6-DOF object tracker using our RGB-D-E pipeline.
\\ ( https://arxiv.org/abs/2006.05011 ,  6483kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05015
Date: Tue, 9 Jun 2020 02:23:22 GMT   (1012kb)

Title: Can Synthetic Data Improve Object Detection Results for Remote Sensing
  Images?
Authors: Weixing Liu, Jun Liu and Bin Luo
Categories: cs.CV
Comments: 5 pages, 5 figures
\\
  Deep learning approaches require enough training samples to perform well, but
it is a challenge to collect enough real training data and label them manually.
In this letter, we propose the use of realistic synthetic data with a wide
distribution to improve the performance of remote sensing image aircraft
detection. Specifically, to increase the variability of synthetic data, we
randomly set the parameters during rendering, such as the size of the instance
and the class of background images. In order to make the synthetic images more
realistic, we then refine the synthetic images at the pixel level using
CycleGAN with real unlabeled images. We also fine-tune the model with a small
amount of real data, to obtain a higher accuracy. Experiments on NWPU VHR-10,
UCAS-AOD and DIOR datasets demonstrate that the proposed method can be applied
for augmenting insufficient real data.
\\ ( https://arxiv.org/abs/2006.05015 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05049
Date: Tue, 9 Jun 2020 04:59:26 GMT   (5734kb,D)

Title: Single Image Deraining via Scale-space Invariant Attention Neural
  Network
Authors: Bo Pang, Deming Zhai, Member, IEEE, Junjun Jiang, Member, IEEE,
  Xianming Liu, Member, IEEE
Categories: cs.CV
\\
  Image enhancement from degradation of rainy artifacts plays a critical role
in outdoor visual computing systems. In this paper, we tackle the notion of
scale that deals with visual changes in appearance of rain steaks with respect
to the camera. Specifically, we revisit multi-scale representation by
scale-space theory, and propose to represent the multi-scale correlation in
convolutional feature domain, which is more compact and robust than that in
pixel domain. Moreover, to improve the modeling ability of the network, we do
not treat the extracted multi-scale features equally, but design a novel
scale-space invariant attention mechanism to help the network focus on parts of
the features. In this way, we summarize the most activated presence of feature
maps as the salient features. Extensive experiments results on synthetic and
real rainy scenes demonstrate the superior performance of our scheme over the
state-of-the-arts.
\\ ( https://arxiv.org/abs/2006.05049 ,  5734kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05066
Date: Tue, 9 Jun 2020 06:09:42 GMT   (611kb,D)

Title: Learning Shared Filter Bases for Efficient ConvNets
Authors: Daeyeon Kim, Woochul Kang
Categories: cs.CV cs.LG eess.IV
\\
  Modern convolutional neural networks (ConvNets) achieve state-of-the-art
performance for many computer vision tasks. However, such high performance
requires millions of parameters and high computational costs. Recently,
inspired by the iterative structure of modern ConvNets, such as ResNets,
parameter sharing among repetitive convolution layers has been proposed to
reduce the size of parameters. However, naive sharing of convolution filters
poses many challenges such as overfitting and vanishing/exploding gradients.
Furthermore, parameter sharing often increases computational complexity due to
additional operations. In this paper, we propose to exploit the linear
structure of convolution filters for effective and efficient sharing of
parameters among iterative convolution layers. Instead of sharing convolution
filters themselves, we hypothesize that a filter basis of linearly-decomposed
convolution layers are more effective units for sharing parameters since a
filter basis is an intrinsic and reusable building block constituting diverse
high dimensional convolution filters. The representation power and peculiarity
of individual convolution layers are further increased by adding a small number
of layer-specific non-shared components to the filter basis. We show
empirically that enforcing orthogonality to shared filter bases can mitigate
the difficulty in training shared parameters. Experimental results show that
our approach achieves significant reductions both in model parameters and
computational costs while maintaining competitive, and often better,
performance than non-shared baseline networks.
\\ ( https://arxiv.org/abs/2006.05066 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05074
Date: Tue, 9 Jun 2020 06:53:58 GMT   (4215kb,D)

Title: Detection of Makeup Presentation Attacks based on Deep Face
  Representations
Authors: Christian Rathgeb, Pawel Drozdowski, Christoph Busch
Categories: cs.CV
\\
  Facial cosmetics have the ability to substantially alter the facial
appearance, which can negatively affect the decisions of a face recognition. In
addition, it was recently shown that the application of makeup can be abused to
launch so-called makeup presentation attacks. In such attacks, the attacker
might apply heavy makeup in order to achieve the facial appearance of a target
subject for the purpose of impersonation. In this work, we assess the
vulnerability of a COTS face recognition system to makeup presentation attacks
employing the publicly available Makeup Induced Face Spoofing (MIFS) database.
It is shown that makeup presentation attacks might seriously impact the
security of the face recognition system. Further, we propose an attack
detection scheme which distinguishes makeup presentation attacks from genuine
authentication attempts by analysing differences in deep face representations
obtained from potential makeup presentation attacks and corresponding target
face images. The proposed detection system employs a machine learning-based
classifier, which is trained with synthetically generated makeup presentation
attacks utilizing a generative adversarial network for facial makeup transfer
in conjunction with image warping. Experimental evaluations conducted using the
MIFS database reveal a detection equal error rate of 0.7% for the task of
separating genuine authentication attempts from makeup presentation attacks.
\\ ( https://arxiv.org/abs/2006.05074 ,  4215kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05077
Date: Tue, 9 Jun 2020 06:56:50 GMT   (7553kb,D)

Title: SEKD: Self-Evolving Keypoint Detection and Description
Authors: Yafei Song, Ling Cai, Jia Li, Yonghong Tian, Mingyang Li
Categories: cs.CV
\\
  Researchers have attempted utilizing deep neural network (DNN) to learn novel
local features from images inspired by its recent successes on a variety of
vision tasks. However, existing DNN-based algorithms have not achieved such
remarkable progress that could be partly attributed to insufficient utilization
of the interactive characters between local feature detector and descriptor. To
alleviate these difficulties, we emphasize two desired properties, i.e.,
repeatability and reliability, to simultaneously summarize the inherent and
interactive characters of local feature detector and descriptor. Guided by
these properties, a self-supervised framework, namely self-evolving keypoint
detection and description (SEKD), is proposed to learn an advanced local
feature model from unlabeled natural images. Additionally, to have performance
guarantees, novel training strategies have also been dedicatedly designed to
minimize the gap between the learned feature and its properties. We benchmark
the proposed method on homography estimation, relative pose estimation, and
structure-from-motion tasks. Extensive experimental results demonstrate that
the proposed method outperforms popular hand-crafted and DNN-based methods by
remarkable margins. Ablation studies also verify the effectiveness of each
critical training strategy. We will release our code along with the trained
model publicly.
\\ ( https://arxiv.org/abs/2006.05077 ,  7553kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05091
Date: Tue, 9 Jun 2020 07:40:23 GMT   (864kb,D)

Title: PNL: Efficient Long-Range Dependencies Extraction with Pyramid Non-Local
  Module for Action Recognition
Authors: Yuecong Xu, Haozhi Cao, Jianfei Yang, Kezhi Mao, Jianxiong Yin and
  Simon See
Categories: cs.CV
Comments: Single column, 26 pages, 6 figures
\\
  Long-range spatiotemporal dependencies capturing plays an essential role in
improving video features for action recognition. The non-local block inspired
by the non-local means is designed to address this challenge and have shown
excellent performance. However, the non-local block brings significant increase
in computation cost to the original network. It also lacks the ability to model
regional correlation in videos. To address the above limitations, we propose
Pyramid Non-Local (PNL) module, which extends the non-local block by
incorporating regional correlation at multiple scales through a pyramid
structured module. This extension upscales the effectiveness of non-local
operation by attending to the interaction between different regions. Empirical
results prove the effectiveness and efficiency of our PNL module, which
achieves state-of-the-art performance of 83.09% on the Mini-Kinetics dataset,
with decreased computation cost compared to the non-local block.
\\ ( https://arxiv.org/abs/2006.05091 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05095
Date: Tue, 9 Jun 2020 07:47:21 GMT   (69kb,D)

Title: Towards an Intrinsic Definition of Robustness for a Classifier
Authors: Th\'eo Giraudon, Vincent Gripon, Matthias L\"owe, Franck Vermet
Categories: cs.CV stat.ML
Comments: 10 pages, 4 figures
\\
  The robustness of classifiers has become a question of paramount importance
in the past few years. Indeed, it has been shown that state-of-the-art deep
learning architectures can easily be fooled with imperceptible changes to their
inputs. Therefore, finding good measures of robustness of a trained classifier
is a key issue in the field. In this paper, we point out that averaging the
radius of robustness of samples in a validation set is a statistically weak
measure. We propose instead to weight the importance of samples depending on
their difficulty. We motivate the proposed score by a theoretical case study
using logistic regression, where we show that the proposed score is independent
of the choice of the samples it is evaluated upon. We also empirically
demonstrate the ability of the proposed score to measure robustness of
classifiers with little dependence on the choice of samples in more complex
settings, including deep convolutional neural networks and real datasets.
\\ ( https://arxiv.org/abs/2006.05095 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05097
Date: Tue, 9 Jun 2020 07:49:49 GMT   (2784kb,D)

Title: GAP++: Learning to generate target-conditioned adversarial examples
Authors: Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue
Categories: cs.CV
Comments: Accepted to IJCAI 2019 AIBS Workshop
\\
  Adversarial examples are perturbed inputs which can cause a serious threat
for machine learning models. Finding these perturbations is such a hard task
that we can only use the iterative methods to traverse. For computational
efficiency, recent works use adversarial generative networks to model the
distribution of both the universal or image-dependent perturbations directly.
However, these methods generate perturbations only rely on input images. In
this work, we propose a more general-purpose framework which infers
target-conditioned perturbations dependent on both input image and target
label. Different from previous single-target attack models, our model can
conduct target-conditioned attacks by learning the relations of attack target
and the semantics in image. Using extensive experiments on the datasets of
MNIST and CIFAR10, we show that our method achieves superior performance with
single target attack models and obtains high fooling rates with small
perturbation norms.
\\ ( https://arxiv.org/abs/2006.05097 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05121
Date: Tue, 9 Jun 2020 08:50:39 GMT   (692kb,D)

Title: Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?
Authors: Corentin Kervadec (LIRIS), Grigory Antipov, Moez Baccouche (imagine),
  Christian Wolf (imagine)
Categories: cs.CV
\\
  To be reliable on rare events is an important requirement for systems based
on machine learning. In this work we focus on Visual Question Answering (VQA),
where, in spite of recent efforts, datasets remain imbalanced, causing
shortcomings of current models: tendencies to overly exploit dataset biases and
struggles to generalise to unseen associations of concepts. We focus on a
systemic evaluation of model error distributions and address fundamental
questions: How is the prediction error distributed? What is the prediction
accuracy on infrequent vs. frequent concepts? In this work, we design a new
benchmark based on a fine-grained reorganization of the GQA dataset [1], which
allows to precisely answer these questions. It introduces distributions shifts
in both validation and test splits, which are defined on question groups and
are thus tailored to each question. We performed a large-scale study and we
experimentally demonstrate that several state-of-the-art VQA models, even those
specifically designed for bias reduction, fail to address questions involving
infrequent concepts. Furthermore, we show that the high accuracy obtained on
the frequent concepts alone is mechanically increasing overall accuracy,
covering up the true behavior of current VQA models.
\\ ( https://arxiv.org/abs/2006.05121 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05127
Date: Tue, 9 Jun 2020 08:59:54 GMT   (11159kb,D)

Title: Over-crowdedness Alert! Forecasting the Future Crowd Distribution
Authors: Yuzhen Niu, Weifeng Shi, Wenxi Liu, Shengfeng He, Jia Pan, Antoni B.
  Chan
Categories: cs.CV
\\
  In recent years, vision-based crowd analysis has been studied extensively due
to its practical applications in real world. In this paper, we formulate a
novel crowd analysis problem, in which we aim to predict the crowd distribution
in the near future given sequential frames of a crowd video without any
identity annotations. Studying this research problem will benefit applications
concerned with forecasting crowd dynamics. To solve this problem, we propose a
global-residual two-stream recurrent network, which leverages the consecutive
crowd video frames as inputs and their corresponding density maps as auxiliary
information to predict the future crowd distribution. Moreover, to strengthen
the capability of our network, we synthesize scene-specific crowd density maps
using simulated data for pretraining. Finally, we demonstrate that our
framework is able to predict the crowd distribution for different crowd
scenarios and we delve into applications including predicting future crowd
count, forecasting high-density region, etc.
\\ ( https://arxiv.org/abs/2006.05127 ,  11159kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05132
Date: Tue, 9 Jun 2020 09:04:41 GMT   (2874kb)

Title: A Survey on Generative Adversarial Networks: Variants, Applications, and
  Training
Authors: Abdul Jabbar, Xi Li, and Bourahla Omar
Categories: cs.CV cs.LG
\\
  The Generative Models have gained considerable attention in the field of
unsupervised learning via a new and practical framework called Generative
Adversarial Networks (GAN) due to its outstanding data generation capability.
Many models of GAN have proposed, and several practical applications emerged in
various domains of computer vision and machine learning. Despite GAN's
excellent success, there are still obstacles to stable training. The problems
are due to Nash-equilibrium, internal covariate shift, mode collapse, vanishing
gradient, and lack of proper evaluation metrics. Therefore, stable training is
a crucial issue in different applications for the success of GAN. Herein, we
survey several training solutions proposed by different researchers to
stabilize GAN training. We survey, (I) the original GAN model and its modified
classical versions, (II) detail analysis of various GAN applications in
different domains, (III) detail study about the various GAN training obstacles
as well as training solutions. Finally, we discuss several new issues as well
as research outlines to the topic.
\\ ( https://arxiv.org/abs/2006.05132 ,  2874kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05142
Date: Tue, 9 Jun 2020 09:33:04 GMT   (1082kb,D)

Title: Smooth Proxy-Anchor Loss for Noisy Metric Learning
Authors: Carlos Roig and David Varas and Issey Masuda and Juan Carlos Riveiro
  and Elisenda Bou-Balust
Categories: cs.CV
Comments: The 4th Workshop on Visual Understanding by Learning from Web Data
  (CVPR 2020)
Journal-ref: The 4th Workshop on Visual Understanding by Learning from Web Data
  (CVPR 2020)
\\
  Many industrial applications use Metric Learning as a way to circumvent
scalability issues when designing systems with a high number of classes.
Because of this, this field of research is attracting a lot of interest from
the academic and non-academic communities. Such industrial applications require
large-scale datasets, which are usually generated with web data and, as a
result, often contain a high number of noisy labels. While Metric Learning
systems are sensitive to noisy labels, this is usually not tackled in the
literature, that relies on manually annotated datasets.
  In this work, we propose a Metric Learning method that is able to overcome
the presence of noisy labels using our novel Smooth Proxy-Anchor Loss. We also
present an architecture that uses the aforementioned loss with a two-phase
learning procedure. First, we train a confidence module that computes sample
class confidences. Second, these confidences are used to weight the influence
of each sample for the training of the embeddings. This results in a system
that is able to provide robust sample embeddings.
  We compare the performance of the described method with current
state-of-the-art Metric Learning losses (proxy-based and pair-based), when
trained with a dataset containing noisy labels. The results showcase an
improvement of 2.63 and 3.29 in Recall@1 with respect to MultiSimilarity and
Proxy-Anchor Loss respectively, proving that our method outperforms the
state-of-the-art of Metric Learning in noisy labeling conditions.
\\ ( https://arxiv.org/abs/2006.05142 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05159
Date: Tue, 9 Jun 2020 09:52:44 GMT   (3241kb,D)

Title: Physically constrained short-term vehicle trajectory forecasting with
  naive semantic maps
Authors: Albert Dulian and John C. Murray
Categories: cs.CV cs.AI cs.NE
\\
  Urban environments manifest a high level of complexity, and therefore it is
of vital importance for safety systems embedded within autonomous vehicles
(AVs) to be able to accurately predict the short-term future motion of nearby
agents. This problem can be further understood as generating a sequence of
future coordinates for a given agent based on its past motion data e.g.
position, velocity, acceleration etc, and whilst current approaches demonstrate
plausible results they have a propensity to neglect a scene's physical
constrains. In this paper we propose the model based on a combination of the
CNN and LSTM encoder-decoder architecture that learns to extract a relevant
road features from semantic maps as well as general motion of agents and uses
this learned representation to predict their short-term future trajectories. We
train and validate the model on the publicly available dataset that provides
data from urban areas, allowing us to examine it in challenging and uncertain
scenarios. We show that our model is not only capable of anticipating future
motion whilst taking into consideration road boundaries, but can also
effectively and precisely predict trajectories for a longer time horizon than
initially trained for.
\\ ( https://arxiv.org/abs/2006.05159 ,  3241kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05179
Date: Tue, 9 Jun 2020 10:56:50 GMT   (1375kb,D)

Title: Reconstruction and Quantification of 3D Iris Surface for Angle-Closure
  Glaucoma Detection in Anterior Segment OCT
Authors: Jinkui Hao, Huazhu Fu, Yanwu Xu, Yan Hu, Fei Li, Xiulan Zhang, Jiang
  Liu, Yitian Zhao
Categories: cs.CV
Comments: has been accepted by MICCAI 2020
\\
  Precise characterization and analysis of iris shape from Anterior Segment OCT
(AS-OCT) are of great importance in facilitating diagnosis of
angle-closure-related diseases. Existing methods focus solely on analyzing
structural properties identified from the 2D slice, while accurate
characterization of morphological changes of iris shape in 3D AS-OCT may be
able to reveal in addition the risk of disease progression. In this paper, we
propose a novel framework for reconstruction and quantification of 3D iris
surface from AS-OCT imagery. We consider it to be the first work to detect
angle-closure glaucoma by means of 3D representation. An iris segmentation
network with wavelet refinement block (WRB) is first proposed to generate the
initial shape of the iris from single AS-OCT slice. The 3D iris surface is then
reconstructed using a guided optimization method with Poisson-disk sampling.
Finally, a set of surface-based features are extracted, which are used in
detecting of angle-closure glaucoma. Experimental results demonstrate that our
method is highly effective in iris segmentation and surface reconstruction.
Moreover, we show that 3D-based representation achieves better performance in
angle-closure glaucoma detection than does 2D-based feature.
\\ ( https://arxiv.org/abs/2006.05179 ,  1375kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05180
Date: Tue, 9 Jun 2020 10:59:15 GMT   (2988kb,D)

Title: Breaking the Limits of Remote Sensing by Simulation and Deep Learning
  for Flood and Debris Flow Mapping
Authors: Naoto Yokoya, Kazuki Yamanoi, Wei He, Gerald Baier, Bruno Adriano,
  Hiroyuki Miura, Satoru Oishi
Categories: cs.CV eess.IV
\\
  We propose a framework that estimates inundation depth (maximum water level)
and debris-flow-induced topographic deformation from remote sensing imagery by
integrating deep learning and numerical simulation. A water and debris flow
simulator generates training data for various artificial disaster scenarios. We
show that regression models based on Attention U-Net and LinkNet architectures
trained on such synthetic data can predict the maximum water level and
topographic deformation from a remote sensing-derived change detection map and
a digital elevation model. The proposed framework has an inpainting capability,
thus mitigating the false negatives that are inevitable in remote sensing image
analysis. Our framework breaks the limits of remote sensing and enables rapid
estimation of inundation depth and topographic deformation, essential
information for emergency response, including rescue and relief activities. We
conduct experiments with both synthetic and real data for two disaster events
that caused simultaneous flooding and debris flows and demonstrate the
effectiveness of our approach quantitatively and qualitatively.
\\ ( https://arxiv.org/abs/2006.05180 ,  2988kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05183
Date: Tue, 9 Jun 2020 11:07:08 GMT   (76kb,D)

Title: A Note on Deepfake Detection with Low-Resources
Authors: Piotr Kawa and Piotr Syga
Categories: cs.CV cs.LG
\\
  Deepfakes are videos that include changes, quite often substituting face of a
portrayed individual with a different face using neural networks. Even though
the technology gained its popularity as a carrier of jokes and parodies it
raises a serious threat to ones security - via biometric impersonation or
besmearing. In this paper we present two methods that allow detecting Deepfakes
for a user without significant computational power. In particular, we enhance
MesoNet by replacing the original activation functions allowing a nearly 1%
improvement as well as increasing the consistency of the results. Moreover, we
introduced and verified a new activation function - Pish that at the cost of
slight time overhead allows even higher consistency.
  Additionally, we present a preliminary results of Deepfake detection method
based on Local Feature Descriptors (LFD), that allows setting up the system
even faster and without resorting to GPU computation. Our method achieved Equal
Error Rate of 0.28, with both accuracy and recall exceeding 0.7.
\\ ( https://arxiv.org/abs/2006.05183 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05187
Date: Tue, 9 Jun 2020 11:19:24 GMT   (550kb,D)

Title: Stereo RGB and Deeper LIDAR Based Network for 3D Object Detection
Authors: Qingdong He, Zhengning Wang, Hao Zeng, Yijun Liu, Shuaicheng Liu, Bing
  Zeng
Categories: cs.CV
\\
  3D object detection has become an emerging task in autonomous driving
scenarios. Previous works process 3D point clouds using either projection-based
or voxel-based models. However, both approaches contain some drawbacks. The
voxel-based methods lack semantic information, while the projection-based
methods suffer from numerous spatial information loss when projected to
different views. In this paper, we propose the Stereo RGB and Deeper LIDAR
(SRDL) framework which can utilize semantic and spatial information
simultaneously such that the performance of network for 3D object detection can
be improved naturally. Specifically, the network generates candidate boxes from
stereo pairs and combines different region-wise features using a deep fusion
scheme. The stereo strategy offers more information for prediction compared
with prior works. Then, several local and global feature extractors are stacked
in the segmentation module to capture richer deep semantic geometric features
from point clouds. After aligning the interior points with fused features, the
proposed network refines the prediction in a more accurate manner and encodes
the whole box in a novel compact method. The decent experimental results on the
challenging KITTI detection benchmark demonstrate the effectiveness of
utilizing both stereo images and point clouds for 3D object detection.
\\ ( https://arxiv.org/abs/2006.05187 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05196
Date: Tue, 9 Jun 2020 11:43:46 GMT   (2666kb,D)

Title: Multi-spectral Facial Landmark Detection
Authors: Jin Keong, Xingbo Dong, Zhe Jin, Khawla Mallat, Jean-Luc Dugelay
Categories: cs.CV
\\
  Thermal face image analysis is favorable for certain circumstances. For
example, illumination-sensitive applications, like nighttime surveillance; and
privacy-preserving demanded access control. However, the inadequate study on
thermal face image analysis calls for attention in responding to the industry
requirements. Detecting facial landmark points are important for many face
analysis tasks, such as face recognition, 3D face reconstruction, and face
expression recognition. In this paper, we propose a robust neural network
enabled facial landmark detection, namely Deep Multi-Spectral Learning (DMSL).
Briefly, DMSL consists of two sub-models, i.e. face boundary detection, and
landmark coordinates detection. Such an architecture demonstrates the
capability of detecting the facial landmarks on both visible and thermal
images. Particularly, the proposed DMSL model is robust in facial landmark
detection where the face is partially occluded, or facing different directions.
The experiment conducted on Eurecom's visible and thermal paired database shows
the superior performance of DMSL over the state-of-the-art for thermal facial
landmark detection. In addition to that, we have annotated a thermal face
dataset with their respective facial landmark for the purpose of
experimentation.
\\ ( https://arxiv.org/abs/2006.05196 ,  2666kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05210
Date: Tue, 9 Jun 2020 12:10:04 GMT   (3287kb,D)

Title: Neural Network Activation Quantization with Bitwise Information
  Bottlenecks
Authors: Xichuan Zhou, Kui Liu, Cong Shi, Haijun Liu, Ji Liu
Categories: cs.CV
\\
  Recent researches on information bottleneck shed new light on the continuous
attempts to open the black box of neural signal encoding. Inspired by the
problem of lossy signal compression for wireless communication, this paper
presents a Bitwise Information Bottleneck approach for quantizing and encoding
neural network activations. Based on the rate-distortion theory, the Bitwise
Information Bottleneck attempts to determine the most significant bits in
activation representation by assigning and approximating the sparse coefficient
associated with each bit. Given the constraint of a limited average code rate,
the information bottleneck minimizes the rate-distortion for optimal activation
quantization in a flexible layer-by-layer manner. Experiments over ImageNet and
other datasets show that, by minimizing the quantization rate-distortion of
each layer, the neural network with information bottlenecks achieves the
state-of-the-art accuracy with low-precision activation. Meanwhile, by reducing
the code rate, the proposed method can improve the memory and computational
efficiency by over six times compared with the deep neural network with
standard single-precision representation. Codes will be available on GitHub
when the paper is accepted \url{https://github.com/BitBottleneck/PublicCode}.
\\ ( https://arxiv.org/abs/2006.05210 ,  3287kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05220
Date: Tue, 9 Jun 2020 12:35:55 GMT   (6914kb,D)

Title: Rethinking Localization Map: Towards Accurate Object Perception with
  Self-Enhancement Maps
Authors: Xiaolin Zhang, Yunchao Wei, Yi Yang, Fei Wu
Categories: cs.CV
\\
  Recently, remarkable progress has been made in weakly supervised object
localization (WSOL) to promote object localization maps. The common practice of
evaluating these maps applies an indirect and coarse way, i.e., obtaining tight
bounding boxes which can cover high-activation regions and calculating
intersection-over-union (IoU) scores between the predicted and ground-truth
boxes. This measurement can evaluate the ability of localization maps to some
extent, but we argue that the maps should be measured directly and delicately,
i.e., comparing the maps with the ground-truth object masks pixel-wisely. To
fulfill the direct evaluation, we annotate pixel-level object masks on the
ILSVRC validation set. We propose to use IoU-Threshold curves for evaluating
the real quality of localization maps. Beyond the amended evaluation metric and
annotated object masks, this work also introduces a novel self-enhancement
method to harvest accurate object localization maps and object boundaries with
only category labels as supervision. We propose a two-stage approach to
generate the localization maps by simply comparing the similarity of point-wise
features between the high-activation and the rest pixels. Based on the
predicted localization maps, we explore to estimate object boundaries on a very
large dataset. A hard-negative suppression loss is proposed for obtaining fine
boundaries. We conduct extensive experiments on the ILSVRC and CUB benchmarks.
In particular, the proposed Self-Enhancement Maps achieve the state-of-the-art
localization accuracy of 54.88% on ILSVRC. The code and the annotated masks are
released at https://github.com/xiaomengyc/SEM.
\\ ( https://arxiv.org/abs/2006.05220 ,  6914kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05325
Date: Tue, 9 Jun 2020 15:02:55 GMT   (3622kb,D)

Title: ComboNet: Combined 2D & 3D Architecture for Aorta Segmentation
Authors: Orhan Akal, Zhigang Peng and Gerardo Hermosillo Valadez
Categories: cs.CV
Comments: 9 pages, 3 figures, 3 tables
\\
  3D segmentation with deep learning if trained with full resolution is the
ideal way of achieving the best accuracy. Unlike in 2D, 3D segmentation
generally does not have sparse outliers, prevents leakage to surrounding soft
tissues, at the very least it is generally more consistent than 2D
segmentation. However, GPU memory is generally the bottleneck for such an
application. Thus, most of the 3D segmentation applications handle sub-sampled
input instead of full resolution, which comes with the cost of losing precision
at the boundary. In order to maintain precision at the boundary and prevent
sparse outliers and leakage, we designed ComboNet. ComboNet is designed in an
end to end fashion with three sub-network structures. The first two are
parallel: 2D UNet with full resolution and 3D UNet with four times sub-sampled
input. The last stage is the concatenation of 2D and 3D outputs along with a
full-resolution input image which is followed by two convolution layers either
with 2D or 3D convolutions. With ComboNet we have achieved $92.1\%$ dice
accuracy for aorta segmentation. With Combonet, we have observed up to $2.3\%$
improvement of dice accuracy as opposed to 2D UNet with the full-resolution
input image.
\\ ( https://arxiv.org/abs/2006.05325 ,  3622kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05327
Date: Tue, 9 Jun 2020 15:05:08 GMT   (6717kb,D)

Title: mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level
  Estimation
Authors: Roberto Daza, Aythami Morales, Julian Fierrez, Ruben Tolosana
Categories: cs.CV cs.HC
\\
  This work presents mEBAL, a multimodal database for eye blink detection and
attention level estimation. The eye blink frequency is related to the cognitive
activity and automatic detectors of eye blinks have been proposed for many
tasks including attention level estimation, analysis of neuro-degenerative
diseases, deception recognition, drive fatigue detection, or face
anti-spoofing. However, most existing databases and algorithms in this area are
limited to experiments involving only a few hundred samples and individual
sensors like face cameras. The proposed mEBAL improves previous databases in
terms of acquisition sensors and samples. In particular, three different
sensors are simultaneously considered: Near Infrared (NIR) and RGB cameras to
capture the face gestures and an Electroencephalography (EEG) band to capture
the cognitive activity of the user and blinking events. Regarding the size of
mEBAL, it comprises 6,000 samples and the corresponding attention level from 38
different students while conducting a number of e-learning tasks of varying
difficulty. In addition to presenting mEBAL, we also include preliminary
experiments on: i) eye blink detection using Convolutional Neural Networks
(CNN) with the facial images, and ii) attention level estimation of the
students based on their eye blink frequency.
\\ ( https://arxiv.org/abs/2006.05327 ,  6717kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05338
Date: Tue, 9 Jun 2020 15:19:26 GMT   (1132kb,D)

Title: Towards Good Practices for Data Augmentation in GAN Training
Authors: Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen,
  Ngai-Man Cheung
Categories: cs.CV eess.IV
\\
  Recent successes in Generative Adversarial Networks (GAN) have affirmed the
importance of using more data in GAN training. Yet it is expensive to collect
data in many domains such as medical applications. Data Augmentation (DA) has
been applied in these applications. In this work, we first argue that the
classical DA approach could mislead the generator to learn the distribution of
the augmented data, which could be different from that of the original data. We
then propose a principled framework, termed Data Augmentation Optimized for GAN
(DAG), to enable the use of augmented data in GAN training to improve the
learning of the original distribution. We provide theoretical analysis to show
that using our proposed DAG aligns with the original GAN in minimizing the JS
divergence w.r.t. the original distribution and it leverages the augmented data
to improve the learnings of discriminator and generator. The experiments show
that DAG improves various GAN models. Furthermore, when DAG is used in some GAN
models, the system establishes state-of-the-art Fr\'echet Inception Distance
(FID) scores.
\\ ( https://arxiv.org/abs/2006.05338 ,  1132kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05353
Date: Tue, 9 Jun 2020 15:35:41 GMT   (7570kb,D)

Title: MeshWalker: Deep Mesh Understanding by Random Walks
Authors: Alon Lahav, Ayellet Tal
Categories: cs.CV cs.CG cs.LG
\\
  Most attempts to represent 3D shapes for deep learning have focused on
volumetric grids, multi-view images and point clouds. In this paper we look at
the most popular representation of 3D shapes in computer graphics - a
triangular mesh - and ask how it can be utilized within deep learning. The few
attempts to answer this question propose to adapt convolutions & pooling to
suit Convolutional Neural Networks (CNNs). This paper proposes a very different
approach, termed MeshWalker, to learn the shape directly from a given mesh. The
key idea is to represent the mesh by random walks along the surface, which
"explore" the mesh's geometry and topology. Each walk is organized as a list of
vertices, which in some manner imposes regularity on the mesh. The walk is fed
into a Recurrent Neural Network (RNN) that "remembers" the history of the walk.
We show that our approach achieves state-of-the-art results for two fundamental
shape analysis tasks: shape classification and semantic segmentation.
Furthermore, even a very small number of examples suffices for learning. This
is highly important, since large datasets of meshes are difficult to acquire.
\\ ( https://arxiv.org/abs/2006.05353 ,  7570kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05355
Date: Tue, 9 Jun 2020 15:39:14 GMT   (8828kb,D)

Title: A Hybrid Framework for Matching Printing Design Files to Product Photos
Authors: Alper Kaplan and Erdem Akagunduz
Categories: cs.CV
Journal-ref: published in Balkan Journal of Electrical and Computer
  Engineering, Volume 8 - Issue 2 - Apr 30, 2020
\\
  We propose a real-time image matching framework, which is hybrid in the sense
that it uses both hand-crafted features and deep features obtained from a
well-tuned deep convolutional network. The matching problem, which we
concentrate on, is specific to a certain application, that is, printing design
to product photo matching. Printing designs are any kind of template image
files, created using a design tool, thus are perfect image signals. However,
photographs of a printed product suffer many unwanted effects, such as
uncontrolled shooting angle, uncontrolled illumination, occlusions, printing
deficiencies in color, camera noise, optic blur, et cetera. For this purpose,
we create an image set that includes printing design and corresponding product
photo pairs with collaboration of an actual printing facility. Using this image
set, we benchmark various hand-crafted and deep features for matching
performance and propose a framework in which deep learning is utilized with
highest contribution, but without disabling real-time operation using an
ordinary desktop computer.
\\ ( https://arxiv.org/abs/2006.05355 ,  8828kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05367
Date: Tue, 9 Jun 2020 16:00:00 GMT   (1514kb,D)

Title: Open-Narrow-Synechiae Anterior Chamber Angle Classification in AS-OCT
  Sequences
Authors: Huaying Hao, Huazhu Fu, Yanwu Xu, Jianlong Yang, Fei Li, Xiulan Zhang,
  Jiang Liu, Yitian Zhao
Categories: cs.CV
Comments: Accepted to MICCAI 2020
\\
  Anterior chamber angle (ACA) classification is a key step in the diagnosis of
angle-closure glaucoma in Anterior Segment Optical Coherence Tomography
(AS-OCT). Existing automated analysis methods focus on a binary classification
system (i.e., open angle or angle-closure) in a 2D AS-OCT slice. However,
clinical diagnosis requires a more discriminating ACA three-class system (i.e.,
open, narrow, or synechiae angles) for the benefit of clinicians who seek
better to understand the progression of the spectrum of angle-closure glaucoma
types. To address this, we propose a novel sequence multi-scale aggregation
deep network (SMA-Net) for open-narrow-synechiae ACA classification based on an
AS-OCT sequence. In our method, a Multi-Scale Discriminative Aggregation (MSDA)
block is utilized to learn the multi-scale representations at slice level,
while a ConvLSTM is introduced to study the temporal dynamics of these
representations at sequence level. Finally, a multi-level loss function is used
to combine the slice-based and sequence-based losses. The proposed method is
evaluated across two AS-OCT datasets. The experimental results show that the
proposed method outperforms existing state-of-the-art methods in applicability,
effectiveness, and accuracy. We believe this work to be the first attempt to
classify ACAs into open, narrow, or synechia types grading using AS-OCT
sequences.
\\ ( https://arxiv.org/abs/2006.05367 ,  1514kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05400
Date: Tue, 9 Jun 2020 16:54:57 GMT   (7461kb,D)

Title: SAL++: Sign Agnostic Learning with Derivatives
Authors: Matan Atzmon and Yaron Lipman
Categories: cs.CV cs.GR cs.LG
\\
  Learning 3D geometry directly from raw data, such as point clouds, triangle
soups, or un-oriented meshes is still a challenging task that feeds many
downstream computer vision and graphics applications. In this paper, we
introduce SAL++: a method for learning implicit neural representations of
shapes directly from such raw data. We build upon the recent sign agnostic
learning (SAL) approach and generalize it to include derivative data in a sign
agnostic manner. In more detail, given the unsigned distance function to the
input raw data, we suggest a novel sign agnostic regression loss, incorporating
both pointwise values and gradients of the unsigned distance function.
Optimizing this loss leads to a signed implicit function solution, the zero
level set of which is a high quality, valid manifold approximation to the input
3D data. We demonstrate the efficacy of SAL++ shape space learning from two
challenging datasets: ShapeNet that contains inconsistent orientation and
non-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups).
On both these datasets, we present state-of-the-art results.
\\ ( https://arxiv.org/abs/2006.05400 ,  7461kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05407
Date: Tue, 9 Jun 2020 17:12:27 GMT   (4748kb,D)

Title: D-VPnet: A Network for Real-time Dominant Vanishing Point Detection in
  Natural Scenes
Authors: Yin-Bo Liu, Ming Zeng, Qing-Hao Meng
Categories: cs.CV eess.IV
Comments: 18 pages, 6 figures, under review
ACM-class: I.4.7
\\
  As an important part of linear perspective, vanishing points (VPs) provide
useful clues for mapping objects from 2D photos to 3D space. Existing methods
are mainly focused on extracting structural features such as lines or contours
and then clustering these features to detect VPs. However, these techniques
suffer from ambiguous information due to the large number of line segments and
contours detected in outdoor environments. In this paper, we present a new
convolutional neural network (CNN) to detect dominant VPs in natural scenes,
i.e., the Dominant Vanishing Point detection Network (D-VPnet). The key
component of our method is the feature line-segment proposal unit (FLPU), which
can be directly utilized to predict the location of the dominant VP. Moreover,
the model also uses the two main parallel lines as an assistant to determine
the position of the dominant VP. The proposed method was tested using a public
dataset and a Parallel Line based Vanishing Point (PLVP) dataset. The
experimental results suggest that the detection accuracy of our approach
outperforms those of state-of-the-art methods under various conditions in
real-time, achieving rates of 115fps.
\\ ( https://arxiv.org/abs/2006.05407 ,  4748kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05456
Date: Tue, 9 Jun 2020 18:53:21 GMT   (280kb,D)

Title: Dialog Policy Learning for Joint Clarification and Active Learning
  Queries
Authors: Aishwarya Padmakumar and Raymond J. Mooney
Categories: cs.CV cs.CL cs.LG
\\
  Intelligent systems need to be able to recover from mistakes, resolve
uncertainty, and adapt to novel concepts not seen during training. Dialog
interaction can enable this by the use of clarifications for correction and
resolving uncertainty, and active learning queries to learn new concepts
encountered during operation. Prior work on dialog systems has either focused
on exclusively learning how to perform clarification/ information seeking, or
to perform active learning. In this work, we train a hierarchical dialog policy
to jointly perform {\it both} clarification and active learning in the context
of an interactive language-based image retrieval task motivated by an on-line
shopping application, and demonstrate that jointly learning dialog policies for
clarification and active learning is more effective than the use of static
dialog policies for one or both of these functions.
\\ ( https://arxiv.org/abs/2006.05456 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05470
Date: Tue, 9 Jun 2020 19:29:49 GMT   (4927kb,D)

Title: Standardised convolutional filtering for radiomics
Authors: Adrien Depeursinge, Vincent Andrearczyk, Philip Whybra, Joost van
  Griethuysen, Henning M\"uller, Roger Schaer, Martin Valli\`eres, Alex
  Zwanenburg (for the Image Biomarker Standardisation Initiative)
Categories: cs.CV eess.IV
Comments: 54 pages. For additional information see https://theibsi.github.io/
MSC-class: 68U10 (Primary) 68T45 (Secondary)
ACM-class: I.4.7; J.3
\\
  The Image Biomarker Standardisation Initiative (IBSI) aims to improve
reproducibility of radiomics studies by standardising the computational process
of extracting image biomarkers (features) from images. We have previously
established reference values for 169 commonly used features, created a standard
radiomics image processing scheme, and developed reporting guidelines for
radiomic studies. However, several aspects are not standardised.
  Here we present a preliminary version of a reference manual on the use of
convolutional image filters in radiomics. Filters, such as wavelets or
Laplacian of Gaussian filters, play an important part in emphasising specific
image characteristics such as edges and blobs. Features derived from filter
response maps have been found to be poorly reproducible. This reference manual
forms the basis of ongoing work on standardising convolutional filters in
radiomics, and will be updated as this work progresses.
\\ ( https://arxiv.org/abs/2006.05470 ,  4927kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05485
Date: Tue, 9 Jun 2020 19:51:34 GMT   (997kb,D)

Title: Off-the-shelf sensor vs. experimental radar -- How much resolution is
  necessary in automotive radar classification?
Authors: Nicolas Scheiner, Ole Schumann, Florian Kraus, Nils Appenrodt,
  J\"urgen Dickmann, Bernhard Sick
Categories: cs.CV cs.LG eess.SP
Comments: Accepted @ 23rd International Conference on Information Fusion
  (FUSION)
\\
  Radar-based road user detection is an important topic in the context of
autonomous driving applications. The resolution of conventional automotive
radar sensors results in a sparse data representation which is tough to refine
during subsequent signal processing. On the other hand, a new sensor generation
is waiting in the wings for its application in this challenging field. In this
article, two sensors of different radar generations are evaluated against each
other. The evaluation criterion is the performance on moving road user object
detection and classification tasks. To this end, two data sets originating from
an off-the-shelf radar and a high resolution next generation radar are
compared. Special attention is given on how the two data sets are assembled in
order to make them comparable. The utilized object detector consists of a
clustering algorithm, a feature extraction module, and a recurrent neural
network ensemble for classification. For the assessment, all components are
evaluated both individually and, for the first time, as a whole. This allows
for indicating where overall performance improvements have their origin in the
pipeline. Furthermore, the generalization capabilities of both data sets are
evaluated and important comparison metrics for automotive radar object
detection are discussed. Results show clear benefits of the next generation
radar. Interestingly, those benefits do not actually occur due to better
performance at the classification stage, but rather because of the vast
improvements at the clustering stage.
\\ ( https://arxiv.org/abs/2006.05485 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05518
Date: Tue, 9 Jun 2020 21:28:17 GMT   (8365kb,D)

Title: MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous
  Driving Using Multiple Views
Authors: Ke Chen, Ryan Oldja, Nikolai Smolyanskiy, Stan Birchfield, Alexander
  Popov, David Wehr, Ibrahim Eden, Joachim Pehserl
Categories: cs.CV cs.RO
Comments: IROS2020 conference submission, for accompanying video, see
  https://youtu.be/2ck5_sToayc
ACM-class: I.2.6; I.4.6; I.5.1
\\
  Autonomous driving requires the inference of actionable information such as
detecting and classifying objects, and determining the drivable space. To this
end, we present a two-stage deep neural network (MVLidarNet) for multi-class
object detection and drivable segmentation using multiple views of a single
LiDAR point cloud. The first stage processes the point cloud projected onto a
perspective view in order to semantically segment the scene. The second stage
then processes the point cloud (along with semantic labels from the first
stage) projected onto a bird's eye view, to detect and classify objects. Both
stages are simple encoder-decoders. We show that our multi-view, multi-stage,
multi-class approach is able to detect and classify objects while
simultaneously determining the drivable space using a single LiDAR scan as
input, in challenging scenes with more than one hundred vehicles and
pedestrians at a time. The system operates efficiently at 150 fps on an
embedded GPU designed for a self-driving car, including a postprocessing step
to maintain identities over time. We show results on both KITTI and a much
larger internal dataset, thus demonstrating the method's ability to scale by an
order of magnitude.
\\ ( https://arxiv.org/abs/2006.05518 ,  8365kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05538
Date: Tue, 9 Jun 2020 22:44:58 GMT   (2248kb,D)

Title: Dual-stream Maximum Self-attention Multi-instance Learning
Authors: Bin Li, Kevin W. Eliceiri
Categories: cs.CV cs.LG
ACM-class: I.2.0
\\
  Multi-instance learning (MIL) is a form of weakly supervised learning where a
single class label is assigned to a bag of instances while the instance-level
labels are not available. Training classifiers to accurately determine the bag
label and instance labels is a challenging but critical task in many practical
scenarios, such as computational histopathology. Recently, MIL models fully
parameterized by neural networks have become popular due to the high
flexibility and superior performance. Most of these models rely on attention
mechanisms that assign attention scores across the instance embeddings in a bag
and produce the bag embedding using an aggregation operator. In this paper, we
proposed a dual-stream maximum self-attention MIL model (DSMIL) parameterized
by neural networks. The first stream deploys a simple MIL max-pooling while the
top-activated instance embedding is determined and used to obtain
self-attention scores across instance embeddings in the second stream.
Different from most of the previous methods, the proposed model jointly learns
an instance classifier and a bag classifier based on the same instance
embeddings. The experiments results show that our method achieves superior
performance compared to the best MIL methods and demonstrates state-of-the-art
performance on benchmark MIL datasets.
\\ ( https://arxiv.org/abs/2006.05538 ,  2248kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05548
Date: Tue, 9 Jun 2020 23:17:24 GMT   (4093kb,D)

Title: 3D Point Cloud Feature Explanations Using Gradient-Based Methods
Authors: Ananya Gupta, Simon Watson, Hujun Yin
Categories: cs.CV
Comments: Accepted for IJCNN 2020
\\
  Explainability is an important factor to drive user trust in the use of
neural networks for tasks with material impact. However, most of the work done
in this area focuses on image analysis and does not take into account 3D data.
We extend the saliency methods that have been shown to work on image data to
deal with 3D data. We analyse the features in point clouds and voxel spaces and
show that edges and corners in 3D data are deemed as important features while
planar surfaces are deemed less important. The approach is model-agnostic and
can provide useful information about learnt features. Driven by the insight
that 3D data is inherently sparse, we visualise the features learnt by a
voxel-based classification network and show that these features are also sparse
and can be pruned relatively easily, leading to more efficient neural networks.
Our results show that the Voxception-ResNet model can be pruned down to 5\% of
its parameters with negligible loss in accuracy.
\\ ( https://arxiv.org/abs/2006.05548 ,  4093kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05560
Date: Tue, 9 Jun 2020 23:50:40 GMT   (7839kb,D)

Title: Tree Annotations in LiDAR Data Using Point Densities and Convolutional
  Neural Networks
Authors: Ananya Gupta, Jonathan Byrne, David Moloney, Simon Watson, Hujun Yin
Categories: cs.CV
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no.
  2, pp. 971-981, Feb. 2020
DOI: 10.1109/TGRS.2019.2942201
\\
  LiDAR provides highly accurate 3D point clouds. However, data needs to be
manually labelled in order to provide subsequent useful information. Manual
annotation of such data is time consuming, tedious and error prone, and hence
in this paper we present three automatic methods for annotating trees in LiDAR
data. The first method requires high density point clouds and uses certain
LiDAR data attributes for the purpose of tree identification, achieving almost
90% accuracy. The second method uses a voxel-based 3D Convolutional Neural
Network on low density LiDAR datasets and is able to identify most large trees
accurately but struggles with smaller ones due to the voxelisation process. The
third method is a scaled version of the PointNet++ method and works directly on
outdoor point clouds and achieves an F_score of 82.1% on the ISPRS benchmark
dataset, comparable to the state-of-the-art methods but with increased
efficiency.
\\ ( https://arxiv.org/abs/2006.05560 ,  7839kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05569
Date: Wed, 10 Jun 2020 00:08:42 GMT   (3279kb,D)

Title: A gaze driven fast-forward method for first-person videos
Authors: Alan Carvalho Neves, Michel Melo Silva, Mario Fernando Montenegro
  Campos, Erickson Rangel Nascimento
Categories: cs.CV
Comments: Accepted for presentation at EPIC@CVPR2020 workshop
\\
  The growing data sharing and life-logging cultures are driving an
unprecedented increase in the amount of unedited First-Person Videos. In this
paper, we address the problem of accessing relevant information in First-Person
Videos by creating an accelerated version of the input video and emphasizing
the important moments to the recorder. Our method is based on an attention
model driven by gaze and visual scene analysis that provides a semantic score
of each frame of the input video. We performed several experimental evaluations
on publicly available First-Person Videos datasets. The results show that our
methodology can fast-forward videos emphasizing moments when the recorder
visually interact with scene components while not including monotonous clips.
\\ ( https://arxiv.org/abs/2006.05569 ,  3279kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05580
Date: Wed, 10 Jun 2020 00:33:18 GMT   (8504kb,D)

Title: Syn2Real Transfer Learning for Image Deraining using Gaussian Processes
Authors: Rajeev Yasarla Vishwanath A. Sindagi Vishal M. Patel
Categories: cs.CV
Comments: Accepted at CVPR 2020
\\
  Recent CNN-based methods for image deraining have achieved excellent
performance in terms of reconstruction error as well as visual quality.
However, these methods are limited in the sense that they can be trained only
on fully labeled data. Due to various challenges in obtaining real world
fully-labeled image deraining datasets, existing methods are trained only on
synthetically generated data and hence, generalize poorly to real-world images.
The use of real-world data in training image deraining networks is relatively
less explored in the literature. We propose a Gaussian Process-based
semi-supervised learning framework which enables the network in learning to
derain using synthetic dataset while generalizing better using unlabeled
real-world images. Through extensive experiments and ablations on several
challenging datasets (such as Rain800, Rain200H and DDN-SIRR), we show that the
proposed method, when trained on limited labeled data, achieves on-par
performance with fully-labeled training. Additionally, we demonstrate that
using unlabeled real-world images in the proposed GP-based framework results in
superior performance as compared to existing methods. Code is available at:
https://github.com/rajeevyasarla/Syn2Real
\\ ( https://arxiv.org/abs/2006.05580 ,  8504kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05589
Date: Wed, 10 Jun 2020 01:06:03 GMT   (5623kb,D)

Title: CNN-Based Semantic Change Detection in Satellite Imagery
Authors: Ananya Gupta, Elisabeth Welburn, Simon Watson, Hujun Yin
Categories: cs.CV
Journal-ref: Proceedings of International Conference on Artificial Neural
  Networks , 2019. pg-669-684
DOI: 10.1007/978-3-030-30493-5_61
\\
  Timely disaster risk management requires accurate road maps and prompt damage
assessment. Currently, this is done by volunteers manually marking satellite
imagery of affected areas but this process is slow and often error-prone.
Segmentation algorithms can be applied to satellite images to detect road
networks. However, existing methods are unsuitable for disaster-struck areas as
they make assumptions about the road network topology which may no longer be
valid in these scenarios. Herein, we propose a CNN-based framework for
identifying accessible roads in post-disaster imagery by detecting changes from
pre-disaster imagery. Graph theory is combined with the CNN output for
detecting semantic changes in road networks with OpenStreetMap data. Our
results are validated with data of a tsunami-affected region in Palu, Indonesia
acquired from DigitalGlobe.
\\ ( https://arxiv.org/abs/2006.05589 ,  5623kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05597
Date: Wed, 10 Jun 2020 01:20:47 GMT   (1399kb,D)

Title: Condensing Two-stage Detection with Automatic Object Key Part Discovery
Authors: Zhe Chen, Jing Zhang, Dacheng Tao
Categories: cs.CV
\\
  Modern two-stage object detectors generally require excessively large models
for their detection heads to achieve high accuracy. To address this problem, we
propose that the model parameters of two-stage detection heads can be condensed
and reduced by concentrating on object key parts. To this end, we first
introduce an automatic object key part discovery task to make neural networks
discover representative sub-parts in each foreground object. With these
discovered key parts, we then decompose the object appearance modeling into a
key part modeling process and a global modeling process for detection. Key part
modeling encodes fine and detailed features from the discovered key parts, and
global modeling encodes rough and holistic object characteristics. In practice,
such decomposition allows us to significantly abridge model parameters without
sacrificing much detection accuracy. Experiments on popular datasets illustrate
that our proposed technique consistently maintains original performance while
waiving around 50% of the model parameters of common two-stage detection heads,
with the performance only deteriorating by 1.5% when waiving around 96% of the
original model parameters. Codes will be shortly released to the public through
GitHub.
\\ ( https://arxiv.org/abs/2006.05597 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05612
Date: Wed, 10 Jun 2020 02:14:08 GMT   (2568kb)

Title: Deep Learning for Change Detection in Remote Sensing Images:
  Comprehensive Review and Meta-Analysis
Authors: Lazhar Khelifi and Max Mignotte
Categories: cs.CV
\\
  Deep learning (DL) algorithms are considered as a methodology of choice for
remote-sensing image analysis over the past few years. Due to its effective
applications, deep learning has also been introduced for automatic change
detection and achieved great success. The present study attempts to provide a
comprehensive review and a meta-analysis of the recent progress in this
subfield. Specifically, we first introduce the fundamentals of deep learning
methods which arefrequently adopted for change detection. Secondly, we present
the details of the meta-analysis conducted to examine the status of change
detection DL studies. Then, we focus on deep learning-based change detection
methodologies for remote sensing images by giving a general overview of the
existing methods. Specifically, these deep learning-based methods were
classified into three groups; fully supervised learning-based methods, fully
unsupervised learning-based methods and transfer learning-based techniques. As
a result of these investigations, promising new directions were identified for
future research. This study will contribute in several ways to our
understanding of deep learning for change detection and will provide a basis
for further research.
\\ ( https://arxiv.org/abs/2006.05612 ,  2568kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05627
Date: Wed, 10 Jun 2020 03:01:59 GMT   (1324kb,D)

Title: A survey on deep hashing for image retrieval
Authors: Xiaopeng Zhang
Categories: cs.CV cs.LG eess.IV
\\
  Hashing has been widely used in approximate nearest search for large-scale
database retrieval for its computation and storage efficiency. Deep hashing,
which devises convolutional neural network architecture to exploit and extract
the semantic information or feature of images, has received increasing
attention recently. In this survey, several deep supervised hashing methods for
image retrieval are evaluated and I conclude three main different directions
for deep supervised hashing methods. Several comments are made at the end.
Moreover, to break through the bottleneck of the existing hashing methods, I
propose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise
a CNN architecture to extract the semantic features of images and design a loss
function to encourage similar images projected close. To this end, I propose a
concept: shadow of the CNN output. During optimization process, the CNN output
and its shadow are guiding each other so as to achieve the optimal solution as
much as possible. Several experiments on dataset CIFAR-10 show the satisfying
performance of SRH.
\\ ( https://arxiv.org/abs/2006.05627 ,  1324kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05646
Date: Wed, 10 Jun 2020 04:12:53 GMT   (3514kb)

Title: Scalable Backdoor Detection in Neural Networks
Authors: Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya,
  Sunil Gupta, and Svetha Venkatesh
Categories: cs.CV
\\
  Recently, it has been shown that deep learning models are vulnerable to
Trojan attacks, where an attacker can install a backdoor during training time
to make the resultant model misidentify samples contaminated with a small
trigger patch. Current backdoor detection methods fail to achieve good
detection performance and are computationally expensive. In this paper, we
propose a novel trigger reverse-engineering based approach whose computational
complexity does not scale with the number of labels, and is based on a measure
that is both interpretable and universal across different network and patch
types. In experiments, we observe that our method achieves a perfect score in
separating Trojaned models from pure models, which is an improvement over the
current state-of-the art method.
\\ ( https://arxiv.org/abs/2006.05646 ,  3514kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05652
Date: Wed, 10 Jun 2020 04:42:07 GMT   (501kb)

Title: Agrupamento de Pixels para o Reconhecimento de Faces
Authors: Tiago Buarque Assun\c{c}\~ao de Carvalho
Categories: cs.CV
Comments: 21 pages, in Portuguese, 5 figures, book chapter, recortado
  (adapatado) da tese
MSC-class: 62H30
ACM-class: I.2.10; I.5.3; I.4.2; I.4.10
\\
  This research starts with the observation that face recognition can suffer a
low impact from significant image shrinkage. To explain this fact, we proposed
the Pixel Clustering methodology. It defines regions in the image in which its
pixels are very similar to each other. We extract features from each region. We
used three face databases in the experiments. We noticed that 512 is the
maximum number of features needed for high accuracy image recognition. The
proposed method is also robust, even if only it uses a few classes from the
training set.
\\ ( https://arxiv.org/abs/2006.05652 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05674
Date: Sat, 23 May 2020 15:40:53 GMT   (21kb)

Title: 3D geometric moment invariants from the point of view of the classical
  invariant theory
Authors: Leonid Bedratyuk
Categories: cs.CV eess.IV
Comments: 19 pages
\\
  The aim of this paper is to clear up the problem of the connection between
the 3D geometric moments invariants and the invariant theory, considering a
problem of describing of the 3D geometric moments invariants as a problem of
the classical invariant theory. Using the remarkable fact that the groups
$SO(3)$ and $SL(2)$ are locally isomorphic, we reduced the problem of deriving
3D geometric moments invariants to the well-known problem of the classical
invariant theory. We give a precise statement of the 3D geometric invariant
moments computation, introducing the notions of the algebras of simultaneous 3D
geometric moment invariants, and prove that they are isomorphic to the algebras
of joint $SL(2)$-invariants of several binary forms. To simplify the
calculating of the invariants we proceed from an action of Lie group $SO(3)$ to
an action of its Lie algebra $\mathfrak{sl}_2$. The author hopes that the
results will be useful to the researchers in the fields of image analysis and
pattern recognition.
\\ ( https://arxiv.org/abs/2006.05674 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05675
Date: Fri, 29 May 2020 21:50:38 GMT   (4656kb,D)

Title: IMUTube: Automatic extraction of virtual on-body accelerometry from
  video for human activity recognition
Authors: Hyeokhyen Kwon, Catherine Tong, Harish Haresamudram, Yan Gao, Gregory
  D. Abowd, Nicholas D. Lane, Thomas Ploetz
Categories: cs.CV eess.IV
\\
  The lack of large-scale, labeled data sets impedes progress in developing
robust and generalized predictive models for on-body sensor-based human
activity recognition (HAR). Labeled data in human activity recognition is
scarce and hard to come by, as sensor data collection is expensive, and the
annotation is time-consuming and error-prone. To address this problem, we
introduce IMUTube, an automated processing pipeline that integrates existing
computer vision and signal processing techniques to convert videos of human
activity into virtual streams of IMU data. These virtual IMU streams represent
accelerometry at a wide variety of locations on the human body. We show how the
virtually-generated IMU data improves the performance of a variety of models on
known HAR datasets. Our initial results are very promising, but the greater
promise of this work lies in a collective approach by the computer vision,
signal processing, and activity recognition communities to extend this work in
ways that we outline. This should lead to on-body, sensor-based HAR becoming
yet another success story in large-dataset breakthroughs in recognition.
\\ ( https://arxiv.org/abs/2006.05675 ,  4656kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05682
Date: Wed, 10 Jun 2020 06:44:53 GMT   (6633kb,D)

Title: H3DNet: 3D Object Detection Using Hybrid Geometric Primitives
Authors: Zaiwei Zhang, Bo Sun, Haitao Yang, Qixing Huang
Categories: cs.CV
\\
  We introduce H3DNet, which takes a colorless 3D point cloud as input and
outputs a collection of oriented object bounding boxes (or BB) and their
semantic labels. The critical idea of H3DNet is to predict a hybrid set of
geometric primitives, i.e., BB centers, BB face centers, and BB edge centers.
We show how to convert the predicted geometric primitives into object proposals
by defining a distance function between an object and the geometric primitives.
This distance function enables continuous optimization of object proposals, and
its local minimums provide high-fidelity object proposals. H3DNet then utilizes
a matching and refinement module to classify object proposals into detected
objects and fine-tune the geometric parameters of the detected objects. The
hybrid set of geometric primitives not only provides more accurate signals for
object detection than using a single type of geometric primitives, but it also
provides an overcomplete set of constraints on the resulting 3D layout.
Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our
model achieves state-of-the-art 3D detection results on two large datasets with
real 3D scans, ScanNet and SUN RGB-D.
\\ ( https://arxiv.org/abs/2006.05682 ,  6633kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05683
Date: Wed, 10 Jun 2020 06:45:05 GMT   (1924kb,D)

Title: TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training
  Model
Authors: Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, Cewu Lu
Categories: cs.CV
Comments: CVPR-2020 oral paper
\\
  Multi-object tracking is a fundamental vision problem that has been studied
for a long time. As deep learning brings excellent performances to object
detection algorithms, Tracking by Detection (TBD) has become the mainstream
tracking framework. Despite the success of TBD, this two-step method is too
complicated to train in an end-to-end manner and induces many challenges as
well, such as insufficient exploration of video spatial-temporal information,
vulnerability when facing object occlusion, and excessive reliance on detection
results. To address these challenges, we propose a concise end-to-end model
TubeTK which only needs one step training by introducing the ``bounding-tube"
to indicate temporal-spatial locations of objects in a short video clip. TubeTK
provides a novel direction of multi-object tracking, and we demonstrate its
potential to solve the above challenges without bells and whistles. We analyze
the performance of TubeTK on several MOT benchmarks and provide empirical
evidence to show that TubeTK has the ability to overcome occlusions to some
extent without any ancillary technologies like Re-ID. Compared with other
methods that adopt private detection results, our one-stage end-to-end model
achieves state-of-the-art performances even if it adopts no ready-made
detection results. We hope that the proposed TubeTK model can serve as a simple
but strong alternative for video-based MOT task. The code and models are
available at https://github.com/BoPang1996/TubeTK.
\\ ( https://arxiv.org/abs/2006.05683 ,  1924kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05698
Date: Wed, 10 Jun 2020 07:28:06 GMT   (16193kb,D)

Title: Rendering Natural Camera Bokeh Effect with Deep Learning
Authors: Andrey Ignatov, Jagruti Patel, Radu Timofte
Categories: cs.CV cs.LG eess.IV
\\
  Bokeh is an important artistic effect used to highlight the main object of
interest on the photo by blurring all out-of-focus areas. While DSLR and system
camera lenses can render this effect naturally, mobile cameras are unable to
produce shallow depth-of-field photos due to a very small aperture diameter of
their optics. Unlike the current solutions simulating bokeh by applying
Gaussian blur to image background, in this paper we propose to learn a
realistic shallow focus technique directly from the photos produced by DSLR
cameras. For this, we present a large-scale bokeh dataset consisting of 5K
shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR with
50mm f/1.8 lenses. We use these images to train a deep learning model to
reproduce a natural bokeh effect based on a single narrow-aperture image. The
experimental results show that the proposed approach is able to render a
plausible non-uniform bokeh even in case of complex input data with multiple
objects. The dataset, pre-trained models and codes used in this paper are
available on the project website.
\\ ( https://arxiv.org/abs/2006.05698 ,  16193kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05700
Date: Wed, 10 Jun 2020 07:37:29 GMT   (5506kb,D)

Title: Delta Descriptors: Change-Based Place Representation for Robust Visual
  Localization
Authors: Sourav Garg, Ben Harwood, Gaurangi Anand and Michael Milford
Categories: cs.CV cs.IR cs.LG cs.RO
Comments: 8 pages and 7 figures. To be published in 2020 IEEE Robotics and
  Automation Letters (RA-L)
\\
  Visual place recognition is challenging because there are so many factors
that can cause the appearance of a place to change, from day-night cycles to
seasonal change to atmospheric conditions. In recent years a large range of
approaches have been developed to address this challenge including deep-learnt
image descriptors, domain translation, and sequential filtering, all with
shortcomings including generality and velocity-sensitivity. In this paper we
propose a novel descriptor derived from tracking changes in any learned global
descriptor over time, dubbed Delta Descriptors. Delta Descriptors mitigate the
offsets induced in the original descriptor matching space in an unsupervised
manner by considering temporal differences across places observed along a
route. Like all other approaches, Delta Descriptors have a shortcoming -
volatility on a frame to frame basis - which can be overcome by combining them
with sequential filtering methods. Using two benchmark datasets, we first
demonstrate the high performance of Delta Descriptors in isolation, before
showing new state-of-the-art performance when combined with sequence-based
matching. We also present results demonstrating the approach working with a
second different underlying descriptor type, and two other beneficial
properties of Delta Descriptors in comparison to existing techniques: their
increased inherent robustness to variations in camera motion and a reduced rate
of performance degradation as dimensional reduction is applied. Source code
will be released upon publication.
\\ ( https://arxiv.org/abs/2006.05700 ,  5506kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05713
Date: Wed, 10 Jun 2020 08:08:26 GMT   (1051kb,D)

Title: Unique Faces Recognition in Videos
Authors: Jiahao Huo and Terence L van Zyl
Categories: cs.CV
Comments: Paper was accepted into Fusion 2020 conference but will only be
  published after the virtual conference in July 2020. 7 pages long
\\
  This paper tackles face recognition in videos employing metric learning
methods and similarity ranking models. The paper compares the use of the
Siamese network with contrastive loss and Triplet Network with triplet loss
implementing the following architectures: Google/Inception architecture, 3D
Convolutional Network (C3D), and a 2-D Long short-term memory (LSTM) Recurrent
Neural Network. We make use of still images and sequences from videos for
training the networks and compare the performances implementing the above
architectures. The dataset used was the YouTube Face Database designed for
investigating the problem of face recognition in videos. The contribution of
this paper is two-fold: to begin, the experiments have established 3-D
Convolutional networks and 2-D LSTMs with the contrastive loss on image
sequences do not outperform Google/Inception architecture with contrastive loss
in top $n$ rank face retrievals with still images. However, the 3-D Convolution
networks and 2-D LSTM with triplet Loss outperform the Google/Inception with
triplet loss in top $n$ rank face retrievals on the dataset; second, a Support
Vector Machine (SVM) was used in conjunction with the CNNs' learned feature
representations for facial identification. The results show that feature
representation learned with triplet loss is significantly better for n-shot
facial identification compared to contrastive loss. The most useful feature
representations for facial identification are from the 2-D LSTM with triplet
loss. The experiments show that learning spatio-temporal features from video
sequences is beneficial for facial recognition in videos.
\\ ( https://arxiv.org/abs/2006.05713 ,  1051kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05724
Date: Wed, 10 Jun 2020 08:30:20 GMT   (5934kb,D)

Title: Real-time single image depth perception in the wild with handheld
  devices
Authors: Filippo Aleotti, Giulio Zaccaroni, Luca Bartolomei, Matteo Poggi,
  Fabio Tosi, Stefano Mattoccia
Categories: cs.CV cs.GR
Comments: 11 pages, 9 figures
\\
  Depth perception is paramount to tackle real-world problems, ranging from
autonomous driving to consumer applications. For the latter, depth estimation
from a single image represents the most versatile solution, since a standard
camera is available on almost any handheld device. Nonetheless, two main issues
limit its practical deployment: i) the low reliability when deployed
in-the-wild and ii) the demanding resource requirements to achieve real-time
performance, often not compatible with such devices. Therefore, in this paper,
we deeply investigate these issues showing how they are both addressable
adopting appropriate network design and training strategies -- also outlining
how to map the resulting networks on handheld devices to achieve real-time
performance. Our thorough evaluation highlights the ability of such fast
networks to generalize well to new environments, a crucial feature required to
tackle the extremely varied contexts faced in real applications. Indeed, to
further support this evidence, we report experimental results concerning
real-time depth-aware augmented reality and image blurring with smartphones
in-the-wild.
\\ ( https://arxiv.org/abs/2006.05724 ,  5934kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05726
Date: Wed, 10 Jun 2020 08:32:56 GMT   (1572kb,D)

Title: Estimating semantic structure for the VQA answer space
Authors: Corentin Kervadec (imagine), Grigory Antipov, Moez Baccouche,
  Christian Wolf (imagine)
Categories: cs.CV cs.CL
\\
  Since its appearance, Visual Question Answering (VQA, i.e. answering a
question posed over an image), has always been treated as a classification
problem over a set of predefined answers. Despite its convenience, this
classification approach poorly reflects the semantics of the problem limiting
the answering to a choice between independent proposals, without taking into
account the similarity between them (e.g. equally penalizing for answering cat
or German shepherd instead of dog). We address this issue by proposing (1) two
measures of proximity between VQA classes, and (2) a corresponding loss which
takes into account the estimated proximity. This significantly improves the
generalization of VQA models by reducing their language bias. In particular, we
show that our approach is completely model-agnostic since it allows consistent
improvements with three different VQA models. Finally, by combining our method
with a language bias reduction approach, we report SOTA-level performance on
the challenging VQAv2-CP dataset.
\\ ( https://arxiv.org/abs/2006.05726 ,  1572kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05728
Date: Wed, 10 Jun 2020 08:35:29 GMT   (1776kb,D)

Title: Diagnosing Rarity in Human-Object Interaction Detection
Authors: Mert Kilickaya and Arnold Smeulders
Categories: cs.CV
Comments: Accepted at CVPR'20 Workshop on Learning from Limited Labels
\\
  Human-object interaction (HOI) detection is a core task in computer vision.
The goal is to localize all human-object pairs and recognize their
interactions. An interaction defined by a <verb, noun> tuple leads to a
long-tailed visual recognition challenge since many combinations are rarely
represented. The performance of the proposed models is limited especially for
the tail categories, but little has been done to understand the reason. To that
end, in this paper, we propose to diagnose rarity in HOI detection. We propose
a three-step strategy, namely Detection, Identification and Recognition where
we carefully analyse the limiting factors by studying state-of-the-art models.
Our findings indicate that detection and identification steps are altered by
the interaction signals like occlusion and relative location, as a result
limiting the recognition accuracy.
\\ ( https://arxiv.org/abs/2006.05728 ,  1776kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05732
Date: Wed, 10 Jun 2020 08:43:40 GMT   (991kb,D)

Title: Object Detection in the DCT Domain: is Luminance the Solution?
Authors: Benjamin Deguerre, Clement Chatelain, Gilles Gasso
Categories: cs.CV cs.LG
\\
  Object detection in images has reached unprecedented performances. The
state-of-the-art methods rely on deep architectures that extract salient
features and predict bounding boxes enclosing the objects of interest. These
methods essentially run on RGB images. However, the RGB images are often
compressed by the acquisition devices for storage purpose and transfer
efficiency. Hence, their decompression is required for object detectors. To
gain in efficiency, this paper proposes to take advantage of the compressed
representation of images to carry out object detection usable in constrained
resources conditions.
  Specifically, we focus on JPEG images and propose a thorough analysis of
detection architectures newly designed in regard of the peculiarities of the
JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard
RGB-based architecture, while only reducing the detection performance by 5.5%.
Additionally, our empirical findings demonstrate that only part of the
compressed JPEG information, namely the luminance component, may be required to
match detection accuracy of the full input methods.
\\ ( https://arxiv.org/abs/2006.05732 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05734
Date: Wed, 10 Jun 2020 08:50:53 GMT   (6697kb,D)

Title: 3D Human Mesh Regression with Dense Correspondence
Authors: Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, Xiaogang Wang
Categories: cs.CV
Comments: To appear at CVPR 2020
\\
  Estimating 3D mesh of the human body from a single 2D image is an important
task with many applications such as augmented reality and Human-Robot
interaction. However, prior works reconstructed 3D mesh from global image
feature extracted by using convolutional neural network (CNN), where the dense
correspondences between the mesh surface and the image pixels are missing,
leading to suboptimal solution. This paper proposes a model-free 3D human mesh
estimation framework, named DecoMR, which explicitly establishes the dense
correspondence between the mesh and the local image features in the UV space
(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts
pixel-to-surface dense correspondence map (i.e., IUV image), with which we
transfer local features from the image space to the UV space. Then the
transferred local image features are processed in the UV space to regress a
location map, which is well aligned with transferred features. Finally we
reconstruct 3D human mesh from the regressed location map with a predefined
mapping function. We also observe that the existing discontinuous UV map are
unfriendly to the learning of network. Therefore, we propose a novel UV map
that maintains most of the neighboring relations on the original mesh surface.
Experiments demonstrate that our proposed local feature alignment and
continuous UV map outperforms existing 3D mesh based methods on multiple public
benchmarks. Code will be made available at
https://github.com/zengwang430521/DecoMR
\\ ( https://arxiv.org/abs/2006.05734 ,  6697kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05787
Date: Wed, 10 Jun 2020 11:57:56 GMT   (1083kb,D)

Title: Image Enhancement and Object Recognition for Night Vision Surveillance
Authors: Aashish Bhandari, Aayush Kafle, Pranjal Dhakal, Prateek Raj Joshi,
  Dinesh Baniya Kshatri
Categories: cs.CV eess.IV eess.SP
Comments: International Conference on Recent Trends in Computational
  Engineering and Technologies, 2018
\\
  Object recognition is a critical part of any surveillance system. It is the
matter of utmost concern to identify intruders and foreign objects in the area
where surveillance is done. The performance of surveillance system using the
traditional camera in daylight is vastly superior as compared to night. The
main problem for surveillance during the night is the objects captured by
traditional cameras have low contrast against the background because of the
absence of ambient light in the visible spectrum. Due to that reason, the image
is taken in low light condition using an Infrared Camera and the image is
enhanced to obtain an image with higher contrast using different enhancing
algorithms based on the spatial domain. The enhanced image is then sent to the
classification process. The classification is done by using convolutional
neural network followed by a fully connected layer of neurons. The accuracy of
classification after implementing different enhancement algorithms is compared
in this paper.
\\ ( https://arxiv.org/abs/2006.05787 ,  1083kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05798
Date: Wed, 10 Jun 2020 12:37:39 GMT   (5816kb,D)

Title: Embedding Task Knowledge into 3D Neural Networks via Self-supervised
  Learning
Authors: Jiuwen Zhu, Yuexiang Li, Yifan Hu, S. Kevin Zhou
Categories: cs.CV
\\
  Deep learning highly relies on the amount of annotated data. However,
annotating medical images is extremely laborious and expensive. To this end,
self-supervised learning (SSL), as a potential solution for deficient annotated
data, attracts increasing attentions from the community. However, SSL
approaches often design a proxy task that is not necessarily related to target
task. In this paper, we propose a novel SSL approach for 3D medical image
classification, namely Task-related Contrastive Prediction Coding (TCPC), which
embeds task knowledge into training 3D neural networks. The proposed TCPC first
locates the initial candidate lesions via supervoxel estimation using simple
linear iterative clustering. Then, we extract features from the sub-volume
cropped around potential lesion areas, and construct a calibrated contrastive
predictive coding scheme for self-supervised learning. Extensive experiments
are conducted on public and private datasets. The experimental results
demonstrate the effectiveness of embedding lesion-related prior-knowledge into
neural networks for 3D medical image classification.
\\ ( https://arxiv.org/abs/2006.05798 ,  5816kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05847
Date: Wed, 10 Jun 2020 14:24:06 GMT   (66kb,D)

Title: Searching Learning Strategy with Reinforcement Learning for 3D Medical
  Image Segmentation
Authors: Dong Yang, Holger Roth, Ziyue Xu, Fausto Milletari, Ling Zhang,
  Daguang Xu
Categories: cs.CV
Comments: 9 pages, 1 figures
Journal-ref: Published at MICCAI 2019
\\
  Deep neural network (DNN) based approaches have been widely investigated and
deployed in medical image analysis. For example, fully convolutional neural
networks (FCN) achieve the state-of-the-art performance in several applications
of 2D/3D medical image segmentation. Even the baseline neural network models
(U-Net, V-Net, etc.) have been proven to be very effective and efficient when
the training process is set up properly. Nevertheless, to fully exploit the
potentials of neural networks, we propose an automated searching approach for
the optimal training strategy with reinforcement learning. The proposed
approach can be utilized for tuning hyper-parameters, and selecting necessary
data augmentation with certain probabilities. The proposed approach is
validated on several tasks of 3D medical image segmentation. The performance of
the baseline model is boosted after searching, and it can achieve comparable
accuracy to other manually-tuned state-of-the-art segmentation approaches.
\\ ( https://arxiv.org/abs/2006.05847 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05848
Date: Wed, 10 Jun 2020 14:24:10 GMT   (8663kb,D)

Title: Geometry-Aware Segmentation of Remote Sensing Images via implicit height
  estimation
Authors: Xiang Li, Yi Fang
Categories: cs.CV
Comments: 13 pages, 10 figures
\\
  Convolutional neural networks have made significant breakthroughs in the
field of remote sensing and greatly advanced the performance of the semantic
segmentation of remote sensing images. Recent studies have shown the benefits
of using additional elevation data (e.g., DSM) for enhancing the performance of
the semantic segmentation of aerial images. However, previous methods mostly
adopt 3D elevation information as additional inputs. While in many real-world
applications, one does not have the corresponding DSM information at hand and
the spatial resolution of acquired DSM images usually do not match the aerial
images. To alleviate this data constraint and also take the advantage of 3D
elevation information, in this paper, we propose a geometry-aware segmentation
model that achieves accurate semantic segmentation of aerial images via
implicit height estimation. Instead of using a single-stream encoder-decoder
network for semantic labeling, we design a separate decoder branch to predict
the height map and use the DSM images as side supervision to train this newly
designed decoder branch. With the newly designed decoder branch, our model can
distill the 3D geometric features from 2D appearance features under the
supervision of ground truth DSM images. Moreover, we develop a new
geometry-aware convolution module that fuses the 3D geometric features from the
height decoder branch and the 2D contextual features from the semantic
segmentation branch. The fused feature embeddings can produce geometry-aware
segmentation maps with enhanced performance. Experiments on ISPRS Vaihingen and
Potsdam datasets demonstrate the effectiveness of our proposed method for the
semantic segmentation of aerial images. Our proposed model achieves remarkable
performance on both datasets without using any hand-crafted features or
post-processing.
\\ ( https://arxiv.org/abs/2006.05848 ,  8663kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05873
Date: Wed, 10 Jun 2020 14:57:58 GMT   (3990kb,D)

Title: WasteNet: Waste Classification at the Edge for Smart Bins
Authors: Gary White, Christian Cabrera, Andrei Palade, Fan Li, Siobhan Clarke
Categories: cs.CV cs.CY
Comments: 8 pages, 9 figures
\\
  Smart Bins have become popular in smart cities and campuses around the world.
These bins have a compaction mechanism that increases the bins' capacity as
well as automated real-time collection notifications. In this paper, we propose
WasteNet, a waste classification model based on convolutional neural networks
that can be deployed on a low power device at the edge of the network, such as
a Jetson Nano. The problem of segregating waste is a big challenge for many
countries around the world. Automated waste classification at the edge allows
for fast intelligent decisions in smart bins without needing access to the
cloud. Waste is classified into six categories: paper, cardboard, glass, metal,
plastic and other. Our model achieves a 97\% prediction accuracy on the test
dataset. This level of classification accuracy will help to alleviate some
common smart bin problems, such as recycling contamination, where different
types of waste become mixed with recycling waste causing the bin to be
contaminated. It also makes the bins more user friendly as citizens do not have
to worry about disposing their rubbish in the correct bin as the smart bin will
be able to make the decision for them.
\\ ( https://arxiv.org/abs/2006.05873 ,  3990kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05888
Date: Wed, 10 Jun 2020 15:19:31 GMT   (5550kb,D)

Title: Speech Fusion to Face: Bridging the Gap Between Human's Vocal
  Characteristics and Facial Imaging
Authors: Yeqi Bai, Tao Ma, Lipo Wang, Zhenjie Zhang
Categories: cs.CV
\\
  While deep learning technologies are now capable of generating realistic
images confusing humans, the research efforts are turning to the synthesis of
images for more concrete and application-specific purposes. Facial image
generation based on vocal characteristics from speech is one of such important
yet challenging tasks. It is the key enabler to influential use cases of image
generation, especially for business in public security and entertainment.
Existing solutions to the problem of speech2face renders limited image quality
and fails to preserve facial similarity due to the lack of quality dataset for
training and appropriate integration of vocal features. In this paper, we
investigate these key technical challenges and propose Speech Fusion to Face,
or SF2F in short, attempting to address the issue of facial image quality and
the poor connection between vocal feature domain and modern image generation
models. By adopting new strategies on data model and training, we demonstrate
dramatic performance boost over state-of-the-art solution, by doubling the
recall of individual identity, and lifting the quality score from 15 to 19
based on the mutual information score with VGGFace classifier.
\\ ( https://arxiv.org/abs/2006.05888 ,  5550kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05895
Date: Wed, 10 Jun 2020 15:29:20 GMT   (4544kb,D)

Title: DisCont: Self-Supervised Visual Attribute Disentanglement using Context
  Vectors
Authors: Sarthak Bhagat, Vishaal Udandarao, Shagun Uppal
Categories: cs.CV
Comments: 10 pages, 6 figures, 4 tables
\\
  Disentangling the underlying feature attributes within an image with no prior
supervision is a challenging task. Models that can disentangle attributes well
provide greater interpretability and control. In this paper, we propose a
self-supervised framework DisCont to disentangle multiple attributes by
exploiting the structural inductive biases within images. Motivated by the
recent surge in contrastive learning paradigms, our model bridges the gap
between self-supervised contrastive learning algorithms and unsupervised
disentanglement. We evaluate the efficacy of our approach, both qualitatively
and quantitatively, on four benchmark datasets.
\\ ( https://arxiv.org/abs/2006.05895 ,  4544kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05918
Date: Wed, 10 Jun 2020 16:12:00 GMT   (3529kb,D)

Title: Deep Learning with Attention Mechanism for Predicting Driver Intention
  at Intersection
Authors: Abenezer Girma, Seifemichael Amsalu, Abrham Workineh, Mubbashar Khan,
  Abdollah Homaifar
Categories: cs.CV cs.AI cs.HC cs.NE
Comments: IEEE Intelligent Vehicles Symposium 2020 (IEEE IV 2020)
\\
  In this paper, a driver's intention prediction near a road intersection is
proposed. Our approach uses a deep bidirectional Long Short-Term Memory (LSTM)
with an attention mechanism model based on a hybrid-state system (HSS)
framework. As intersection is considered to be as one of the major source of
road accidents, predicting a driver's intention at an intersection is very
crucial. Our method uses a sequence to sequence modeling with an attention
mechanism to effectively exploit temporal information out of the time-series
vehicular data including velocity and yaw-rate. The model then predicts ahead
of time whether the target vehicle/driver will go straight, stop, or take right
or left turn. The performance of the proposed approach is evaluated on a
naturalistic driving dataset and results show that our method achieves high
accuracy as well as outperforms other methods. The proposed solution is
promising to be applied in advanced driver assistance systems (ADAS) and as
part of active safety system of autonomous vehicles.
\\ ( https://arxiv.org/abs/2006.05918 ,  3529kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05926
Date: Wed, 10 Jun 2020 16:21:17 GMT   (5002kb,D)

Title: Separable Four Points Fundamental Matrix
Authors: Gil Ben-Artzi
Categories: cs.CV
\\
  We present an approach for the computation of the fundamental matrix based on
epipolar homography decomposition. We analyze the geometrical meaning of the
decomposition-based representation and show that it guarantees a minimal number
of RANSAC samples, on the condition that four correspondences are on an image
line. Experiments on real-world image pairs show that our approach successfully
recovers such four correspondences, provides accurate results and requires a
very small number of RANSAC iterations.
\\ ( https://arxiv.org/abs/2006.05926 ,  5002kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05927
Date: Wed, 10 Jun 2020 16:25:28 GMT   (6054kb,D)

Title: Recent Advances in 3D Object and Hand Pose Estimation
Authors: Vincent Lepetit
Categories: cs.CV
\\
  3D object and hand pose estimation have huge potentials for Augmented
Reality, to enable tangible interfaces, natural interfaces, and blurring the
boundaries between the real and virtual worlds. In this chapter, we present the
recent developments for 3D object and hand pose estimation using cameras, and
discuss their abilities and limitations and the possible future development of
the field.
\\ ( https://arxiv.org/abs/2006.05927 ,  6054kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05929
Date: Wed, 10 Jun 2020 16:30:52 GMT   (2520kb,D)

Title: Dataset Condensation with Gradient Matching
Authors: Bo Zhao, Konda Reddy Mopuri, Hakan Bilen
Categories: cs.CV cs.LG
\\
  Efficient training of deep neural networks is an increasingly important
problem in the era of sophisticated architectures and large-scale datasets.
This paper proposes a training set synthesis technique, called Dataset
Condensation, that learns to produce a small set of informative samples for
training deep neural networks from scratch in a small fraction of the required
computational cost on the original data while achieving comparable results. We
rigorously evaluate its performance in several computer vision benchmarks and
show that it significantly outperforms the state-of-the-art methods. Finally we
show promising applications of our method in continual learning and domain
adaptation.
\\ ( https://arxiv.org/abs/2006.05929 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05938
Date: Wed, 10 Jun 2020 16:46:12 GMT   (3994kb,D)

Title: Simple and effective localized attribute representations for zero-shot
  learning
Authors: Shiqi Yang, Kai Wang, Luis Herranz, Joost van de Weijer
Categories: cs.CV
\\
  Zero-shot learning (ZSL) aims to discriminate images from unseen classes by
exploiting relations to seen classes via their semantic descriptions. Some
recent papers have shown the importance of localized features together with
fine-tuning the feature extractor to obtain discriminative and transferable
features. However, these methods require complex attention or part detection
modules to perform explicit localization in the visual space. In contrast, in
this paper we propose localizing representations in the semantic/attribute
space, with a simple but effective pipeline where localization is implicit.
Focusing on attribute representations, we show that our method obtains
state-of-the-art performance on CUB and SUN datasets, and also achieves
competitive results on AWA2 dataset, outperforming generally more complex
methods with explicit localization in the visual space. Our method can be
implemented easily, which can be used as a new baseline for zero shot learning.
\\ ( https://arxiv.org/abs/2006.05938 ,  3994kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05941
Date: Wed, 10 Jun 2020 16:47:56 GMT   (997kb,D)

Title: MultiResolution Attention Extractor for Small Object Detection
Authors: Fan Zhang, Licheng Jiao, Lingling Li, Fang Liu, and Xu Liu
Categories: cs.CV
Comments: 11 pages, 5 figures
\\
  Small objects are difficult to detect because of their low resolution and
small size. The existing small object detection methods mainly focus on data
preprocessing or narrowing the differences between large and small objects.
Inspired by human vision "attention" mechanism, we exploit two feature
extraction methods to mine the most useful information of small objects. Both
methods are based on multiresolution feature extraction. We initially design
and explore the soft attention method, but we find that its convergence speed
is slow. Then we present the second method, an attention-based feature
interaction method, called a MultiResolution Attention Extractor (MRAE),
showing significant improvement as a generic feature extractor in small object
detection. After each building block in the vanilla feature extractor, we
append a small network to generate attention weights followed by a weighted-sum
operation to get the final attention maps. Our attention-based feature
extractor is 2.0 times the AP of the "hard" attention counterpart (plain
architecture) on the COCO small object detection benchmark, proving that MRAE
can capture useful location and contextual information through adaptive
learning.
\\ ( https://arxiv.org/abs/2006.05941 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04933
Date: Mon, 8 Jun 2020 20:58:23 GMT   (284kb,D)

Title: A New Integer Programming Formulation of the Graphical Traveling
  Salesman Problem
Authors: Robert D. Carr, Neil Simonetti
Categories: cs.DM cs.DS
Comments: 19 pages, only one figure from an external image
ACM-class: G.2.2
\\
  In the Traveling Salesman Problem (TSP), a salesman wants to visit a set of
cities and return home. There is a cost $c_{ij}$ of traveling from city $i$ to
city $j$, which is the same in either direction for the Symmetric TSP. The
objective is to visit each city exactly once, minimizing total travel costs. In
the Graphical TSP, a city may be visited more than once, which may be necessary
on a sparse graph. We present a new integer programming formulation for the
Graphical TSP requiring only two classes of constraints that are either
polynomial in number or polynomially separable, while addressing an open
question proposed by Denis Naddef.
\\ ( https://arxiv.org/abs/2006.04933 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05657
Date: Wed, 10 Jun 2020 05:18:13 GMT   (2765kb,D)

Title: Methodology for Realizing VMM with Binary RRAM Arrays: Experimental
  Demonstration of Binarized-ADALINE Using OxRAM Crossbar
Authors: Sandeep Kaur Kingra, Vivek Parmar, Shubham Negi, Sufyan Khan, Boris
  Hudec, Tuo-Hung Hou and Manan Suri
Categories: cs.ET cs.NE
Comments: Accepted for presentation at the IEEE International Symposium on
  Circuits and Systems (ISCAS) 2020
\\
  In this paper, we present an efficient hardware mapping methodology for
realizing vector matrix multiplication (VMM) on resistive memory (RRAM) arrays.
Using the proposed VMM computation technique, we experimentally demonstrate a
binarized-ADALINE (Adaptive Linear) classifier on an OxRAM crossbar. An 8x8
OxRAM crossbar with Ni/3-nm HfO2/7 nm Al-doped-TiO2/TiN device stack is used.
Weight training for the binarized-ADALINE classifier is performed ex-situ on
UCI cancer dataset. Post weight generation the OxRAM array is carefully
programmed to binary weight-states using the proposed weight mapping technique
on a custom-built testbench. Our VMM powered binarized-ADALINE network achieves
a classification accuracy of 78% in simulation and 67% in experiments.
Experimental accuracy was found to drop mainly due to crossbar inherent
sneak-path issues and RRAM device programming variability.
\\ ( https://arxiv.org/abs/2006.05657 ,  2765kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05743
Date: Wed, 10 Jun 2020 09:19:48 GMT   (1284kb,D)

Title: Towards 3D Dance Motion Synthesis and Control
Authors: Wenlin Zhuang, Yangang Wang, Joseph Robinson, Congyi Wang, Ming Shao,
  Yun Fu, Siyu Xia
Categories: cs.GR
Comments: 9 pages
\\
  3D human dance motion is a cooperative and elegant social movement. Unlike
regular simple locomotion, it is challenging to synthesize artistic dance
motions due to the irregularity, kinematic complexity and diversity. It
requires the synthesized dance is realistic, diverse and controllable. In this
paper, we propose a novel generative motion model based on temporal convolution
and LSTM,TC-LSTM, to synthesize realistic and diverse dance motion. We
introduce a unique control signal, dance melody line, to heighten
controllability. Hence, our model, and its switch for control signals, promote
a variety of applications: random dance synthesis, music-to-dance, user
control, and more. Our experiments demonstrate that our model can synthesize
artistic dance motion in various dance types. Compared with existing methods,
our method achieved start-of-the-art results.
\\ ( https://arxiv.org/abs/2006.05743 ,  1284kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05921
Date: Wed, 10 Jun 2020 16:15:16 GMT   (7594kb,D)

Title: Computational Design and Evaluation Methods for Empowering Non-Experts
  in Digital Fabrication
Authors: Nurcan Gecer Ulu
Categories: cs.GR cs.CE
Comments: PhD Thesis, Carnegie Mellon University
\\
  Despite the increasing availability of personal fabrication hardware and
services, the true potential of digital fabrication remains unrealized due to
lack of computational techniques that can support 3D shape design by
non-experts. This work develops computational methods that address two key
aspects of content creation:(1) Function-driven design synthesis, (2) Design
assessment.
  For design synthesis, a generative shape modeling algorithm that facilitates
automatic geometry synthesis and user-driven modification for non-experts is
introduced. A critical observation that arises from this study is that the most
geometrical specifications are dictated by functional requirements. To support
design by high-level functional prescriptions, a physics based shape
optimization method for compliant coupling behavior design has been developed.
In line with this idea, producing complex 3D surfaces from flat 2D sheets by
exploiting the concept of buckling beams has also been explored. Effective
design assessment, the second key aspect, becomes critical for problems in
which computational solutions do not exist. For these problems, this work
proposes crowdsourcing as a way to empower non-experts in esoteric design
domains that traditionally require expertise and specialized knowledge.
\\ ( https://arxiv.org/abs/2006.05921 ,  7594kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2006.05660 (*cross-listing*)
Date: Wed, 10 Jun 2020 05:26:09 GMT   (21kb)

Title: The nearest-colattice algorithm
Authors: Thomas Espitau, Paul Kirchner
Categories: cs.DS cs.CG cs.CR
Comments: 19 pages, presented at the Algorithmic Number Theory Symposium (ANTS
  2020)
\\
  In this work, we exhibit a hierarchy of polynomial time algorithms solving
approximate variants of the Closest Vector Problem (\CVP). Our first
contribution is a heuristic algorithm achieving the same distance tradeoff as
\HSVP{} algorithms, namely $\approx
  \beta^{\frac{n}{2\beta}}\textrm{covol}(\Lambda)^{\frac{1}{n}}$ for a random
lattice $\Lambda$ of rank $n$. Compared to the so-called Kannan's embedding
technique, our algorithm allows using precomputations and can be used for
efficient batch \CVP~instances. This implies that some attacks on lattice-based
signatures lead to very cheap forgeries, after a precomputation. Our second
contribution is a proven reduction from approximating the closest vector with a
factor $\approx n^{\frac32}\beta^{\frac{3n}{2\beta}}$ to the Shortest Vector
Problem (\SVP) in dimension $\beta$.
\\ ( https://arxiv.org/abs/2006.05660 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2004.08051 (*cross-listing*)
Date: Fri, 17 Apr 2020 03:36:50 GMT   (6822kb,D)
Date (revised v2): Wed, 10 Jun 2020 03:37:41 GMT   (6822kb,D)

Title: Approximate Inverse Reinforcement Learning from Vision-based Imitation
  Learning
Authors: Keuntaek Lee, Bogdan Vlahov, Jason Gibson, James M. Rehg, Evangelos A.
  Theodorou
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  In this work, we present a method for obtaining an implicit objective
function for vision-based navigation. The proposed methodology relies on
Imitation Learning, Model Predictive Control (MPC), and Deep Learning. We use
Imitation Learning as a means to do Inverse Reinforcement Learning in order to
create an approximate costmap generator for a visual navigation challenge. The
resulting costmap is used in conjunction with a Model Predictive Controller for
real-time control and outperforms other state-of-the-art costmap generators
combined with MPC in novel environments. The proposed process allows for simple
training and robustness to out-of-sample data. We apply our method to the task
of vision-based autonomous driving in multiple real and simulated environments
using the same weights for the costmap predictor in all environments.
\\ ( https://arxiv.org/abs/2004.08051 ,  6822kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04878 (*cross-listing*)
Date: Mon, 8 Jun 2020 18:59:24 GMT   (4747kb,D)

Title: KiU-Net: Towards Accurate Segmentation of Biomedical Images using
  Over-complete Representations
Authors: Jeya Maria Jose, Vishwanath Sindagi, Ilker Hacihaliloglu, Vishal M.
  Patel
Categories: eess.IV cs.CV
Comments: Provisionally Accepted at MICCAI 2020
\\
  Due to its excellent performance, U-Net is the most widely used backbone
architecture for biomedical image segmentation in the recent years. However, in
our studies, we observe that there is a considerable performance drop in the
case of detecting smaller anatomical landmarks with blurred noisy boundaries.
We analyze this issue in detail, and address it by proposing an over-complete
architecture (Ki-Net) which involves projecting the data onto higher dimensions
(in the spatial sense). This network, when augmented with U-Net, results in
significant improvements in the case of segmenting small anatomical landmarks
and blurred noisy boundaries while obtaining better overall performance.
Furthermore, the proposed network has additional benefits like faster
convergence and fewer number of parameters. We evaluate the proposed method on
the task of brain anatomy segmentation from 2D Ultrasound (US) of preterm
neonates, and achieve an improvement of around 4% in terms of the DICE accuracy
and Jaccard index as compared to the standard-U-Net, while outperforming the
recent best methods by 2%. Code:
https://github.com/jeya-maria-jose/KiU-Net-pytorch .
\\ ( https://arxiv.org/abs/2006.04878 ,  4747kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04988 (*cross-listing*)
Date: Mon, 8 Jun 2020 23:30:43 GMT   (6223kb,D)

Title: Big GANs Are Watching You: Towards Unsupervised Object Segmentation with
  Off-the-Shelf Generative Models
Authors: Andrey Voynov, Stanislav Morozov, Artem Babenko
Categories: cs.LG cs.CV stat.ML
\\
  Since collecting pixel-level groundtruth data is expensive, unsupervised
visual understanding problems are currently an active research topic. In
particular, several recent methods based on generative models have achieved
promising results for object segmentation and saliency detection. However,
since generative models are known to be unstable and sensitive to
hyperparameters, the training of these methods can be challenging and
time-consuming.
  In this work, we introduce an alternative, much simpler way to exploit
generative models for unsupervised object segmentation. First, we explore the
latent space of the BigBiGAN -- the state-of-the-art unsupervised GAN, which
parameters are publicly available. We demonstrate that object saliency masks
for GAN-produced images can be obtained automatically with BigBiGAN. These
masks then are used to train a discriminative segmentation model. Being very
simple and easy-to-reproduce, our approach provides competitive performance on
common benchmarks in the unsupervised scenario.
\\ ( https://arxiv.org/abs/2006.04988 ,  6223kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04996 (*cross-listing*)
Date: Tue, 9 Jun 2020 00:20:21 GMT   (2370kb,D)

Title: Implicit Class-Conditioned Domain Alignment for Unsupervised Domain
  Adaptation
Authors: Xiang Jiang, Qicheng Lao, Stan Matwin, Mohammad Havaei
Categories: cs.LG cs.CV stat.ML
Comments: Accepted at ICML2020. For code, see
  https://github.com/xiangdal/implicit_alignment
MSC-class: 68T07
\\
  We present an approach for unsupervised domain adaptation---with a strong
focus on practical considerations of within-domain class imbalance and
between-domain class distribution shift---from a class-conditioned domain
alignment perspective. Current methods for class-conditioned domain alignment
aim to explicitly minimize a loss function based on pseudo-label estimations of
the target domain. However, these methods suffer from pseudo-label bias in the
form of error accumulation. We propose a method that removes the need for
explicit optimization of model parameters from pseudo-labels directly. Instead,
we present a sampling-based implicit alignment approach, where the sample
selection procedure is implicitly guided by the pseudo-labels. Theoretical
analysis reveals the existence of a domain-discriminator shortcut in misaligned
classes, which is addressed by the proposed implicit alignment approach to
facilitate domain-adversarial learning. Empirical results and ablation studies
confirm the effectiveness of the proposed approach, especially in the presence
of within-domain class imbalance and between-domain class distribution shift.
\\ ( https://arxiv.org/abs/2006.04996 ,  2370kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04998 (*cross-listing*)
Date: Tue, 9 Jun 2020 00:40:35 GMT   (1230kb)

Title: Automated Quantification of CT Patterns Associated with COVID-19 from
  Chest CT
Authors: Shikha Chaganti, Abishek Balachandran, Guillaume Chabin, Stuart Cohen,
  Thomas Flohr, apl. Prof., Bogdan Georgescu, Philippe Grenier, Prof., Sasa
  Grbic, Siqi Liu, Fran\c{c}ois Mellot, Nicolas Murray, Savvas Nicolaou,
  William Parker, Thomas Re, Pina Sanelli, Alexander W. Sauter, Zhoubing Xu,
  Youngjin Yoo, Valentin Ziebandt, Dorin Comaniciu
Categories: eess.IV cs.CV cs.LG
\\
  Purpose: To present a method that automatically segments and quantifies
abnormal CT patterns commonly present in coronavirus disease 2019 (COVID-19),
namely ground glass opacities and consolidations. Materials and Methods: In
this retrospective study, the proposed method takes as input a non-contrasted
chest CT and segments the lesions, lungs, and lobes in three dimensions, based
on a dataset of 9749 chest CT volumes. The method outputs two combined measures
of the severity of lung and lobe involvement, quantifying both the extent of
COVID-19 abnormalities and presence of high opacities, based on deep learning
and deep reinforcement learning. The first measure of (PO, PHO) is global,
while the second of (LSS, LHOS) is lobe-wise. Evaluation of the algorithm is
reported on CTs of 200 participants (100 COVID-19 confirmed patients and 100
healthy controls) from institutions from Canada, Europe and the United States
collected between 2002-Present (April, 2020). Ground truth is established by
manual annotations of lesions, lungs, and lobes. Correlation and regression
analyses were performed to compare the prediction to the ground truth. Results:
Pearson correlation coefficient between method prediction and ground truth for
COVID-19 cases was calculated as 0.92 for PO (P < .001), 0.97 for PHO(P <
.001), 0.91 for LSS (P < .001), 0.90 for LHOS (P < .001). 98 of 100 healthy
controls had a predicted PO of less than 1%, 2 had between 1-2%. Automated
processing time to compute the severity scores was 10 seconds per case compared
to 30 minutes required for manual annotations. Conclusion: A new method
segments regions of CT abnormalities associated with COVID-19 and computes (PO,
PHO), as well as (LSS, LHOS) severity scores.
\\ ( https://arxiv.org/abs/2006.04998 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05018 (*cross-listing*)
Date: Tue, 9 Jun 2020 02:38:40 GMT   (1290kb)

Title: Deep learning to estimate the physical proportion of infected region of
  lung for COVID-19 pneumonia with CT image set
Authors: Wei Wu, Yu Shi, Xukun Li, Yukun Zhou, Peng Du, Shuangzhi Lv, Tingbo
  Liang, Jifang Sheng
Categories: eess.IV cs.CV cs.LG
\\
  Utilizing computed tomography (CT) images to quickly estimate the severity of
cases with COVID-19 is one of the most straightforward and efficacious methods.
Two tasks were studied in this present paper. One was to segment the mask of
intact lung in case of pneumonia. Another was to generate the masks of regions
infected by COVID-19. The masks of these two parts of images then were
converted to corresponding volumes to calculate the physical proportion of
infected region of lung. A total of 129 CT image set were herein collected and
studied. The intrinsic Hounsfiled value of CT images was firstly utilized to
generate the initial dirty version of labeled masks both for intact lung and
infected regions. Then, the samples were carefully adjusted and improved by two
professional radiologists to generate the final training set and test
benchmark. Two deep learning models were evaluated: UNet and 2.5D UNet. For the
segment of infected regions, a deep learning based classifier was followed to
remove unrelated blur-edged regions that were wrongly segmented out such as air
tube and blood vessel tissue etc. For the segmented masks of intact lung and
infected regions, the best method could achieve 0.972 and 0.757 measure in mean
Dice similarity coefficient on our test benchmark. As the overall proportion of
infected region of lung, the final result showed 0.961 (Pearson's correlation
coefficient) and 11.7% (mean absolute percent error). The instant proportion of
infected regions of lung could be used as a visual evidence to assist clinical
physician to determine the severity of the case. Furthermore, a quantified
report of infected regions can help predict the prognosis for COVID-19 cases
which were scanned periodically within the treatment cycle.
\\ ( https://arxiv.org/abs/2006.05018 ,  1290kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05030 (*cross-listing*)
Date: Tue, 9 Jun 2020 03:21:30 GMT   (2516kb,D)

Title: High Tissue Contrast MRI Synthesis Using Multi-Stage Attention-GAN for
  Glioma Segmentation
Authors: Mohammad Hamghalam, Baiying Lei, Tianfu Wang
Categories: eess.IV cs.CV cs.LG
Comments: Will be published in Thirty-Fourth AAAI Conference on Artificial
  Intelligence (AAAI-2020)
\\
  Magnetic resonance imaging (MRI) provides varying tissue contrast images of
internal organs based on a strong magnetic field. Despite the non-invasive
advantage of MRI in frequent imaging, the low contrast MR images in the target
area make tissue segmentation a challenging problem. This paper demonstrates
the potential benefits of image-to-image translation techniques to generate
synthetic high tissue contrast (HTC) images. Notably, we adopt a new cycle
generative adversarial network (CycleGAN) with an attention mechanism to
increase the contrast within underlying tissues. The attention block, as well
as training on HTC images, guides our model to converge on certain tissues. To
increase the resolution of HTC images, we employ multi-stage architecture to
focus on one particular tissue as a foreground and filter out the irrelevant
background in each stage. This multi-stage structure also alleviates the common
artifacts of the synthetic images by decreasing the gap between source and
target domains. We show the application of our method for synthesizing HTC
images on brain MR scans, including glioma tumor. We also employ HTC MR images
in both the end-to-end and two-stage segmentation structure to confirm the
effectiveness of these images. The experiments over three competitive
segmentation baselines on BraTS 2018 dataset indicate that incorporating the
synthetic HTC images in the multi-modal segmentation framework improves the
average Dice scores 0.8%, 0.6%, and 0.5% on the whole tumor, tumor core, and
enhancing tumor, respectively, while eliminating one real MRI sequence from the
segmentation procedure.
\\ ( https://arxiv.org/abs/2006.05030 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05123 (*cross-listing*)
Date: Tue, 9 Jun 2020 08:54:54 GMT   (5433kb,D)

Title: Orientation Attentive Robot Grasp Synthesis
Authors: Nikolaos Gkanatsios, Georgia Chalvatzaki, Petros Maragos, Jan Peters
Categories: cs.RO cs.CV
Comments: 8 pages, 5 figures
\\
  Physical neighborhoods of grasping points in common objects may offer a wide
variety of plausible grasping configurations. For a fixed center of a simple
spherical object, for example, there is an infinite number of valid grasping
orientations. Such structures create ambiguous and discontinuous grasp maps
that confuse neural regressors. We perform a thorough investigation of the
challenging Jacquard dataset to show that the existing pixel-wise learning
approaches are prone to box overlaps of drastically different orientations. We
then introduce a novel augmented map representation that partitions the angle
space into bins to allow for the co-occurrence of such orientations and observe
larger accuracy margins on the ground truth grasp map reconstructions. On top
of that, we build the ORientation AtteNtive Grasp synthEsis (ORANGE) framework
that jointly solves a bin classification problem and a real-value regression.
The grasp synthesis is attentively supervised by combining discrete and
continuous estimations into a single map. We provide experimental evidence by
appending ORANGE to two existing unimodal architectures and boost their
performance to state-of-the-art levels on Jacquard, specifically 94.71\%, over
all related works, even multimodal. Code is available at
\url{https://github.com/nickgkan/orange}.
\\ ( https://arxiv.org/abs/2006.05123 ,  5433kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05181 (*cross-listing*)
Date: Tue, 9 Jun 2020 11:00:06 GMT   (689kb,D)

Title: Automated Design Space Exploration for optimised Deployment of DNN on
  Arm Cortex-A CPUs
Authors: Miguel de Prado, Andrew Mundy, Rabia Saeed, Maurizio Denna, Nuria
  Pazos and Luca Benini
Categories: cs.LG cs.CV eess.IV stat.ML
\\
  The spread of deep learning on embedded devices has prompted the development
of numerous methods to optimise the deployment of deep neural networks (DNN).
Works have mainly focused on: i) efficient DNN architectures, ii) network
optimisation techniques such as pruning and quantisation, iii) optimised
algorithms to speed up the execution of the most computational intensive layers
and, iv) dedicated hardware to accelerate the data flow and computation.
However, there is a lack of research on the combination of these methods as the
space of approaches becomes too large to test and obtain a globally optimised
solution, which leads to suboptimal deployment in terms of latency, accuracy,
and memory.
  In this work, we first detail and analyse the methods to improve the
deployment of DNNs across the different levels of software optimisation.
Building on this knowledge, we present an automated exploration framework to
ease the deployment of DNNs for industrial applications by automatically
exploring the design space and learning an optimised solution that speeds up
the performance and reduces the memory on embedded CPU platforms. The framework
relies on a Reinforcement Learning -based search that, combined with a deep
learning inference framework, enables the deployment of DNN implementations to
obtain empirical measurements on embedded AI applications. Thus, we present a
set of results for state-of-the-art DNNs on a range of Arm Cortex-A CPU
platforms achieving up to 4x improvement in performance and over 2x reduction
in memory with negligible loss in accuracy with respect to the BLAS
floating-point implementation.
\\ ( https://arxiv.org/abs/2006.05181 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05218 (*cross-listing*)
Date: Tue, 9 Jun 2020 12:32:16 GMT   (3726kb,D)

Title: Super-resolution Variational Auto-Encoders
Authors: Ioannis Gatopoulos, Maarten Stol, Jakub M. Tomczak
Categories: cs.LG cs.CV stat.ML
Comments: 13 pages, 11 figures, 3 tables. Code available at:
  https://github.com/ioangatop/srVAE
\\
  The framework of variational autoencoders (VAEs) provides a principled method
for jointly learning latent-variable models and corresponding inference models.
However, the main drawback of this approach is the blurriness of the generated
images. Some studies link this effect to the objective function, namely, the
(negative) log-likelihood. Here, we propose to enhance VAEs by adding a random
variable that is a downscaled version of the original image and still use the
log-likelihood function as the learning objective. Further, by providing the
downscaled image as an input to the decoder, it can be used in a manner similar
to the super-resolution. We present empirically that the proposed approach
performs comparably to VAEs in terms of the negative log-likelihood function,
but it obtains a better FID score.
\\ ( https://arxiv.org/abs/2006.05218 ,  3726kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05245 (*cross-listing*)
Date: Tue, 9 Jun 2020 13:29:15 GMT   (276kb)

Title: A Review of Automatically Diagnosing COVID-19 based on Scanning Image
Authors: Delong Chen, Fan Liu, Zewen Li
Categories: eess.IV cs.CV cs.LG
Comments: under review of PRCV2020
\\
  The pandemic of COVID-19 has caused millions of infectious. Due to the
false-negative rate and the time cost of conventional RT-PCR tests, X-ray
images and Computed Tomography (CT) images based diagnosing become widely
adopted. Therefore, researchers of the computer vision area have developed many
automatic diagnosing models to help the radiologists and pro-mote the
diagnosing accuracy. In this paper, we present a review of these recently
emerging automatic diagnosing models. 62 models from 14, February to 5, May,
2020 are involved. We analyzed the models from the perspective of
preprocessing, feature extraction, classification, and evaluation. Then we
pointed out that domain adaption in transfer learning and interpretability
promotion are the possible future directions.
\\ ( https://arxiv.org/abs/2006.05245 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05249 (*cross-listing*)
Date: Tue, 9 Jun 2020 13:33:04 GMT   (598kb)

Title: What takes the brain so long: Object recognition at the level of minimal
  images develops for up to seconds of presentation time
Authors: Hanna Benoni, Daniel Harari and Shimon Ullman
Categories: q-bio.NC cs.AI cs.CV
Comments: 7 pages, 2 figures, 1 table
\\
  Rich empirical evidence has shown that visual object recognition in the brain
is fast and effortless, with relevant brain signals reported to start as early
as 80 ms. Here we study the time trajectory of the recognition process at the
level of minimal recognizable images (termed MIRC). These are images that can
be recognized reliably, but in which a minute change of the image (reduction by
either size or resolution) has a drastic effect on recognition. Subjects were
assigned to one of nine exposure conditions: 200, 500, 1000, 2000 ms with or
without masking, as well as unlimited time. The subjects were not limited in
time to respond after presentation. The results show that in the masked
conditions, recognition rates develop gradually over an extended period, e.g.
average of 18% for 200 ms exposure and 45% for 500 ms, increasing significantly
with longer exposure even above 2 secs. When presented for unlimited time
(until response), MIRC recognition rates were equivalent to the rates of
full-object images presented for 50 ms followed by masking. What takes the
brain so long to recognize such images? We discuss why processes involving
eye-movements, perceptual decision-making and pattern completion are unlikely
explanations. Alternatively, we hypothesize that MIRC recognition requires an
extended top-down process complementing the feed-forward phase.
\\ ( https://arxiv.org/abs/2006.05249 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05274 (*cross-listing*)
Date: Sat, 6 Jun 2020 19:24:35 GMT   (2174kb,D)

Title: UMLS-ChestNet: A deep convolutional neural network for radiological
  findings, differential diagnoses and localizations of COVID-19 in chest
  x-rays
Authors: Germ\'an Gonz\'alez, Aurelia Bustos, Jos\'e Mar\'ia Salinas, Mar\'ia
  de la Iglesia-Vaya, Joaqu\'in Galant, Carlos Cano-Espinosa, Xavier Barber,
  Domingo Orozco-Beltr\'an, Miguel Cazorla and Antonio Pertusa
Categories: eess.IV cs.CV cs.LG
Comments: 17 pages, 3 figures, 3 tables
\\
  In this work we present a method for the detection of radiological findings,
their location and differential diagnoses from chest x-rays. Unlike prior works
that focus on the detection of few pathologies, we use a hierarchical taxonomy
mapped to the Unified Medical Language System (UMLS) terminology to identify
189 radiological findings, 22 differential diagnosis and 122 anatomic
locations, including ground glass opacities, infiltrates, consolidations and
other radiological findings compatible with COVID-19. We train the system on
one large database of 92,594 frontal chest x-rays (AP or PA, standing, supine
or decubitus) and a second database of 2,065 frontal images of COVID-19
patients identified by at least one positive Polymerase Chain Reaction (PCR)
test. The reference labels are obtained through natural language processing of
the radiological reports. On 23,159 test images, the proposed neural network
obtains an AUC of 0.94 for the diagnosis of COVID-19. To our knowledge, this
work uses the largest chest x-ray dataset of COVID-19 positive cases to date
and is the first one to use a hierarchical labeling schema and to provide
interpretability of the results, not only by using network attention methods,
but also by indicating the radiological findings that have led to the
diagnosis.
\\ ( https://arxiv.org/abs/2006.05274 ,  2174kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05332 (*cross-listing*)
Date: Sun, 7 Jun 2020 20:42:25 GMT   (8929kb,D)

Title: A Comparative Study on Early Detection of COVID-19 from Chest X-Ray
  Images
Authors: Mete Ahishali, Aysen Degerli, Mehmet Yamac, Serkan Kiranyaz, Muhammad
  E. H. Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
Categories: eess.IV cs.CV
Comments: 11 pages
\\
  In this study, our first aim is to evaluate the ability of recent
state-of-the-art Machine Learning techniques to early detect COVID-19 from
plain chest X-ray images. Both compact classifiers and deep learning approaches
are considered in this study. Furthermore, we propose a recent compact
classifier, Convolutional Support Estimator Network (CSEN) approach for this
purpose since it is well-suited for a scarce-data classification task. Finally,
this study introduces a new benchmark dataset called Early-QaTa-COV19, which
consists of 175 early-stage COVID-19 Pneumonia samples (very limited or no
infection signs) labelled by the medical doctors and 1579 samples for control
(normal) class. A detailed set of experiments show that the CSEN achieves the
top (over 98.5%) sensitivity with over 96% specificity. Moreover, transfer
learning over the deep CheXNet fine-tuned with the augmented data produces the
leading performance among other deep networks with 97.14% sensitivity and
99.49% specificity.
\\ ( https://arxiv.org/abs/2006.05332 ,  8929kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05389 (*cross-listing*)
Date: Tue, 9 Jun 2020 16:39:07 GMT   (383kb,D)

Title: A t-distribution based operator for enhancing \\ out of distribution
  robustness of neural network classifiers
Authors: Niccol\`o Antonello, Philip N. Garner
Categories: eess.SP cs.CV cs.LG stat.ML
Comments: 5 pages, 5 figures, to be published in IEEE Signal Processing
  Letters, reproducible code https://github.com/idiap/tsoftmax
\\
  Neural Network (NN) classifiers can assign extreme probabilities to samples
that have not appeared during training (out-of-distribution samples) resulting
in erroneous and unreliable predictions. One of the causes for this unwanted
behaviour lies in the use of the standard softmax operator which pushes the
posterior probabilities to be either zero or unity hence failing to model
uncertainty. The statistical derivation of the softmax operator relies on the
assumption that the distributions of the latent variables for a given class are
Gaussian with known variance. However, it is possible to use different
assumptions in the same derivation and attain from other families of
distributions as well. This allows derivation of novel operators with more
favourable properties. Here, a novel operator is proposed that is derived using
$t$-distributions which are capable of providing a better description of
uncertainty. It is shown that classifiers that adopt this novel operator can be
more robust to out of distribution samples, often outperforming NNs that use
the standard softmax operator. These enhancements can be reached with minimal
changes to the NN architecture.
\\ ( https://arxiv.org/abs/2006.05389 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05398 (*cross-listing*)
Date: Tue, 9 Jun 2020 16:52:02 GMT   (5154kb,D)

Title: Deep Visual Reasoning: Learning to Predict Action Sequences for Task and
  Motion Planning from an Initial Scene Image
Authors: Danny Driess, Jung-Su Ha, Marc Toussaint
Categories: cs.LG cs.AI cs.CV cs.RO stat.ML
Comments: Robotics: Science and Systems (R:SS) 2020
\\
  In this paper, we propose a deep convolutional recurrent neural network that
predicts action sequences for task and motion planning (TAMP) from an initial
scene image. Typical TAMP problems are formalized by combining reasoning on a
symbolic, discrete level (e.g. first-order logic) with continuous motion
planning such as nonlinear trajectory optimization. Due to the great
combinatorial complexity of possible discrete action sequences, a large number
of optimization/motion planning problems have to be solved to find a solution,
which limits the scalability of these approaches.
  To circumvent this combinatorial complexity, we develop a neural network
which, based on an initial image of the scene, directly predicts promising
discrete action sequences such that ideally only one motion planning problem
has to be solved to find a solution to the overall TAMP problem. A key aspect
is that our method generalizes to scenes with many and varying number of
objects, although being trained on only two objects at a time. This is possible
by encoding the objects of the scene in images as input to the neural network,
instead of a fixed feature vector. Results show runtime improvements of several
magnitudes. Video: https://youtu.be/i8yyEbbvoEk
\\ ( https://arxiv.org/abs/2006.05398 ,  5154kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05415 (*cross-listing*)
Date: Tue, 9 Jun 2020 17:28:25 GMT   (916kb)

Title: Neuroevolution in Deep Neural Networks: Current Trends and Future
  Challenges
Authors: Edgar Galv\'an and Peter Mooney
Categories: cs.NE cs.CV cs.LG
Comments: 20 pages (double column), 2 figures, 3 tables, 157 references
\\
  A variety of methods have been applied to the architectural configuration and
learning or training of artificial deep neural networks (DNN). These methods
play a crucial role in the success or failure of the DNN for most problems and
applications. Evolutionary Algorithms (EAs) are gaining momentum as a
computationally feasible method for the automated optimisation and training of
DNNs. Neuroevolution is a term which describes these processes of automated
configuration and training of DNNs using EAs. While many works exist in the
literature, no comprehensive surveys currently exist focusing exclusively on
the strengths and limitations of using neuroevolution approaches in DNNs.
Prolonged absence of such surveys can lead to a disjointed and fragmented field
preventing DNNs researchers potentially adopting neuroevolutionary methods in
their own research, resulting in lost opportunities for improving performance
and wider application within real-world deep learning problems. This paper
presents a comprehensive survey, discussion and evaluation of the
state-of-the-art works on using EAs for architectural configuration and
training of DNNs. Based on this survey, the paper highlights the most pertinent
current issues and challenges in neuroevolution and identifies multiple
promising future research directions.
\\ ( https://arxiv.org/abs/2006.05415 ,  916kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05467 (*cross-listing*)
Date: Tue, 9 Jun 2020 19:21:57 GMT   (1141kb,D)

Title: Pruning neural networks without any data by iteratively conserving
  synaptic flow
Authors: Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli
Categories: cs.LG cond-mat.dis-nn cs.CV q-bio.NC stat.ML
\\
  Pruning the parameters of deep neural networks has generated intense interest
due to potential savings in time, memory and energy both during training and at
test time. Recent works have identified, through an expensive sequence of
training and pruning cycles, the existence of winning lottery tickets or sparse
trainable subnetworks at initialization. This raises a foundational question:
can we identify highly sparse trainable subnetworks at initialization, without
ever training, or indeed without ever looking at the data? We provide an
affirmative answer to this question through theory driven algorithm design. We
first mathematically formulate and experimentally verify a conservation law
that explains why existing gradient-based pruning algorithms at initialization
suffer from layer-collapse, the premature pruning of an entire layer rendering
a network untrainable. This theory also elucidates how layer-collapse can be
entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow
Pruning (SynFlow). This algorithm can be interpreted as preserving the total
flow of synaptic strengths through the network at initialization subject to a
sparsity constraint. Notably, this algorithm makes no reference to the training
data and consistently outperforms existing state-of-the-art pruning algorithms
at initialization over a range of models (VGG and ResNet), datasets
(CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9
percent). Thus our data-agnostic pruning algorithm challenges the existing
paradigm that data must be used to quantify which synapses are important.
\\ ( https://arxiv.org/abs/2006.05467 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05480 (*cross-listing*)
Date: Tue, 9 Jun 2020 19:44:10 GMT   (1246kb)

Title: DcardNet: Diabetic Retinopathy Classification at Multiple Depths Based
  on Structural and Angiographic Optical Coherence Tomography
Authors: Pengxiao Zang, Liqin Gao, Tristan T. Hormel, Jie Wang, Qisheng You,
  Thomas S. Hwang and Yali Jia
Categories: eess.IV cs.CV cs.LG
Comments: Submitted to IEEE Transactions on Biomedical Engineering
\\
  Optical coherence tomography (OCT) and its angiography (OCTA) have several
advantages for the early detection and diagnosis of diabetic retinopathy (DR).
However, automated, complete DR classification frameworks based on both OCT and
OCTA data have not been proposed. In this study, a densely and continuously
connected neural network with adaptive rate dropout (DcardNet) is proposed to
fulfill a DR classification framework using en face OCT and OCTA. The proposed
network outputs three separate classification depths on each case based on the
International Clinical Diabetic Retinopathy scale. At the highest level the
network classifies scans as referable or non-referable for DR. The second depth
classifies the eye as non-DR, non-proliferative DR (NPDR), or proliferative DR
(PDR). The last depth classifies the case as no DR, mild and moderate NPDR,
severe NPDR, and PDR. We used 10-fold cross-validation with 10% of the data to
assess the performance of our network. The overall classification accuracies of
the three depths were 95.7%, 85.0%, and 71.0% respectively.
\\ ( https://arxiv.org/abs/2006.05480 ,  1246kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05509 (*cross-listing*)
Date: Tue, 9 Jun 2020 21:06:46 GMT   (433kb)

Title: Can artificial intelligence (AI) be used to accurately detect
  tuberculosis (TB) from chest x-ray? A multiplatform evaluation of five AI
  products used for TB screening in a high TB-burden setting
Authors: Zhi Zhen Qin, Shahriar Ahmed, Mohammad Shahnewaz Sarker, Kishor Paul,
  Ahammad Shafiq Sikder Adel, Tasneem Naheyan, Sayera Banu, Jacob Creswell
Categories: eess.IV cs.CV cs.LG q-bio.QM
Comments: 27 pages, 5 Tables 3 Figures
MSC-class: 92B20
ACM-class: I.2.1
\\
  Powered by artificial intelligence (AI), particularly deep neural networks,
computer aided detection (CAD) tools can be trained to recognize TB-related
abnormalities on chest radiographs, thereby screening large numbers of people
and reducing the pressure on healthcare professionals. Addressing the lack of
studies comparing the performance of different products, we evaluated five AI
software platforms specific to TB: CAD4TB (v6), InferReadDR (v2), Lunit INSIGHT
for Chest Radiography (v4.9.0) , JF CXR-1 (v2) by and qXR (v3) by on an unseen
dataset of chest X-rays collected in three TB screening center in Dhaka,
Bangladesh. The 23,566 individuals included in the study all received a CXR
read by a group of three Bangladeshi board-certified radiologists. A sample of
CXRs were re-read by US board-certified radiologists. Xpert was used as the
reference standard. All five AI platforms significantly outperformed the human
readers. The areas under the receiver operating characteristic curves are qXR:
0.91 (95% CI:0.90-0.91), Lunit INSIGHT CXR: 0.89 (95% CI:0.88-0.89),
InferReadDR: 0.85 (95% CI:0.84-0.86), JF CXR-1: 0.85 (95% CI:0.84-0.85),
CAD4TB: 0.82 (95% CI:0.81-0.83). We also proposed a new analytical framework
that evaluates a screening and triage test and informs threshold selection
through tradeoff between cost efficiency and ability to triage. Further, we
assessed the performance of the five AI algorithms across the subgroups of age,
use cases, and prior TB history, and found that the threshold scores performed
differently across different subgroups. The positive results of our evaluation
indicate that these AI products can be useful screening and triage tools for
active case finding in high TB-burden regions.
\\ ( https://arxiv.org/abs/2006.05509 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05513 (*cross-listing*)
Date: Tue, 9 Jun 2020 21:16:47 GMT   (4694kb)

Title: A Deep Learning-Based Method for Automatic Segmentation of Proximal
  Femur from Quantitative Computed Tomography Images
Authors: Chen Zhao, Joyce H. Keyak, Jinshan Tang, Tadashi S. Kaneko, Sundeep
  Khosla, Shreyasee Amin, Elizabeth J. Atkinson, Lan-Juan Zhao, Michael J.
  Serou, Chaoyang Zhang, Hui Shen, Hong-Wen Deng, Weihua Zhou
Categories: physics.med-ph cs.CV eess.IV
\\
  Purpose: Proximal femur image analyses based on quantitative computed
tomography (QCT) provide a method to quantify the bone density and evaluate
osteoporosis and risk of fracture. We aim to develop a deep-learning-based
method for automatic proximal femur segmentation. Methods and Materials: We
developed a 3D image segmentation method based on V-Net, an end-to-end fully
convolutional neural network (CNN), to extract the proximal femur QCT images
automatically. The proposed V-net methodology adopts a compound loss function,
which includes a Dice loss and a L2 regularizer. We performed experiments to
evaluate the effectiveness of the proposed segmentation method. In the
experiments, a QCT dataset which included 397 QCT subjects was used. For the
QCT image of each subject, the ground truth for the proximal femur was
delineated by a well-trained scientist. During the experiments for the entire
cohort then for male and female subjects separately, 90% of the subjects were
used in 10-fold cross-validation for training and internal validation, and to
select the optimal parameters of the proposed models; the rest of the subjects
were used to evaluate the performance of models. Results: Visual comparison
demonstrated high agreement between the model prediction and ground truth
contours of the proximal femur portion of the QCT images. In the entire cohort,
the proposed model achieved a Dice score of 0.9815, a sensitivity of 0.9852 and
a specificity of 0.9992. In addition, an R2 score of 0.9956 (p<0.001) was
obtained when comparing the volumes measured by our model prediction with the
ground truth. Conclusion: This method shows a great promise for clinical
application to QCT and QCT-based finite element analysis of the proximal femur
for evaluating osteoporosis and hip fracture risk.
\\ ( https://arxiv.org/abs/2006.05513 ,  4694kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05521 (*cross-listing*)
Date: Tue, 9 Jun 2020 21:38:05 GMT   (806kb,D)

Title: Supervised Learning of Sparsity-Promoting Regularizers for Denoising
Authors: Michael T. McCann, Saiprasad Ravishankar
Categories: eess.IV cs.CV
\\
  We present a method for supervised learning of sparsity-promoting
regularizers for image denoising. Sparsity-promoting regularization is a key
ingredient in solving modern image reconstruction problems; however, the
operators underlying these regularizers are usually either designed by hand or
learned from data in an unsupervised way. The recent success of supervised
learning (mainly convolutional neural networks) in solving image reconstruction
problems suggests that it could be a fruitful approach to designing
regularizers. As a first experiment in this direction, we propose to denoise
images using a variational formulation with a parametric, sparsity-promoting
regularizer, where the parameters of the regularizer are learned to minimize
the mean squared error of reconstructions on a training set of (ground truth
image, measurement) pairs. Training involves solving a challenging bilievel
optimization problem; we derive an expression for the gradient of the training
loss using Karush-Kuhn-Tucker conditions and provide an accompanying gradient
descent algorithm to minimize it. Our experiments on a simple synthetic,
denoising problem show that the proposed method can learn an operator that
outperforms well-known regularizers (total variation, DCT-sparsity, and
unsupervised dictionary learning) and collaborative filtering. While the
approach we present is specific to denoising, we believe that it can be adapted
to the whole class of inverse problems with linear measurement models, giving
it applicability to a wide range of image reconstruction problems.
\\ ( https://arxiv.org/abs/2006.05521 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05544 (*cross-listing*)
Date: Tue, 9 Jun 2020 23:07:55 GMT   (3271kb,D)

Title: Resolution-Enhanced MRI-Guided Navigation of Spinal Cellular Injection
  Robot
Authors: Daniel Enrique Martinez, Waiman Meinhold, John Oshinski, Ai-Ping Hu,
  and Jun Ueda
Categories: cs.RO cs.CV eess.IV
Comments: 6 pages, 10 figures, 3 tables, conference
\\
  This paper presents a method of navigating a surgical robot beyond the
resolution of magnetic resonance imaging (MRI) by using a resolution
enhancement technique enabled by high-precision piezoelectric actuation. The
surgical robot was specifically designed for injecting stem cells into the
spinal cord. This particular therapy can be performed in a shorter time by
using a MRI-compatible robotic platform than by using a manual needle
positioning platform. Imaging resolution of fiducial markers attached to the
needle guide tubing was enhanced by reconstructing a high-resolution image from
multiple images with sub-pixel movements of the robot. The parallel-plane
direct-drive needle positioning mechanism positioned the needle guide with a
high spatial precision that is two orders of magnitude higher than typical MRI
resolution up to 1 mm. Reconstructed resolution enhanced images were used to
navigate the robot precisely that would not have been possible by using
standard MRI. Experiments were conducted to verify the effectiveness of the
proposed enhanced-resolution image-guided intervention.
\\ ( https://arxiv.org/abs/2006.05544 ,  3271kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05575 (*cross-listing*)
Date: Wed, 10 Jun 2020 00:19:58 GMT   (7910kb,D)

Title: Deep Learning-based Aerial Image Segmentation with Open Data for
  Disaster Impact Assessment
Authors: Ananya Gupta, Simon Watson, Hujun Yin
Categories: eess.IV cs.CV
Comments: Accepted in Neurocomputing, 2020
\\
  Satellite images are an extremely valuable resource in the aftermath of
natural disasters such as hurricanes and tsunamis where they can be used for
risk assessment and disaster management. In order to provide timely and
actionable information for disaster response, in this paper a framework
utilising segmentation neural networks is proposed to identify impacted areas
and accessible roads in post-disaster scenarios. The effectiveness of
pretraining with ImageNet on the task of aerial image segmentation has been
analysed and performances of popular segmentation models compared. Experimental
results show that pretraining on ImageNet usually improves the segmentation
performance for a number of models. Open data available from OpenStreetMap
(OSM) is used for training, forgoing the need for time-consuming manual
annotation. The method also makes use of graph theory to update road network
data available from OSM and to detect the changes caused by a natural disaster.
Extensive experiments on data from the 2018 tsunami that struck Palu, Indonesia
show the effectiveness of the proposed framework. ENetSeparable, with 30% fewer
parameters compared to ENet, achieved comparable segmentation results to that
of the state-of-the-art networks.
\\ ( https://arxiv.org/abs/2006.05575 ,  7910kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05586 (*cross-listing*)
Date: Wed, 10 Jun 2020 01:03:09 GMT   (2376kb,D)

Title: Dual-level Semantic Transfer Deep Hashing for Efficient Social Image
  Retrieval
Authors: Lei Zhu, Hui Cui, Zhiyong Cheng, Jingjing Li, Zheng Zhang
Categories: cs.IR cs.CV
Comments: Accepted by IEEE TCSVT
\\
  Social network stores and disseminates a tremendous amount of user shared
images. Deep hashing is an efficient indexing technique to support large-scale
social image retrieval, due to its deep representation capability, fast
retrieval speed and low storage cost. Particularly, unsupervised deep hashing
has well scalability as it does not require any manually labelled data for
training. However, owing to the lacking of label guidance, existing methods
suffer from severe semantic shortage when optimizing a large amount of deep
neural network parameters. Differently, in this paper, we propose a Dual-level
Semantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a
unified deep hash learning framework. Our model targets at learning the
semantically enhanced deep hash codes by specially exploiting the
user-generated tags associated with the social images. Specifically, we design
a complementary dual-level semantic transfer mechanism to efficiently discover
the potential semantics of tags and seamlessly transfer them into binary hash
codes. On the one hand, instance-level semantics are directly preserved into
hash codes from the associated tags with adverse noise removing. Besides, an
image-concept hypergraph is constructed for indirectly transferring the latent
high-order semantic correlations of images and tags into hash codes. Moreover,
the hash codes are obtained simultaneously with the deep representation
learning by the discrete hash optimization strategy. Extensive experiments on
two public social image retrieval datasets validate the superior performance of
our method compared with state-of-the-art hashing methods. The source codes of
our method can be obtained at https://github.com/research2020-1/DSTDH
\\ ( https://arxiv.org/abs/2006.05586 ,  2376kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05782 (*cross-listing*)
Date: Wed, 10 Jun 2020 11:37:49 GMT   (568kb)

Title: Applying Deep-Learning-Based Computer Vision to Wireless Communications:
  Methodologies, Opportunities, and Challenges
Authors: Yu Tian and Gaofeng Pan and Mohamed-Slim Alouini
Categories: eess.SP cs.CV cs.LG
\\
  Deep learning (DL) has obtained great success in computer vision (CV) field,
and the related techniques have been widely used in security, healthcare,
remote sensing, etc. On the other hand, visual data is universal in our daily
life, which is easily generated by prevailing but low-cost cameras. Therefore,
DL-based CV can be explored to obtain and forecast some useful information
about the objects, e.g., the number, locations, distribution, motion, etc.
Intuitively, DL-based CV can facilitate and improve the designs of wireless
communications, especially in dynamic network scenarios. However, so far, it is
rare to see such kind of works in the existing literature. Then, the primary
purpose of this article is to introduce ideas of applying DL-based CV in
wireless communications to bring some novel degrees of freedom for both
theoretical researches and engineering applications. To illustrate how DL-based
CV can be applied in wireless communications, an example of using DL-based CV
to millimeter wave (mmWave) system is given to realize optimal mmWave
multiple-input and multiple-output (MIMO) beamforming in mobile scenarios. In
this example, we proposed a framework to predict the future beam indices from
the previously-observed beam indices and images of street views by using
ResNet, 3-dimensional ResNext, and long short term memory network. Experimental
results show that our frameworks can achieve much higher accuracy than the
baseline method, and visual data can help significantly improve the performance
of MIMO beamforming system. Finally, we discuss the opportunities and
challenges of applying DL-based CV in wireless communications.
\\ ( https://arxiv.org/abs/2006.05782 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05838 (*cross-listing*)
Date: Wed, 10 Jun 2020 14:00:14 GMT   (17814kb,D)

Title: To Regularize or Not To Regularize? The Bias Variance Trade-off in
  Regularized AEs
Authors: Arnab Kumar Mondal, Himanshu Asnani, Parag Singla, Prathosh AP
Categories: cs.LG cs.CV stat.ML
\\
  Regularized Auto-Encoders (AE) form a rich class of methods within the
landscape of neural generative models. They effectively model the
joint-distribution between the data and a latent space using an Encoder-Decoder
combination, with regularization imposed in terms of a prior over the latent
space. Despite their advantages such as stability in training, the performance
of AE based models has not reached that of the other models such as GANs. While
several reasons including the presence of conflicting terms in the objective,
distributional choices imposed on the Encoder and the Decoder, and
dimensionality of the latent space have been identified as possible causes for
the suboptimal performance, the role of the regularization (prior distribution)
imposed has not been studied systematically. Motivated by this, we examine the
effect of the latent prior on the generation quality of the AE models in this
paper. We show that there is no single fixed prior which is optimal for all
data distributions, given a Gaussian Decoder. Further, with finite data, we
show that there exists a bias-variance trade-off that comes with prior
imposition. As a remedy, we optimize a generalized ELBO objective, with an
additional state space over the latent prior. We implicitly learn this flexible
prior jointly with the AE training using an adversarial learning technique,
which facilitates operation on different points of the bias-variance curve. Our
experiments on multiple datasets show that the proposed method is the new
state-of-the-art for AE based generative models.
\\ ( https://arxiv.org/abs/2006.05838 ,  17814kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05861 (*cross-listing*)
Date: Wed, 10 Jun 2020 14:38:05 GMT   (2873kb,D)

Title: A systematic review on the role of artificial intelligence in
  sonographic diagnosis of thyroid cancer: Past, present and future
Authors: Fatemeh Abdolali, Atefeh Shahroudnejad, Abhilash Rakkunedeth
  Hareendranathan, Jacob L Jaremko, Michelle Noga, Kumaradevan Punithakumar
Categories: eess.IV cs.CV
\\
  Thyroid cancer is common worldwide, with a rapid increase in prevalence
across North America in recent years. While most patients present with palpable
nodules through physical examination, a large number of small and medium-sized
nodules are detected by ultrasound examination. Suspicious nodules are then
sent for biopsy through fine needle aspiration. Since biopsies are invasive and
sometimes inconclusive, various research groups have tried to develop
computer-aided diagnosis systems. Earlier approaches along these lines relied
on clinically relevant features that were manually identified by radiologists.
With the recent success of artificial intelligence (AI), various new methods
are being developed to identify these features in thyroid ultrasound
automatically. In this paper, we present a systematic review of
state-of-the-art on AI application in sonographic diagnosis of thyroid cancer.
This review follows a methodology-based classification of the different
techniques available for thyroid cancer diagnosis. With more than 50 papers
included in this review, we reflect on the trends and challenges of the field
of sonographic diagnosis of thyroid malignancies and potential of
computer-aided diagnosis to increase the impact of ultrasound applications on
the future of thyroid cancer diagnosis. Machine learning will continue to play
a fundamental role in the development of future thyroid cancer diagnosis
frameworks.
\\ ( https://arxiv.org/abs/2006.05861 ,  2873kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05679 (*cross-listing*)
Date: Wed, 10 Jun 2020 06:27:07 GMT   (1725kb,D)

Title: Dense and sparse vertex connectivity in networks
Authors: Djellabi Mehdi, Jouve Bertrand, Amblard Fr\'ed\'eric
Categories: cs.SI cs.DM
\\
  The different approaches developed to analyze the structure of complex
networks have generated a large number of studies. In the field of social
networks at least, studies mainly address the detection and analysis of
communities. In this paper, we challenge these approaches and focus on nodes
that have meaningful local interactions able to identify the internal
organization of communities or the way communities are assembled. We propose an
algorithm, ItRich, to identify this type of nodes, based on the decomposition
of a graph into successive, less and less dense, layers. Our method is tested
on synthetic and real data sets and meshes well with other methods such as
community detection or k-core decomposition.
\\ ( https://arxiv.org/abs/2006.05679 ,  1725kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05740 (*cross-listing*)
Date: Wed, 10 Jun 2020 09:15:32 GMT   (364kb)

Title: An Asymptotically Optimal Algorithm for Online Stacking
Authors: Martin Olsen, Allan Gross
Categories: cs.DS cs.DM
\\
  Consider a storage area where arriving items are stored temporarily in
bounded capacity stacks until their departure. We look into the problem of
deciding where to put an arriving item with the objective of minimizing the
maximum number of stacks used over time. The decision has to be made as soon as
an item arrives, and we assume that we only have information on the departure
times for the arriving item and the items currently at the storage area. We are
only allowed to put an item on top of another item if the item below departs at
a later time. We refer to this problem as online stacking. We assume that the
storage time intervals are picked i.i.d. from $[0, 1] \times [0, 1]$ using an
unknown distribution with a bounded probability density function. Under this
mild condition, we present a simple polynomial time online algorithm and show
that the competitive ratio converges to $1$ in probability. The result holds if
the stack capacity is $o(\sqrt{n})$, where $n$ is the number of items,
including the realistic case where the capacity is a constant. Our experiments
show that our results also have practical relevance. To the best of our
knowledge, we are the first to present an asymptotically optimal algorithm for
online stacking, which is an important problem with many real-world
applications within computational logistics.
\\ ( https://arxiv.org/abs/2006.05740 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05979 (*cross-listing*)
Date: Wed, 10 Jun 2020 17:49:16 GMT   (230kb,D)

Title: Product Forms for FCFS Queueing Models with Arbitrary Server-Job
  Compatibilities: An Overview
Authors: Kristen Gardner and Rhonda Righter
Categories: cs.PF cs.DM
Comments: 32 pages, 5 figures
\\
  In recent years a number of models involving different compatibilities
between jobs and servers in queueing systems, or between agents and resources
in matching systems, have been studied, and, under Markov assumptions and
appropriate stability conditions, the stationary distributions have been shown
to have product forms. We survey these results and show how, under an
appropriate detailed description of the state, many are corollaries of similar
results for the Order Independent Queue. We also discuss how to use the product
form results to determine distributions for steady-state response times.
\\ ( https://arxiv.org/abs/2006.05979 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04831 (*cross-listing*)
Date: Mon, 8 Jun 2020 18:00:18 GMT   (1111kb,D)

Title: Evaluation of Quantum Approximate Optimization Algorithm based on the
  approximation ratio of single samples
Authors: Jason Larkin, Mat\'ias Jonsson, Daniel Justice, and Gian Giacomo
  Guerreschi
Categories: quant-ph cs.CC cs.ET
\\
  The Quantum Approximate Optimization Algorithm (QAOA) is a hybrid
quantum-classical algorithm to solve binary-variable optimization problems. Due
to its expected robustness to systematic errors and the short circuit depth, it
is one of the promising candidates likely to run on near-term quantum devices.
We project the performance of QAOA applied to the Max-Cut problem and compare
it with some of the best classical alternatives, both for exact or approximate
solution. When comparing approximate solvers, their performance is
characterized by the computational time taken to achieve a given quality of
solution. Since QAOA is based on sampling, we introduce performance metrics
based on the probability of observing a sample above a certain quality. In
addition, we show that the QAOA performance varies significantly with the graph
type. By selecting a suitable optimizer for the variational parameters and
reducing the number of function evaluations, QAOA performance improves by up to
2 orders of magnitude compared to previous estimates. Especially for 3-regular
random graphs, this setting decreases the performance gap with classical
alternatives.
\\ ( https://arxiv.org/abs/2006.04831 ,  1111kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05696 (*cross-listing*)
Date: Wed, 10 Jun 2020 07:27:14 GMT   (998kb,D)

Title: Unified Characterization Platform for Emerging NVM Technology: Neural
  Network Application Benchmarking Using off-the-shelf NVM Chips
Authors: Supriya Chakraborty, Abhishek Gupta, and Manan Suri
Categories: cs.AR cs.ET
Comments: Accepted at 2020 IEEE International Symposium on Circuits and Systems
  (ISCAS)
Journal-ref: 2020 IEEE International Symposium on Circuits and Systems (ISCAS)
\\
  In this paper, we present a unified FPGA based electrical test-bench for
characterizing different emerging NonVolatile Memory (NVM) chips. In
particular, we present detailed electrical characterization and benchmarking of
multiple commercially available, off-the-shelf, NVM chips viz.: MRAM, FeRAM,
CBRAM, and ReRAM. We investigate important NVM parameters such as: (i) current
consumption patterns, (ii) endurance, and (iii) error characterization. The
proposed FPGA based testbench is then utilized for a Proof-of-Concept (PoC)
Neural Network (NN) image classification application. Four emerging NVM chips
are benchmarked against standard SRAM and Flash technology for the AI
application as active weight memory during inference mode.
\\ ( https://arxiv.org/abs/2006.05696 ,  998kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05868 (*cross-listing*)
Date: Wed, 10 Jun 2020 14:50:28 GMT   (600kb,D)

Title: Improving Dependability of Neuromorphic Computing With Non-Volatile
  Memory
Authors: Shihao Song, Anup Das, Nagarajan Kandasamy
Categories: cs.NE cs.AR cs.ET
Comments: 8 pages, 13 figures, accepted in 16th European Dependable Computing
  Conference
\\
  As process technology continues to scale aggressively, circuit aging in a
neuromorphic hardware due to negative bias temperature instability (NBTI) and
time-dependent dielectric breakdown (TDDB) is becoming a critical reliability
issue and is expected to proliferate when using non-volatile memory (NVM) for
synaptic storage. This is because an NVM requires high voltage and current to
access its synaptic weight, which further accelerates the circuit aging in a
neuromorphic hardware. Current methods for qualifying reliability are overly
conservative, since they estimate circuit aging considering worst-case
operating conditions and unnecessarily constrain performance. This paper
proposes RENEU, a reliability-oriented approach to map machine learning
applications to neuromorphic hardware, with the aim of improving system-wide
reliability without compromising key performance metrics such as execution time
of these applications on the hardware. Fundamental to RENEU is a novel
formulation of the aging of CMOS-based circuits in a neuromorphic hardware
considering different failure mechanisms. Using this formulation, RENEU
develops a system-wide reliability model which can be used inside a
design-space exploration framework involving the mapping of neurons and
synapses to the hardware. To this end, RENEU uses an instance of Particle Swarm
Optimization (PSO) to generate mappings that are Pareto-optimal in terms of
performance and reliability. We evaluate RENEU using different machine learning
applications on a state-of-the-art neuromorphic hardware with NVM synapses. Our
results demonstrate an average 38\% reduction in circuit aging, leading to an
average 18% improvement in the lifetime of the hardware compared to current
practices. RENEU only introduces a marginal performance overhead of 5% compared
to a performance-oriented state-of-the-art.
\\ ( https://arxiv.org/abs/2006.05868 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04951 (*cross-listing*)
Date: Tue, 2 Jun 2020 17:32:32 GMT   (1210kb,D)

Title: Network visualizations with Pyvis and VisJS
Authors: Giancarlo Perrone, Jose Unpingco, Haw-minn Lu
Categories: cs.SI cs.GR
Comments: Accepted and submitted to 19th Python in Science Conference. (SciPy
  2020)
\\
  Pyvis is a Python module that enables visualizing and interactively
manipulating network graphs in the Jupyter notebook, or as a standalone web
application. Pyvis is built on top of the powerful and mature VisJS JavaScript
library, which allows for fast and responsive interactions while also
abstracting away the low-level JavaScript and HTML. This means that elements of
the rendered graph visualization, such as node/edge attributes can be specified
within Python and shipped to the JavaScript layer for VisJS to render. This
declarative approach makes it easy to quickly explore graph visualizations and
investigate data relationships. In addition, Pyvis is highly customizable so
that colors, sizes, and hover tooltips can be assigned to the rendered graph.
The network graph layout is controlled by a front-end physics engine that is
configurable from a Python interface, allowing for the detailed placement of
the graph elements. In this paper, we outline use cases for Pyvis with specific
examples to highlight key features for any analysis workflow. A brief overview
of Pyvis' implementation describes how the Python front-end binding uses simple
Pyvis calls.
\\ ( https://arxiv.org/abs/2006.04951 ,  1210kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1912.03033
replaced with revised version Wed, 10 Jun 2020 16:00:42 GMT   (2808kb,D)

Title: Recovering the homology of immersed manifolds
Authors: Rapha\"el Tinarrage (DATASHAPE, LMO)
Categories: cs.CG math.DG
\\ ( https://arxiv.org/abs/1912.03033 ,  2808kb)
------------------------------------------------------------------------------
\\
arXiv:1904.07220
replaced with revised version Mon, 8 Jun 2020 18:31:34 GMT   (1527kb,D)

Title: Learning Discriminative Model Prediction for Tracking
Authors: Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte
Categories: cs.CV
\\ ( https://arxiv.org/abs/1904.07220 ,  1527kb)
------------------------------------------------------------------------------
\\
arXiv:1908.07323
replaced with revised version Wed, 10 Jun 2020 01:42:50 GMT   (1499kb,D)

Title: Instance Scale Normalization for image understanding
Authors: Zewen He, He Huang, Yudong Wu, Guan Huang, Wensheng Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/1908.07323 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:1910.11102
replaced with revised version Tue, 9 Jun 2020 04:13:30 GMT   (207kb,D)

Title: Vatex Video Captioning Challenge 2020: Multi-View Features and Hybrid
  Reward Strategies for Video Captioning
Authors: Xinxin Zhu, Longteng Guo, Peng Yao, Shichen Lu, Wei Liu, Jing Liu
Categories: cs.CV cs.MM
Comments: 4 pages,2 figure
\\ ( https://arxiv.org/abs/1910.11102 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:1910.14184
replaced with revised version Tue, 9 Jun 2020 13:29:31 GMT   (1433kb,D)

Title: Beyond Universal Person Re-ID Attack
Authors: Wenjie Ding, Xing Wei, Yunfeng Qiu, Rongrong Ji, Xiaopeng Hong, Yihong
  Gong
Categories: cs.CV
\\ ( https://arxiv.org/abs/1910.14184 ,  1433kb)
------------------------------------------------------------------------------
\\
arXiv:1911.02683
replaced with revised version Mon, 8 Jun 2020 18:35:31 GMT   (586kb,D)

Title: Shaping Visual Representations with Language for Few-shot Classification
Authors: Jesse Mu, Percy Liang, Noah Goodman
Categories: cs.CV cs.CL
Comments: ACL 2020. Version 1 appeared at the NeurIPS 2019 Workshop on Visually
  Grounded Interaction and Language (ViGIL)
\\ ( https://arxiv.org/abs/1911.02683 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:1911.07034
replaced with revised version Tue, 9 Jun 2020 08:38:33 GMT   (7208kb,D)

Title: Instance Shadow Detection
Authors: Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, and Chi-Wing Fu
Categories: cs.CV
Comments: Accepted to CVPR 2020
\\ ( https://arxiv.org/abs/1911.07034 ,  7208kb)
------------------------------------------------------------------------------
\\
arXiv:1912.00998
replaced with revised version Wed, 10 Jun 2020 03:05:26 GMT   (37kb)

Title: A Multigrid Method for Efficiently Training Video Models
Authors: Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer,
  Philipp Kr\"ahenb\"uhl
Categories: cs.CV
Comments: CVPR 2020
\\ ( https://arxiv.org/abs/1912.00998 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:1912.05014
replaced with revised version Tue, 9 Jun 2020 23:48:47 GMT   (710kb,D)

Title: Hybrid Style Siamese Network: Incorporating style loss in complementary
  apparels retrieval
Authors: Mayukh Bhattacharyya, Sayan Nag
Categories: cs.CV cs.LG cs.MM
Comments: Paper Accepted in the Third Workshop on Computer Vision for Fashion,
  Art and Design, CVPR 2020
\\ ( https://arxiv.org/abs/1912.05014 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:1912.12270
replaced with revised version Tue, 9 Jun 2020 20:51:23 GMT   (6101kb,D)

Title: Combining Deep Learning and Verification for Precise Object Instance
  Detection
Authors: Siddharth Ancha, Junyu Nan, David Held
Categories: cs.CV cs.LG eess.IV
Comments: 9 pages main paper, 2 pages references, 10 pages supplementary
  material
Journal-ref: Conference on Robot Learning (CoRL), 2019
\\ ( https://arxiv.org/abs/1912.12270 ,  6101kb)
------------------------------------------------------------------------------
\\
arXiv:2001.09598
replaced with revised version Tue, 9 Jun 2020 12:39:14 GMT   (2636kb,D)

Title: FakeLocator: Robust Localization of GAN-Based Face Manipulations
Authors: Yihao Huang, Felix Juefei-Xu, Run Wang, Qing Guo, Xiaofei Xie, Lei Ma,
  Jianwen Li, Weikai Miao, Yang Liu, Geguang Pu
Categories: cs.CV cs.LG
Comments: 9 pages
\\ ( https://arxiv.org/abs/2001.09598 ,  2636kb)
------------------------------------------------------------------------------
\\
arXiv:2003.05549
replaced with revised version Tue, 9 Jun 2020 18:37:09 GMT   (5289kb,D)

Title: Frequency-Tuned Universal Adversarial Attacks
Authors: Yingpeng Deng and Lina J. Karam
Categories: cs.CV
\\ ( https://arxiv.org/abs/2003.05549 ,  5289kb)
------------------------------------------------------------------------------
\\
arXiv:2003.08040
replaced with revised version Tue, 9 Jun 2020 17:56:27 GMT   (770kb,D)

Title: Differential Treatment for Stuff and Things: A Simple Unsupervised
  Domain Adaptation Method for Semantic Segmentation
Authors: Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong,
  Wen-mei Hwu, Thomas S. Huang, Humphrey Shi
Categories: cs.CV cs.LG eess.IV
Comments: CVPR 2020
\\ ( https://arxiv.org/abs/2003.08040 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2003.09005
replaced with revised version Tue, 9 Jun 2020 14:11:06 GMT   (8685kb,D)

Title: Semi-Supervised Semantic Segmentation with Cross-Consistency Training
Authors: Yassine Ouali, C\'eline Hudelot, Myriam Tami
Categories: cs.CV
Comments: Published at CVPR 2020
\\ ( https://arxiv.org/abs/2003.09005 ,  8685kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13630
replaced with revised version Tue, 9 Jun 2020 06:16:27 GMT   (894kb,D)

Title: TResNet: High Performance GPU-Dedicated Architecture
Authors: Tal Ridnik, Hussam Lawen, Asaf Noy, Itamar Friedman, Emanuel Ben
  Baruch, Gilad Sharir
Categories: cs.CV cs.LG eess.IV
Comments: 11 pages, 5 figures
\\ ( https://arxiv.org/abs/2003.13630 ,  894kb)
------------------------------------------------------------------------------
\\
arXiv:2004.00794
replaced with revised version Tue, 9 Jun 2020 22:38:27 GMT   (5005kb,D)

Title: Alleviating Semantic-level Shift: A Semi-supervised Domain Adaptation
  Method for Semantic Segmentation
Authors: Zhonghao Wang, Yunchao Wei, Rogerior Feris, Jinjun Xiong, Wen-Mei Hwu,
  Thomas S. Huang, Humphrey Shi
Categories: cs.CV cs.LG
Comments: CVPRW 2020
\\ ( https://arxiv.org/abs/2004.00794 ,  5005kb)
------------------------------------------------------------------------------
\\
arXiv:2004.01459
replaced with revised version Wed, 10 Jun 2020 16:50:39 GMT   (2387kb,D)

Title: Self-Paced Deep Regression Forests with Consideration on
  Underrepresented Samples
Authors: Lili Pan, Shijie Ai, Yazhou Ren and Zenglin Xu
Categories: cs.CV
Comments: 18 pages, 6 figures
\\ ( https://arxiv.org/abs/2004.01459 ,  2387kb)
------------------------------------------------------------------------------
\\
arXiv:2004.04727
replaced with revised version Wed, 10 Jun 2020 14:21:03 GMT   (8197kb,D)

Title: 3D Photography using Context-aware Layered Depth Inpainting
Authors: Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang
Categories: cs.CV eess.IV
Comments: CVPR 2020. Project page:
  https://shihmengli.github.io/3D-Photo-Inpainting/ Code:
  https://github.com/vt-vl-lab/3d-photo-inpainting Demo:
  https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz
\\ ( https://arxiv.org/abs/2004.04727 ,  8197kb)
------------------------------------------------------------------------------
\\
arXiv:2004.14143
replaced with revised version Tue, 9 Jun 2020 12:04:17 GMT   (8266kb,D)

Title: Zero-Shot Learning and its Applications from Autonomous Vehicles to
  COVID-19 Diagnosis: A Review
Authors: Mahdi Rezaei and Mahsa Shahidi
Categories: cs.CV cs.LG stat.ML
Comments: Version 2.0
\\ ( https://arxiv.org/abs/2004.14143 ,  8266kb)
------------------------------------------------------------------------------
\\
arXiv:2005.02159
replaced with revised version Mon, 8 Jun 2020 18:51:57 GMT   (1353kb,D)

Title: A fast and memory-efficient algorithm for smooth interpolation of
  polyrigid transformations: application to human joint tracking
Authors: K. Makki, B. Borotikar, M. Garetier, S. Brochard, D. Ben Salem, F.
  Rousseau
Categories: cs.CV math.GR
\\ ( https://arxiv.org/abs/2005.02159 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2005.08501
replaced with revised version Wed, 10 Jun 2020 07:09:15 GMT   (5406kb,D)

Title: VecQ: Minimal Loss DNN Model Compression With Vectorized Weight
  Quantization
Authors: Cheng Gong, Yao Chen, Ye Lu, Tao Li, Cong Hao, Deming Chen
Categories: cs.CV
Comments: 14 pages, 9 figures, Journal
DOI: 10.1109/TC.2020.2995593
\\ ( https://arxiv.org/abs/2005.08501 ,  5406kb)
------------------------------------------------------------------------------
\\
arXiv:2005.11035
replaced with revised version Wed, 10 Jun 2020 01:22:47 GMT   (1405kb,D)

Title: Position-based Scaled Gradient for Model Quantization and Sparse
  Training
Authors: Jangho Kim, KiYoon Yoo, Nojun Kwak
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2005.11035 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2005.12155
replaced with revised version Tue, 9 Jun 2020 08:10:33 GMT   (1344kb,D)

Title: AGVNet: Attention Guided Velocity Learning for 3D Human Motion
  Prediction
Authors: Xiaoli Liu, Jianqin Yin, Huaping Liu, Jun Liu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2005.12155 ,  1344kb)
------------------------------------------------------------------------------
\\
arXiv:2005.13983
replaced with revised version Wed, 10 Jun 2020 09:32:54 GMT   (7016kb,D)

Title: Uncertainty-Aware Blind Image Quality Assessment in the Laboratory and
  Wild
Authors: Weixia Zhang and Kede Ma and Guangtao Zhai and Xiaokang Yang
Categories: cs.CV cs.LG cs.MM eess.IV
Comments: Under review. The implementations are available at
  https://github.com/zwx8981/UNIQUE
\\ ( https://arxiv.org/abs/2005.13983 ,  7016kb)
------------------------------------------------------------------------------
\\
arXiv:2006.03810
replaced with revised version Tue, 9 Jun 2020 13:01:00 GMT   (4109kb,D)

Title: An Empirical Analysis of the Impact of Data Augmentation on Knowledge
  Distillation
Authors: Deepan Das, Haley Massa, Abhimanyu Kulkarni, Theodoros Rekatsinas
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2006.03810 ,  4109kb)
------------------------------------------------------------------------------
\\
arXiv:2006.03876
replaced with revised version Tue, 9 Jun 2020 02:34:52 GMT   (3068kb,D)

Title: ARID: A New Dataset for Recognizing Action in the Dark
Authors: Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin and
  Simon See
Categories: cs.CV
Comments: 6 pages, 7 figures, Data available at https://xuyu0010.github.io/arid
\\ ( https://arxiv.org/abs/2006.03876 ,  3068kb)
------------------------------------------------------------------------------
\\
arXiv:2006.03919
replaced with revised version Tue, 9 Jun 2020 03:06:11 GMT   (3236kb,D)

Title: A Robust Attentional Framework for License Plate Recognition in the Wild
Authors: Linjiang Zhang, Peng Wang, Hui Li, Zhen Li, Chunhua Shen, Yanning
  Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.03919 ,  3236kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04082
replaced with revised version Tue, 9 Jun 2020 07:40:51 GMT   (2790kb,D)

Title: End-to-end Learning for Inter-Vehicle Distance and Relative Velocity
  Estimation in ADAS with a Monocular Camera
Authors: Zhenbo Song, Jianfeng Lu, Tong Zhang, Hongdong Li
Categories: cs.CV cs.LG cs.RO
\\ ( https://arxiv.org/abs/2006.04082 ,  2790kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04323
replaced with revised version Tue, 9 Jun 2020 07:52:51 GMT   (54kb,D)

Title: Ensemble Model with Batch Spectral Regularization and Data Blending for
  Cross-Domain Few-Shot Learning with Unlabeled Data
Authors: Zhen Zhao, Bingyu Liu, Yuhong Guo, Jieping Ye
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.04323 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04604
replaced with revised version Tue, 9 Jun 2020 18:22:03 GMT   (5229kb,D)

Title: SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds
Authors: Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, Nam Soo
  Kim
Categories: cs.CV cs.LG
Comments: 17 pages, 15figures
\\ ( https://arxiv.org/abs/2006.04604 ,  5229kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04719
replaced with revised version Tue, 9 Jun 2020 11:39:18 GMT   (579kb,D)

Title: ResKD: Residual-Guided Knowledge Distillation
Authors: Xuewei Li, Songyuan Li, Bourahla Omar, and Xi Li
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.04719 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:1907.02494
replaced with revised version Tue, 9 Jun 2020 19:26:57 GMT   (226kb,D)

Title: Packing Directed Cycles Quarter- and Half-Integrally
Authors: Tom\'a\v{s} Masa\v{r}\'ik and Irene Muzi and Marcin Pilipczuk and
  Pawe{\l} Rz\k{a}\.zewski and Manuel Sorge
Categories: cs.DM math.CO
Comments: Accepted to European Symposium on Algorithms (ESA '19)
\\ ( https://arxiv.org/abs/1907.02494 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2002.07168
replaced with revised version Tue, 9 Jun 2020 07:59:05 GMT   (88kb,D)

Title: Density of Binary Disc Packings: The 9 Compact Packings
Authors: Thomas Fernique and Nicolas B\'edaride
Categories: cs.DM math.MG
Comments: 23 pages, 8 figures, code (SageMath) included in the source archive.
  arXiv admin note: text overlap with arXiv:1912.02297
MSC-class: 52C15
\\ ( https://arxiv.org/abs/2002.07168 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:1906.07631
replaced with revised version Mon, 8 Jun 2020 22:20:33 GMT   (28901kb,D)

Title: Topologically robust CAD model generation for structural optimisation
Authors: Ge Yin, Xiao Xiao, Fehmi Cirak
Categories: cs.GR cs.CE cs.CG
\\ ( https://arxiv.org/abs/1906.07631 ,  28901kb)
------------------------------------------------------------------------------
\\
arXiv:1912.12399 (*cross-listing*)
replaced with revised version Tue, 9 Jun 2020 04:23:44 GMT   (97kb,D)

Title: Persistent Homotopy Groups of Metric Spaces
Authors: Facundo M\'emoli, Ling Zhou
Categories: math.AT cs.CG
Comments: 60 pages. This version updates some details of Theorem 5.12 (Theorem
  5.11 in the previous version)
\\ ( https://arxiv.org/abs/1912.12399 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2002.08477
replaced with revised version Tue, 9 Jun 2020 18:41:34 GMT   (883kb,D)

Title: Optimally Guarding Perimeters and Regions with Mobile Range Sensors
Authors: Si Wei Feng and Jingjin Yu
Categories: cs.RO cs.CG
\\ ( https://arxiv.org/abs/2002.08477 ,  883kb)
------------------------------------------------------------------------------
\\
arXiv:1903.11508
replaced with revised version Wed, 10 Jun 2020 12:20:04 GMT   (796kb,D)

Title: Text Processing Like Humans Do: Visually Attacking and Shielding NLP
  Systems
Authors: Steffen Eger and G\"ozde G\"ul \c{S}ahin and Andreas R\"uckl\'e and
  Ji-Ung Lee and Claudia Schulz and Mohsen Mesgar and Krishnkant Swarnkar and
  Edwin Simpson and Iryna Gurevych
Categories: cs.CL cs.CR cs.CV cs.LG
Comments: Accepted as long paper at NAACL-2019; fixed one ungrammatical
  sentence
\\ ( https://arxiv.org/abs/1903.11508 ,  796kb)
------------------------------------------------------------------------------
\\
arXiv:1904.06194
replaced with revised version Wed, 10 Jun 2020 03:26:01 GMT   (110kb,D)

Title: Compressing deep neural networks by matrix product operators
Authors: Ze-Feng Gao, Song Cheng, Rong-Qiang He, Z. Y. Xie, Hui-Hai Zhao,
  Zhong-Yi Lu, Tao Xiang
Categories: cs.LG cs.CV cs.NE physics.comp-ph quant-ph
Comments: 8+9 pages, 3+7 figures, 2+11 tables
Journal-ref: PhysRevResearch.2.023300 (2020)
DOI: 10.1103/PhysRevResearch.2.023300
\\ ( https://arxiv.org/abs/1904.06194 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:1906.02909
replaced with revised version Wed, 10 Jun 2020 17:06:41 GMT   (2950kb,D)

Title: AutoGrow: Automatic Layer Growing in Deep Convolutional Networks
Authors: Wei Wen, Feng Yan, Yiran Chen, Hai Li
Categories: cs.LG cs.CV cs.NE stat.ML
Comments: KDD 2020
\\ ( https://arxiv.org/abs/1906.02909 ,  2950kb)
------------------------------------------------------------------------------
\\
arXiv:1908.08016
replaced with revised version Tue, 9 Jun 2020 05:17:48 GMT   (4753kb,D)

Title: Testing Robustness Against Unforeseen Adversaries
Authors: Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, Jacob Steinhardt
Categories: cs.LG cs.CR cs.CV stat.ML
\\ ( https://arxiv.org/abs/1908.08016 ,  4753kb)
------------------------------------------------------------------------------
\\
arXiv:1911.06479
replaced with revised version Wed, 10 Jun 2020 05:26:51 GMT   (0kb,I)

Title: On Model Robustness Against Adversarial Examples
Authors: Shufei Zhang and Kaizhu Huang and Zenglin Xu
Categories: cs.LG cs.CR cs.CV stat.ML
Comments: some theoretical bounds need to be revised
\\ ( https://arxiv.org/abs/1911.06479 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:1911.09816 (*cross-listing*)
replaced with revised version Wed, 10 Jun 2020 08:09:29 GMT   (3835kb,D)

Title: Two-stage dimension reduction for noisy high-dimensional images and
  application to Cryogenic Electron Microscopy
Authors: Szu-Chi Chung, Shao-Hsuan Wang, Po-Yao Niu, Su-Yun Huang, Wei-Hau
  Chang and I-Ping Tu
Categories: eess.IV cs.CV stat.AP
Comments: 29 pages, 8 figures and 3 tables
\\ ( https://arxiv.org/abs/1911.09816 ,  3835kb)
------------------------------------------------------------------------------
\\
arXiv:1912.09597 (*cross-listing*)
replaced with revised version Wed, 10 Jun 2020 16:24:01 GMT   (622kb,D)

Title: Non-congruent non-degenerate curves with identical signatures
Authors: Eric Geiger, Irina A. Kogan
Categories: math.DG cs.CV
Comments: 32 pages, 26 figures. Updated to include results on local symmetries
  and improvements to presentation
MSC-class: 53A04, 53A55, 68T45
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/1912.09597 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2001.02728
replaced with revised version Tue, 9 Jun 2020 21:26:44 GMT   (6471kb,D)

Title: Learning Generative Models using Denoising Density Estimators
Authors: Siavash A. Bigdeli, Geng Lin, Tiziano Portenier, L. Andrea Dunbar,
  Matthias Zwicker
Categories: cs.LG cs.CV stat.ML
Comments: Code and models available at
  https://drive.google.com/file/d/1EzKRxnFG1Hd8g6Ggvt-jvKkgpDDwK2bY
\\ ( https://arxiv.org/abs/2001.02728 ,  6471kb)
------------------------------------------------------------------------------
\\
arXiv:2002.12398
replaced with revised version Tue, 9 Jun 2020 16:20:43 GMT   (328kb,D)

Title: Provable Robust Learning Based on Transformation-Specific Smoothing
Authors: Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Tao Xie, Ce Zhang,
  Bo Li
Categories: cs.LG cs.CV stat.ML
Comments: 58 pages, 7 figures
\\ ( https://arxiv.org/abs/2002.12398 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13198
replaced with revised version Wed, 10 Jun 2020 02:14:00 GMT   (1155kb,D)

Title: InterBERT: An Effective Multi-Modal Pretraining Approach via
  Vision-and-Language Interaction
Authors: Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia
  Yang
Categories: cs.CL cs.CV
Comments: 15 pages
\\ ( https://arxiv.org/abs/2003.13198 ,  1155kb)
------------------------------------------------------------------------------
\\
arXiv:2005.04790
replaced with revised version Mon, 8 Jun 2020 18:01:54 GMT   (2031kb,D)

Title: The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes
Authors: Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet
  Singh, Pratik Ringshia, Davide Testuggine
Categories: cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2005.04790 ,  2031kb)
------------------------------------------------------------------------------
\\
arXiv:2006.01385 (*cross-listing*)
replaced with revised version Tue, 9 Jun 2020 18:15:47 GMT   (5250kb)

Title: Adaptive convolutional neural networks for k-space data interpolation in
  fast magnetic resonance imaging
Authors: Tianming Du, Honggang Zhang, Yuemeng Li, Hee Kwon Song, Yong Fan
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2006.01385 ,  5250kb)
------------------------------------------------------------------------------
\\
arXiv:2006.01895
replaced with revised version Tue, 9 Jun 2020 05:18:55 GMT   (2373kb,D)

Title: Learning to Branch for Multi-Task Learning
Authors: Pengsheng Guo, Chen-Yu Lee, Daniel Ulbricht
Categories: cs.LG cs.CV stat.ML
Comments: Accepted at ICML 2020
\\ ( https://arxiv.org/abs/2006.01895 ,  2373kb)
------------------------------------------------------------------------------
\\
arXiv:2006.02570 (*cross-listing*)
replaced with revised version Wed, 10 Jun 2020 16:01:34 GMT   (9285kb,D)

Title: Exploration of Interpretability Techniques for Deep COVID-19
  Classification using Chest X-ray Images
Authors: Soumick Chatterjee, Fatima Saad, Chompunuch Sarasaen, Suhita Ghosh,
  Rupali Khatun, Petia Radeva, Georg Rose, Sebastian Stober, Oliver Speck,
  Andreas N\"urnberger
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.02570 ,  9285kb)
------------------------------------------------------------------------------
\\
arXiv:1912.07965 (*cross-listing*)
replaced with revised version Mon, 8 Jun 2020 20:35:03 GMT   (14kb)

Title: Erd\H{o}s-P\'osa from ball packing
Authors: Wouter Cames van Batenburg and Gwena\"el Joret and Arthur Ulmer
Categories: math.CO cs.DM
Comments: v4: Minor change v3: Referees' comments implemented v2: Additional
  references to prior works
\\ ( https://arxiv.org/abs/1912.07965 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2001.00514 (*cross-listing*)
replaced with revised version Mon, 8 Jun 2020 20:08:58 GMT   (95kb,D)

Title: Piecewise Polyhedral Formulations for a Multilinear Term
Authors: Kaarthik Sundar, Harsha Nagarajan, Jeff Linderoth, Site Wang, Russell
  Bent
Categories: math.OC cs.DM
\\ ( https://arxiv.org/abs/2001.00514 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2002.03317
replaced with revised version Wed, 10 Jun 2020 13:18:21 GMT   (131kb,D)

Title: Reed-Muller Codes: Theory and Algorithms
Authors: Emmanuel Abbe and Amir Shpilka and Min Ye
Categories: cs.IT cs.DM math.IT
\\ ( https://arxiv.org/abs/2002.03317 ,  131kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
