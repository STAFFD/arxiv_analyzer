Delivered-To: brucelu2013@gmail.com
Received: by 2002:a54:2e8d:0:0:0:0:0 with SMTP id s13csp118368ecp;
        Tue, 28 Jul 2020 02:27:48 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJzVu5u+iRTzGzdHAwS24IcDvKU7PjEHHJ8e5T2QodIibPEaBkeSzO3M7/gJ8ZBkU093N08B
X-Received: by 2002:a0c:e591:: with SMTP id t17mr7028339qvm.74.1595928468185;
        Tue, 28 Jul 2020 02:27:48 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1595928468; cv=none;
        d=google.com; s=arc-20160816;
        b=huYj+WLu7iadcn6NCR09im3xiltk9MXJXaFw62vqDv6JtCwqvvw5cYXD221UAc93K/
         FBSXmPMJD+uFAq0o4xdb5A91PGJ2FODBO5yls4cv0QWnCFkNt2TfIKbEVAjLCGZzhqj3
         D0AoB1lpPyXqso0o4Hb56QyqhCxgyCCN/y4TjfSGnEB5B+jw+jFKMWyNld6ZsEe2QmbW
         qoGD9FrHQv83pEwI1An6So3lOFZcf9LwC5qY3wGOglhyFhDMt16ts4JYC8Wz7Yq04XtB
         LGvwLWOh5ivSrCcUb9y0TqEwPdT4RYz9QPgZq/+cRBNi3Gbgy5sApTypns9m3zQ3NgRM
         rNnQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=wES86PhsEma10pImFE4vpSAcvHob8wN/D3OxcYzxB0k=;
        b=Din4+4xkFKsi38Icrj9e2jt5kuQTOcEvYO4rv63Dvf3ihSMue5TxR74ZrLOgsKhLlW
         fFnIHP9BeX2QmQmHrBVaFUeR6dsMXWcpD2Mc3w3v1lXcZIBIpqoim6NnwLQ0AbVjs0+3
         R7uhzu9gh3TzMXafNXmQE6Zj5cSoXk4Gc8bIR7qy85SaHIU4hDqwe3ulJgPPPZ1b13Bk
         3gZg+Z0JKiB9dw0m+4jb3o8ltKQ7+N6JqCkJ8IpFE6OVkdIfXckAAA4AMXUI70pLj9dW
         wQmPcOcDD640BWbQMDYmVmYqe6idJ26fI2ZHJRVTKGfdm0VZXT84cU6C0SfeDrT3IEwZ
         lKrA==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id k4si12632308qko.130.2020.07.28.02.27.47
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 28 Jul 2020 02:27:48 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 06S9RlJf010961;
	Tue, 28 Jul 2020 05:27:47 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 06S9RlwI034548;
	Tue, 28 Jul 2020 05:27:47 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 06S9RkjX034547;
	Tue, 28 Jul 2020 05:27:46 -0400
Date: Tue, 28 Jul 2020 05:27:46 -0400
Message-Id: <202007280927.06S9RkjX034547@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing a840 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computer Vision and Pattern Recognition
Discrete Mathematics
Emerging Technologies
Graphics
 received from  Fri 24 Jul 20 18:00:00 GMT  to  Mon 27 Jul 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2007.12749
Date: Fri, 24 Jul 2020 19:34:58 GMT   (30414kb,D)

Title: Hard negative examples are hard, but useful
Authors: Hong Xuan, Abby Stylianou, Xiaotong Liu, Robert Pless
Categories: cs.CV cs.AI
Comments: CV, Triplet loss, Image embedding, 14 pages, 9 figures, ECCV 2020
\\
  Triplet loss is an extremely common approach to distance metric learning.
Representations of images from the same class are optimized to be mapped closer
together in an embedding space than representations of images from different
classes. Much work on triplet losses focuses on selecting the most useful
triplets of images to consider, with strategies that select dissimilar examples
from the same class or similar examples from different classes. The consensus
of previous research is that optimizing with the \textit{hardest} negative
examples leads to bad training behavior. That's a problem -- these hardest
negatives are literally the cases where the distance metric fails to capture
semantic similarity. In this paper, we characterize the space of triplets and
derive why hard negatives make triplet loss training fail. We offer a simple
fix to the loss function and show that, with this fix, optimizing with hard
negative examples becomes feasible. This leads to more generalizable features,
and image retrieval results that outperform state of the art for datasets with
high intra-class variance.
\\ ( https://arxiv.org/abs/2007.12749 ,  30414kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12750
Date: Fri, 24 Jul 2020 19:35:57 GMT   (23851kb,D)

Title: Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data
Authors: Michael Cogswell, Jiasen Lu, Rishabh Jain, Stefan Lee, Devi Parikh,
  Dhruv Batra
Categories: cs.CV cs.AI cs.CL
Comments: 19 pages, 8 figures
\\
  Can we develop visually grounded dialog agents that can efficiently adapt to
new tasks without forgetting how to talk to people? Such agents could leverage
a larger variety of existing data to generalize to new tasks, minimizing
expensive data collection and annotation. In this work, we study a setting we
call "Dialog without Dialog", which requires agents to develop visually
grounded dialog models that can adapt to new tasks without language level
supervision. By factorizing intention and language, our model minimizes
linguistic drift after fine-tuning for new tasks. We present qualitative
results, automated metrics, and human studies that all show our model can adapt
to new tasks and maintain language quality. Baselines either fail to perform
well at new tasks or experience language drift, becoming unintelligible to
humans. Code has been made available at
https://github.com/mcogswell/dialog_without_dialog
\\ ( https://arxiv.org/abs/2007.12750 ,  23851kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12806
Date: Fri, 24 Jul 2020 23:50:46 GMT   (6754kb,D)

Title: Spatiotemporal Bundle Adjustment for Dynamic 3D Human Reconstruction in
  the Wild
Authors: Minh Vo, Yaser Sheikh, and Srinivasa G. Narasimhan
Categories: cs.CV
Comments: Accepted to IEEE TPAMI
\\
  Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D
point triangulation to reconstruct a static scene. The triangulation
constraint, however, is invalid for moving points captured in multiple
unsynchronized videos and bundle adjustment is not designed to estimate the
temporal alignment between cameras. We present a spatiotemporal bundle
adjustment framework that jointly optimizes four coupled sub-problems:
estimating camera intrinsics and extrinsics, triangulating static 3D points, as
well as sub-frame temporal alignment between cameras and computing 3D
trajectories of dynamic points. Key to our joint optimization is the careful
integration of physics-based motion priors within the reconstruction pipeline,
validated on a large motion capture corpus of human subjects. We devise an
incremental reconstruction and alignment algorithm to strictly enforce the
motion prior during the spatiotemporal bundle adjustment. This algorithm is
further made more efficient by a divide and conquer scheme while still
maintaining high accuracy. We apply this algorithm to reconstruct 3D motion
trajectories of human bodies in dynamic events captured by multiple
uncalibrated and unsynchronized video cameras in the wild. To make the
reconstruction visually more interpretable, we fit a statistical 3D human body
model to the asynchronous video streams.Compared to the baseline, the fitting
significantly benefits from the proposed spatiotemporal bundle adjustment
procedure. Because the videos are aligned with sub-frame precision, we
reconstruct 3D motion at much higher temporal resolution than the input videos.
\\ ( https://arxiv.org/abs/2007.12806 ,  6754kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12808
Date: Fri, 24 Jul 2020 23:52:03 GMT   (554kb,D)

Title: Counting Fish and Dolphins in Sonar Images Using Deep Learning
Authors: Stefan Schneider and Alex Zhuang
Categories: cs.CV
Comments: 19 pages, 5 figures, 1 table
\\
  Deep learning provides the opportunity to improve upon conflicting reports
considering the relationship between the Amazon river's fish and dolphin
abundance and reduced canopy cover as a result of deforestation. Current
methods of fish and dolphin abundance estimates are performed by on-site
sampling using visual and capture/release strategies. We propose a novel
approach to calculating fish abundance using deep learning for fish and dolphin
estimates from sonar images taken from the back of a trolling boat. We consider
a data set of 143 images ranging from 0-34 fish, and 0-3 dolphins provided by
the Fund Amazonia research group. To overcome the data limitation, we test the
capabilities of data augmentation on an unconventional 15/85 training/testing
split. Using 20 training images, we simulate a gradient of data up to 25,000
images using augmented backgrounds and randomly placed/rotation cropped fish
and dolphin taken from the training set. We then train four multitask network
architectures: DenseNet201, InceptionNetV2, Xception, and MobileNetV2 to
predict fish and dolphin numbers using two function approximation methods:
regression and classification. For regression, Densenet201 performed best for
fish and Xception best for dolphin with mean squared errors of 2.11 and 0.133
respectively. For classification, InceptionResNetV2 performed best for fish and
MobileNetV2 best for dolphins with a mean error of 2.07 and 0.245 respectively.
Considering the 123 testing images, our results show the success of data
simulation for limited sonar data sets. We find DenseNet201 is able to identify
dolphins after approximately 5000 training images, while fish required the full
25,000. Our method can be used to lower costs and expedite the data analysis of
fish and dolphin abundance to real-time along the Amazon river and river
systems worldwide.
\\ ( https://arxiv.org/abs/2007.12808 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12831
Date: Sat, 25 Jul 2020 02:14:42 GMT   (7264kb,D)

Title: A Self-Training Approach for Point-Supervised Object Detection and
  Counting in Crowds
Authors: Yi Wang, Junhui Hou, Xinyu Hou, and Lap-Pui Chau
Categories: cs.CV
Comments: 10 pages
\\
  In this paper, we propose a novel self-training approach which enables a
typical object detector trained only with point-level annotations (i.e.,
objects are labeled with points) to estimate both the center points and sizes
of crowded objects. Specifically, during training we utilize the available
point annotations to directly supervise the estimation of the center points of
objects. Based on a locally-uniform distribution assumption, we initialize
pseudo object sizes from the point-level supervisory information, which are
then leveraged to guide the regression of object sizes via a crowdedness-aware
loss. Meanwhile, we propose a confidence and order-aware refinement scheme to
continuously refine the initial pseudo object sizes such that the ability of
the detector is increasingly boosted to simultaneously detect and count objects
in crowds. Moreover, to address extremely crowded scenes, we propose an
effective decoding method to improve the representation ability of the
detector. Experimental results on the WiderFace benchmark show that our
approach significantly outperforms state-of-the-art point-supervised methods
under both detection and counting tasks, i.e., our method improves the average
precision by more than 10% and reduces the counting error by 31.2%. In
addition, our method obtains the best results on the dense crowd counting
dataset (i.e., ShanghaiTech) and vehicle counting datasets (i.e., CARPK and
PUCPR+) when compared with state-of-the-art counting-by-detection methods. We
will make the code publicly available to facilitate future research.
\\ ( https://arxiv.org/abs/2007.12831 ,  7264kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12868
Date: Sat, 25 Jul 2020 06:48:47 GMT   (15186kb,D)

Title: OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene
  Datasets
Authors: Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Sai Bi, Zexiang Xu,
  Hong-Xing Yu, Kalyan Sunkavalli, Milo\v{s} Ha\v{s}an, Ravi Ramamoorthi,
  Manmohan Chandraker
Categories: cs.CV
\\
  Large-scale photorealistic datasets of indoor scenes, with ground truth
geometry, materials and lighting, are important for deep learning applications
in scene reconstruction and augmented reality. The associated shape, material
and lighting assets can be scanned or artist-created, both of which are
expensive; the resulting data is usually proprietary. We aim to make the
dataset creation process for indoor scenes widely accessible, allowing
researchers to transform casually acquired scans to large-scale datasets with
high-quality ground truth. We achieve this by estimating consistent furniture
and scene layout, ascribing high quality materials to all surfaces and
rendering images with spatially-varying lighting consisting of area lights and
environment maps. We demonstrate an instantiation of our approach on the
publicly available ScanNet dataset. Deep networks trained on our proposed
dataset achieve competitive performance for shape, material and lighting
estimation on real images and can be used for photorealistic augmented reality
applications, such as object insertion and material editing. Importantly, the
dataset and all the tools to create such datasets from scans will be released,
enabling others in the community to easily build large-scale datasets of their
own. All code, models, data, dataset creation tool will be publicly released on
our project page.
\\ ( https://arxiv.org/abs/2007.12868 ,  15186kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12869
Date: Sat, 25 Jul 2020 07:07:23 GMT   (381kb)

Title: Applying Semantic Segmentation to Autonomous Cars in the Snowy
  Environment
Authors: Zhaoyu Pan, Takanori Emaru, Ankit Ravankar, Yukinori Kobayashi
Categories: cs.CV
Comments: 4 pages, 5 Figures
Report-no: 18A1853718
Journal-ref: 36th Annual Conference of the Robot Society of Japan, Nagoya, 2018
\\
  This paper mainly focuses on environment perception in snowy situations which
forms the backbone of the autonomous driving technology. For the purpose,
semantic segmentation is employed to classify the objects while the vehicle is
driven autonomously. We train the Fully Convolutional Networks (FCN) on our own
dataset and present the experimental results. Finally, the outcomes are
analyzed to give a conclusion. It can be concluded that the database still
needs to be optimized and a favorable algorithm should be proposed to get
better results.
\\ ( https://arxiv.org/abs/2007.12869 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12881
Date: Sat, 25 Jul 2020 08:31:15 GMT   (4890kb,D)

Title: MirrorNet: Bio-Inspired Adversarial Attack for Camouflaged Object
  Segmentation
Authors: Jinnan Yan, Trung-Nghia Le, Khanh-Duy Nguyen, Minh-Triet Tran,
  Thanh-Toan Do, Tam V. Nguyen
Categories: cs.CV
Comments: Accept with Minor Revision to Pattern Recognition Journal
\\
  Camouflaged objects are generally difficult to be detected in their natural
environment even for human beings. In this paper, we propose a novel
bio-inspired network, named the MirrorNet, that leverages both instance
segmentation and adversarial attack for the camouflaged object segmentation.
Differently from existing networks for segmentation, our proposed network
possesses two segmentation streams: the main stream and the adversarial stream
corresponding with the original image and its flipped image, respectively. The
output from the adversarial stream is then fused into the main stream's result
for the final camouflage map to boost up the segmentation accuracy. Extensive
experiments conducted on the public CAMO dataset demonstrate the effectiveness
of our proposed network. Our proposed method achieves 89% in accuracy,
outperforming the state-of-the-arts.
  Project Page: https://sites.google.com/view/ltnghia/research/camo
\\ ( https://arxiv.org/abs/2007.12881 ,  4890kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12885
Date: Sat, 25 Jul 2020 08:54:26 GMT   (8940kb,D)

Title: Learning Disentangled Representations with Latent Variation
  Predictability
Authors: Xinqi Zhu and Chang Xu and Dacheng Tao
Categories: cs.CV
Comments: 14 pages, ECCV20
\\
  Latent traversal is a popular approach to visualize the disentangled latent
representations. Given a bunch of variations in a single unit of the latent
representation, it is expected that there is a change in a single factor of
variation of the data while others are fixed. However, this impressive
experimental observation is rarely explicitly encoded in the objective function
of learning disentangled representations. This paper defines the variation
predictability of latent disentangled representations. Given image pairs
generated by latent codes varying in a single dimension, this varied dimension
could be closely correlated with these image pairs if the representation is
well disentangled. Within an adversarial generation process, we encourage
variation predictability by maximizing the mutual information between latent
variations and corresponding image pairs. We further develop an evaluation
metric that does not rely on the ground-truth generative factors to measure the
disentanglement of latent representations. The proposed variation
predictability is a general constraint that is applicable to the VAE and GAN
frameworks for boosting disentanglement of latent representations. Experiments
show that the proposed variation predictability correlates well with existing
ground-truth-required metrics and the proposed algorithm is effective for
disentanglement learning.
\\ ( https://arxiv.org/abs/2007.12885 ,  8940kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12887
Date: Sat, 25 Jul 2020 09:07:35 GMT   (6257kb,D)

Title: Approximated Bilinear Modules for Temporal Modeling
Authors: Xinqi Zhu and Chang Xu and Langwen Hui and Cewu Lu and Dacheng Tao
Categories: cs.CV
Comments: 8 pages, ICCV19
\\
  We consider two less-emphasized temporal properties of video: 1. Temporal
cues are fine-grained; 2. Temporal modeling needs reasoning. To tackle both
problems at once, we exploit approximated bilinear modules (ABMs) for temporal
modeling. There are two main points making the modules effective: two-layer
MLPs can be seen as a constraint approximation of bilinear operations, thus can
be used to construct deep ABMs in existing CNNs while reusing pretrained
parameters; frame features can be divided into static and dynamic parts because
of visual repetition in adjacent frames, which enables temporal modeling to be
more efficient. Multiple ABM variants and implementations are investigated,
from high performance to high efficiency. Specifically, we show how two-layer
subnets in CNNs can be converted to temporal bilinear modules by adding an
auxiliary-branch. Besides, we introduce snippet sampling and shifting inference
to boost sparse-frame video classification performance. Extensive ablation
studies are conducted to show the effectiveness of proposed techniques. Our
models can outperform most state-of-the-art methods on Something-Something v1
and v2 datasets without Kinetics pretraining, and are also competitive on other
YouTube-like action recognition datasets. Our code is available on
https://github.com/zhuxinqimac/abm-pytorch.
\\ ( https://arxiv.org/abs/2007.12887 ,  6257kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12918
Date: Sat, 25 Jul 2020 12:10:16 GMT   (3292kb,D)

Title: Crowdsourced 3D Mapping: A Combined Multi-View Geometry and
  Self-Supervised Learning Approach
Authors: Hemang Chawla, Matti Jukola, Terence Brouns, Elahe Arani, and Bahram
  Zonooz
Categories: cs.CV cs.LG cs.RO
Comments: Accepted at 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
\\
  The ability to efficiently utilize crowdsourced visual data carries immense
potential for the domains of large scale dynamic mapping and autonomous
driving. However, state-of-the-art methods for crowdsourced 3D mapping assume
prior knowledge of camera intrinsics. In this work, we propose a framework that
estimates the 3D positions of semantically meaningful landmarks such as traffic
signs without assuming known camera intrinsics, using only monocular color
camera and GPS. We utilize multi-view geometry as well as deep learning based
self-calibration, depth, and ego-motion estimation for traffic sign
positioning, and show that combining their strengths is important for
increasing the map coverage. To facilitate research on this task, we construct
and make available a KITTI based 3D traffic sign ground truth positioning
dataset. Using our proposed framework, we achieve an average single-journey
relative and absolute positioning accuracy of 39cm and 1.26m respectively, on
this dataset.
\\ ( https://arxiv.org/abs/2007.12918 ,  3292kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12928
Date: Sat, 25 Jul 2020 13:39:54 GMT   (1786kb,D)

Title: Video Super Resolution Based on Deep Learning: A comprehensive survey
Authors: Hongying Liu, Zhubo Ruan, Peng Zhao, Fanhua Shang, Linlin Yang,
  Yuanyuan Liu
Categories: cs.CV eess.IV
\\
  In recent years, deep learning has made great progress in the fields of image
recognition, video analysis, natural language processing and speech
recognition, including video super-resolution tasks. In this survey, we
comprehensively investigate 28 state-of-the-art video super-resolution methods
based on deep learning. It is well known that the leverage of information
within video frames is important for video super-resolution. Hence we propose a
taxonomy and classify the methods into six sub-categories according to the ways
of utilizing inter-frame information. Moreover, the architectures and
implementation details (including input and output, loss function and learning
rate) of all the methods are depicted in details. Finally, we summarize and
compare their performance on some benchmark datasets under different
magnification factors. We also discuss some challenges, which need to be
further addressed by researchers in the community of video super-resolution.
Therefore, this work is expected to make a contribution to the future
development of research in video super-resolution, and alleviate
understandability and transferability of existing and future techniques into
practice.
\\ ( https://arxiv.org/abs/2007.12928 ,  1786kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12942
Date: Sat, 25 Jul 2020 14:30:03 GMT   (1481kb,D)

Title: Gradient Regularized Contrastive Learning for Continual Domain
  Adaptation
Authors: Peng Su, Shixiang Tang, Peng Gao, Di Qiu, Ni Zhao, Xiaogang Wang
Categories: cs.CV cs.LG
\\
  Human beings can quickly adapt to environmental changes by leveraging
learning experience. However, the poor ability of adapting to dynamic
environments remains a major challenge for AI models. To better understand this
issue, we study the problem of continual domain adaptation, where the model is
presented with a labeled source domain and a sequence of unlabeled target
domains. There are two major obstacles in this problem: domain shifts and
catastrophic forgetting. In this work, we propose Gradient Regularized
Contrastive Learning to solve the above obstacles. At the core of our method,
gradient regularization plays two key roles: (1) enforces the gradient of
contrastive loss not to increase the supervised training loss on the source
domain, which maintains the discriminative power of learned features; (2)
regularizes the gradient update on the new domain not to increase the
classification loss on the old target domains, which enables the model to adapt
to an in-coming target domain while preserving the performance of previously
observed domains. Hence our method can jointly learn both semantically
discriminative and domain-invariant features with labeled source domain and
unlabeled target domains. The experiments on Digits, DomainNet and
Office-Caltech benchmarks demonstrate the strong performance of our approach
when compared to the state-of-the-art.
\\ ( https://arxiv.org/abs/2007.12942 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12944
Date: Sat, 25 Jul 2020 14:41:51 GMT   (7635kb,D)

Title: MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part
  Disentanglement
Authors: Rinon Gal, Amit Bermano, Hao Zhang, Daniel Cohen-Or
Categories: cs.CV cs.LG
\\
  We present MRGAN, a multi-rooted adversarial network which generates
part-disentangled 3D point-cloud shapes without part-based shape supervision.
The network fuses multiple branches of tree-structured graph convolution layers
which produce point clouds, with learnable constant inputs at the tree roots.
Each branch learns to grow a different shape part, offering control over the
shape generation at the part level. Our network encourages disentangled
generation of semantic parts via two key ingredients: a root-mixing training
strategy which helps decorrelate the different branches to facilitate
disentanglement, and a set of loss terms designed with part disentanglement and
shape semantics in mind. Of these, a novel convexity loss incentivizes the
generation of parts that are more convex, as semantic parts tend to be. In
addition, a root-dropping loss further ensures that each root seeds a single
part, preventing the degeneration or over-growth of the point-producing
branches. We evaluate the performance of our network on a number of 3D shape
classes, and offer qualitative and quantitative comparisons to previous works
and baseline approaches. We demonstrate the controllability offered by our
part-disentangled generation through two applications for shape modeling: part
mixing and individual part variation, without receiving segmented shapes as
input.
\\ ( https://arxiv.org/abs/2007.12944 ,  7635kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12979
Date: Sat, 25 Jul 2020 17:09:53 GMT   (7223kb,D)

Title: GP-Aligner: Unsupervised Non-rigid Groupwise Point Set Registration
  Based On Optimized Group Latent Descriptor
Authors: Lingjing Wang, Xiang Li, Yi Fang
Categories: cs.CV
\\
  In this paper, we propose a novel method named GP-Aligner to deal with the
problem of non-rigid groupwise point set registration. Compared to previous
non-learning approaches, our proposed method gains competitive advantages by
leveraging the power of deep neural networks to effectively and efficiently
learn to align a large number of highly deformed 3D shapes with superior
performance. Unlike most learning-based methods that use an explicit feature
encoding network to extract the per-shape features and their correlations, our
model leverages a model-free learnable latent descriptor to characterize the
group relationship. More specifically, for a given group we first define an
optimizable Group Latent Descriptor (GLD) to characterize the gruopwise
relationship among a group of point sets. Each GLD is randomly initialized from
a Gaussian distribution and then concatenated with the coordinates of each
point of the associated point sets in the group. A neural network-based decoder
is further constructed to predict the coherent drifts as the desired
transformation from input groups of shapes to aligned groups of shapes. During
the optimization process, GP-Aligner jointly updates all GLDs and weight
parameters of the decoder network towards the minimization of an unsupervised
groupwise alignment loss. After optimization, for each group our model
coherently drives each point set towards a middle, common position (shape)
without specifying one as the target. GP-Aligner does not require large-scale
training data for network training and it can directly align groups of point
sets in a one-stage optimization process. GP-Aligner shows both accuracy and
computational efficiency improvement in comparison with state-of-the-art
methods for groupwise point set registration. Moreover, GP-Aligner is shown
great efficiency in aligning a large number of groups of real-world 3D shapes.
\\ ( https://arxiv.org/abs/2007.12979 ,  7223kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13003
Date: Sat, 25 Jul 2020 19:52:25 GMT   (16614kb,D)

Title: Robust and Generalizable Visual Representation Learning via Random
  Convolutions
Authors: Zhenlin Xu, Deyi Liu, Junlin Yang, Marc Niethammer
Categories: cs.CV cs.LG
\\
  While successful for various computer vision tasks, deep neural networks have
shown to be vulnerable to texture style shifts and small perturbations to which
humans are robust. Hence, our goal is to train models in such a way that
improves their robustness to these perturbations. We are motivated by the
approximately shape-preserving property of randomized convolutions, which is
due to distance preservation under random linear transforms. Intuitively,
randomized convolutions create an infinite number of new domains with similar
object shapes but random local texture. Therefore, we explore using outputs of
multi-scale random convolutions as new images or mixing them with the original
images during training. When applying a network trained with our approach to
unseen domains, our method consistently improves the performance on domain
generalization benchmarks and is scalable to ImageNet. Especially for the
challenging scenario of generalizing to the sketch domain in PACS and to
ImageNet-Sketch, our method outperforms state-of-art methods by a large margin.
More interestingly, our method can benefit downstream tasks by providing a more
robust pretrained visual representation.
\\ ( https://arxiv.org/abs/2007.13003 ,  16614kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13007
Date: Sat, 25 Jul 2020 20:42:21 GMT   (9417kb,D)

Title: HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast
  Biopsy Images
Authors: Sachin Mehta, Ximing Lu, Donald Weaver, Joann G. Elmore, Hannaneh
  Hajishirzi, Linda Shapiro
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: 10 pages
\\
  Training end-to-end networks for classifying gigapixel size histopathological
images is computationally intractable. Most approaches are patch-based and
first learn local representations (patch-wise) before combining these local
representations to produce image-level decisions. However, dividing large
tissue structures into patches limits the context available to these networks,
which may reduce their ability to learn representations from clinically
relevant structures. In this paper, we introduce a novel attention-based
network, the Holistic ATtention Network (HATNet) to classify breast biopsy
images. We streamline the histopathological image classification pipeline and
show how to learn representations from gigapixel size images end-to-end. HATNet
extends the bag-of-words approach and uses self-attention to encode global
information, allowing it to learn representations from clinically relevant
tissue structures without any explicit supervision. It outperforms the previous
best network Y-Net, which uses supervision in the form of tissue-level
segmentation masks, by 8%. Importantly, our analysis reveals that HATNet learns
representations from clinically relevant structures, and it matches the
classification accuracy of human pathologists for this challenging test set.
Our source code is available at \url{https://github.com/sacmehta/HATNet}
\\ ( https://arxiv.org/abs/2007.13007 ,  9417kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13010
Date: Sat, 25 Jul 2020 21:17:51 GMT   (9960kb,D)

Title: Style is a Distribution of Features
Authors: Eddie Huang, Sahil Gupta
Categories: cs.CV cs.LG eess.IV
\\
  Neural style transfer (NST) is a powerful image generation technique that
uses a convolutional neural network (CNN) to merge the content of one image
with the style of another. Contemporary methods of NST use first or second
order statistics of the CNN's features to achieve transfers with relatively
little computational cost. However, these methods cannot fully extract the
style from the CNN's features. We present a new algorithm for style transfer
that fully extracts the style from the features by redefining the style loss as
the Wasserstein distance between the distribution of features. Thus, we set a
new standard in style transfer quality. In addition, we state two important
interpretations of NST. The first is a re-emphasis from Li et al., which states
that style is simply the distribution of features. The second states that NST
is a type of generative adversarial network (GAN) problem.
\\ ( https://arxiv.org/abs/2007.13010 ,  9960kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13034
Date: Sun, 26 Jul 2020 00:08:37 GMT   (6787kb,D)

Title: Mask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve
Authors: Weicheng Kuo, Anelia Angelova, Tsung-Yi Lin, Angela Dai
Categories: cs.CV cs.AI
Comments: ECCV 2020 (Spotlight)
\\
  Object recognition has seen significant progress in the image domain, with
focus primarily on 2D perception. We propose to leverage existing large-scale
datasets of 3D models to understand the underlying 3D structure of objects seen
in an image by constructing a CAD-based representation of the objects and their
poses. We present Mask2CAD, which jointly detects objects in real-world images
and for each detected object, optimizes for the most similar CAD model and its
pose. We construct a joint embedding space between the detected regions of an
image corresponding to an object and 3D CAD models, enabling retrieval of CAD
models for an input RGB image. This produces a clean, lightweight
representation of the objects in an image; this CAD-based representation
ensures a valid, efficient shape representation for applications such as
content creation or interactive scenarios, and makes a step towards
understanding the transformation of real-world imagery to a synthetic domain.
Experiments on real-world images from Pix3D demonstrate the advantage of our
approach in comparison to state of the art. To facilitate future research, we
additionally propose a new image-to-3D baseline on ScanNet which features
larger shape diversity, real-world occlusions, and challenging image views.
\\ ( https://arxiv.org/abs/2007.13034 ,  6787kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13049
Date: Sun, 26 Jul 2020 03:35:37 GMT   (3062kb,D)

Title: A Dual Iterative Refinement Method for Non-rigid Shape Matching
Authors: Rui Xiang, Rongjie Lai, Hongkai Zhao
Categories: cs.CV
Comments: 10 pages, 11 figures and 1 table
\\
  In this work, a simple and efficient dual iterative refinement (DIR) method
is proposed for dense correspondence between two nearly isometric shapes. The
key idea is to use dual information, such as spatial and spectral, or local and
global features, in a complementary and effective way, and extract more
accurate information from current iteration to use for the next iteration. In
each DIR iteration, starting from current correspondence, a zoom-in process at
each point is used to select well matched anchor pairs by a local mapping
distortion criterion. These selected anchor pairs are then used to align
spectral features (or other appropriate global features) whose dimension
adaptively matches the capacity of the selected anchor pairs. Thanks to the
effective combination of complementary information in a data-adaptive way, DIR
is not only efficient but also robust to render accurate results within a few
iterations. By choosing appropriate dual features, DIR has the flexibility to
handle patch and partial matching as well. Extensive experiments on various
data sets demonstrate the superiority of DIR over other state-of-the-art
methods in terms of both accuracy and efficiency.
\\ ( https://arxiv.org/abs/2007.13049 ,  3062kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13072
Date: Sun, 26 Jul 2020 07:33:22 GMT   (258kb)

Title: Approaches of large-scale images recognition with more than 50,000
  categoris
Authors: Wanhong Huang, Rui Geng
Categories: cs.CV
\\
  Though current CV models have been able to achieve high levels of accuracy on
small-scale images classification dataset with hundreds or thousands of
categories, many models become infeasible in computational or space consumption
when it comes to large-scale dataset with more than 50,000 categories. In this
paper, we provide a viable solution for classifying large-scale species
datasets using traditional CV techniques such as.features extraction and
processing, BOVW(Bag of Visual Words) and some statistical learning technics
like Mini-Batch K-Means,SVM which are used in our works. And then mixed with a
neural network model. When applying these techniques, we have done some
optimization in time and memory consumption, so that it can be feasible for
large-scale dataset. And we also use some technics to reduce the impact of
mislabeling data. We use a dataset with more than 50, 000 categories, and all
operations are done on common computer with l 6GB RAM and a CPU of 3. OGHz. Our
contributions are: 1) analysis what problems may meet in the training
processes, and presents several feasible ways to solve these problems. 2) Make
traditional CV models combined with neural network models provide some feasible
scenarios for training large-scale classified datasets within the constraints
of time and spatial resources.
\\ ( https://arxiv.org/abs/2007.13072 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13078
Date: Sun, 26 Jul 2020 08:17:10 GMT   (15754kb,D)

Title: SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction
Authors: Sriram N N, Buyu Liu, Francesco Pittaluga, Manmohan Chandraker
Categories: cs.CV cs.LG
Comments: Accepted at ECCV 2020
\\
  We propose advances that address two key challenges in future trajectory
prediction: (i) multimodality in both training data and predictions and (ii)
constant time inference regardless of number of agents. Existing trajectory
predictions are fundamentally limited by lack of diversity in training data,
which is difficult to acquire with sufficient coverage of possible modes. Our
first contribution is an automatic method to simulate diverse trajectories in
the top-view. It uses pre-existing datasets and maps as initialization, mines
existing trajectories to represent realistic driving behaviors and uses a
multi-agent vehicle dynamics simulator to generate diverse new trajectories
that cover various modes and are consistent with scene layout constraints. Our
second contribution is a novel method that generates diverse predictions while
accounting for scene semantics and multi-agent interactions, with constant-time
inference independent of the number of agents. We propose a convLSTM with novel
state pooling operations and losses to predict scene-consistent states of
multiple agents in a single forward pass, along with a CVAE for diversity. We
validate our proposed multi-agent trajectory prediction approach by training
and testing on the proposed simulated dataset and existing real datasets of
traffic scenes. In both cases, our approach outperforms SOTA methods by a large
margin, highlighting the benefits of both our diverse dataset simulation and
constant-time diverse trajectory prediction methods.
\\ ( https://arxiv.org/abs/2007.13078 ,  15754kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13092
Date: Sun, 26 Jul 2020 10:12:42 GMT   (7407kb,D)

Title: U2-ONet: A Two-level Nested Octave U-structure with Multiscale Attention
  Mechanism for Moving Instances Segmentation
Authors: Chenjie Wang and Chengyuan Li and Bin Luo
Categories: cs.CV cs.AI
Comments: 10 pages, 7 figures,
\\
  Most scenes in practical applications are dynamic scenes containing moving
objects, so segmenting accurately moving objects is crucial for many computer
vision applications. In order to efficiently segment out all moving objects in
the scene, regardless of whether the object has a predefined semantic label, we
propose a two-level nested Octave U-structure network with a multiscale
attention mechanism called U2-ONet. Each stage of U2-ONet is filled with our
newly designed Octave ReSidual U-block (ORSU) to enhance the ability to obtain
more context information at different scales while reducing spatial redundancy
of feature maps. In order to efficiently train our multi-scale deep network, we
introduce a hierarchical training supervision strategy that calculates the loss
at each level while adding a knowledge matching loss to keep the optimization
consistency. Experimental results show that our method achieves
state-of-the-art performance in several general moving objects segmentation
datasets.
\\ ( https://arxiv.org/abs/2007.13092 ,  7407kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13098
Date: Sun, 26 Jul 2020 10:56:37 GMT   (9291kb,D)

Title: Towards Purely Unsupervised Disentanglement of Appearance and Shape for
  Person Images Generation
Authors: Hongtao Yang, Tong Zhang, Wenbing Huang, Xuming He
Categories: cs.CV
\\
  There have been a fairly of research interests in exploring the
disentanglement of appearance and shape from human images. Most existing
endeavours pursuit this goal by either using training images with annotations
or regulating the training process with external clues such as human skeleton,
body segmentation or cloth patches etc. In this paper, we aim to address this
challenge in a more unsupervised manner---we do not require any annotation nor
any external task-specific clues. To this end, we formulate an
encoder-decoder-like network to extract both the shape and appearance features
from input images at the same time, and train the parameters by three losses:
feature adversarial loss, color consistency loss and reconstruction loss. The
feature adversarial loss mainly impose little to none mutual information
between the extracted shape and appearance features, while the color
consistency loss is to encourage the invariance of person appearance
conditioned on different shapes. More importantly, our
unsupervised\footnote{Unsupervised learning has many interpretations in
different tasks. To be clear, in this paper, we refer unsupervised learning as
learning without task-specific human annotations, pairs or any form of weak
supervision.} framework utilizes learned shape features as masks which are
applied to the input itself in order to obtain clean appearance features.
Without using fixed input human skeleton, our network better preserves the
conditional human posture while requiring less supervision. Experimental
results on DeepFashion and Market1501 demonstrate that the proposed method
achieves clean disentanglement and is able to synthesis novel images of
comparable quality with state-of-the-art weakly-supervised or even supervised
methods.
\\ ( https://arxiv.org/abs/2007.13098 ,  9291kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13106
Date: Sun, 26 Jul 2020 11:50:50 GMT   (13119kb,D)

Title: Detection and Annotation of Plant Organs from Digitized Herbarium Scans
  using Deep Learning
Authors: Sohaib Younis, Marco Schmidt, Claus Weiland, Steffan Dressler,
  Bernhard Seeger, Thomas Hickler
Categories: cs.CV q-bio.QM
ACM-class: I.2.10; J.3
\\
  As herbarium specimens are increasingly becoming digitized and accessible in
online repositories, advanced computer vision techniques are being used to
extract information from them. The presence of certain plant organs on
herbarium sheets is useful information in various scientific contexts and
automatic recognition of these organs will help mobilize such information. In
our study we use deep learning to detect plant organs on digitized herbarium
specimens with Faster R-CNN. For our experiment we manually annotated hundreds
of herbarium scans with thousands of bounding boxes for six types of plant
organs and used them for training and evaluating the plant organ detection
model. The model worked particularly well on leaves and stems, while flowers
were also present in large numbers in the sheets, but not equally well
recognized.
\\ ( https://arxiv.org/abs/2007.13106 ,  13119kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13119
Date: Sun, 26 Jul 2020 12:32:38 GMT   (10430kb,D)

Title: SADet: Learning An Efficient and Accurate Pedestrian Detector
Authors: Chubin Zhuang and Zhen Lei and Stan Z. Li
Categories: cs.CV
\\
  Although the anchor-based detectors have taken a big step forward in
pedestrian detection, the overall performance of algorithm still needs further
improvement for practical applications, \emph{e.g.}, a good trade-off between
the accuracy and efficiency. To this end, this paper proposes a series of
systematic optimization strategies for the detection pipeline of one-stage
detector, forming a single shot anchor-based detector (SADet) for efficient and
accurate pedestrian detection, which includes three main improvements. Firstly,
we optimize the sample generation process by assigning soft tags to the outlier
samples to generate semi-positive samples with continuous tag value between $0$
and $1$, which not only produces more valid samples, but also strengthens the
robustness of the model. Secondly, a novel Center-$IoU$ loss is applied as a
new regression loss for bounding box regression, which not only retains the
good characteristics of IoU loss, but also solves some defects of it. Thirdly,
we also design Cosine-NMS for the postprocess of predicted bounding boxes, and
further propose adaptive anchor matching to enable the model to adaptively
match the anchor boxes to full or visible bounding boxes according to the
degree of occlusion, making the NMS and anchor matching algorithms more
suitable for occluded pedestrian detection. Though structurally simple, it
presents state-of-the-art result and real-time speed of $20$ FPS for
VGA-resolution images ($640 \times 480$) on challenging pedestrian detection
benchmarks, i.e., CityPersons, Caltech, and human detection benchmark
CrowdHuman, leading to a new attractive pedestrian detector.
\\ ( https://arxiv.org/abs/2007.13119 ,  10430kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13120
Date: Sun, 26 Jul 2020 12:39:15 GMT   (18074kb,D)

Title: Towards End-to-end Video-based Eye-Tracking
Authors: Seonwook Park and Emre Aksan and Xucong Zhang and Otmar Hilliges
Categories: cs.CV
Comments: Accepted at ECCV 2020
\\
  Estimating eye-gaze from images alone is a challenging task, in large parts
due to un-observable person-specific factors. Achieving high accuracy typically
requires labeled data from test users which may not be attainable in real
applications. We observe that there exists a strong relationship between what
users are looking at and the appearance of the user's eyes. In response to this
understanding, we propose a novel dataset and accompanying method which aims to
explicitly learn these semantic and temporal relationships. Our video dataset
consists of time-synchronized screen recordings, user-facing camera views, and
eye gaze data, which allows for new benchmarks in temporal gaze tracking as
well as label-free refinement of gaze. Importantly, we demonstrate that the
fusion of information from visual stimuli as well as eye images can lead
towards achieving performance similar to literature-reported figures acquired
through supervised personalization. Our final method yields significant
performance improvements on our proposed EVE dataset, with up to a 28 percent
improvement in Point-of-Gaze estimates (resulting in 2.49 degrees in angular
error), paving the path towards high-accuracy screen-based eye tracking purely
from webcam sensors. The dataset and reference source code are available at
https://ait.ethz.ch/projects/2020/EVE
\\ ( https://arxiv.org/abs/2007.13120 ,  18074kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13124
Date: Sun, 26 Jul 2020 13:05:55 GMT   (4157kb,D)

Title: GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and
  Scene-aware Supervision
Authors: Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, Chi-Keung Tang
Categories: cs.CV
Comments: ECCV 2020
\\
  We present a novel end-to-end framework named as GSNet (Geometric and
Scene-aware Network), which jointly estimates 6DoF poses and reconstructs
detailed 3D car shapes from single urban street view. GSNet utilizes a unique
four-way feature extraction and fusion scheme and directly regresses 6DoF poses
and shapes in a single forward pass. Extensive experiments show that our
diverse feature extraction and fusion scheme can greatly improve model
performance. Based on a divide-and-conquer 3D shape representation strategy,
GSNet reconstructs 3D vehicle shape with great detail (1352 vertices and 2700
faces). This dense mesh representation further leads us to consider geometrical
consistency and scene context, and inspires a new multi-objective loss function
to regularize network training, which in turn improves the accuracy of 6D pose
estimation and validates the merit of jointly performing both tasks. We
evaluate GSNet on the largest multi-task ApolloCar3D benchmark and achieve
state-of-the-art performance both quantitatively and qualitatively. Project
page is available at https://lkeab.github.io/gsnet/.
\\ ( https://arxiv.org/abs/2007.13124 ,  4157kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13135
Date: Sun, 26 Jul 2020 14:26:18 GMT   (2953kb,D)

Title: Contrastive Visual-Linguistic Pretraining
Authors: Lei Shi, Kai Shuang, Shijie Geng, Peng Su, Zhengkai Jiang, Peng Gao,
  Zuohui Fu, Gerard de Melo, Sen Su
Categories: cs.CV eess.IV
\\
  Several multi-modality representation learning approaches such as LXMERT and
ViLBERT have been proposed recently. Such approaches can achieve superior
performance due to the high-level semantic information captured during
large-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual
region regression and classification loss, they often suffer from domain gap
and noisy label problems, based on the visual features having been pretrained
on the Visual Genome dataset. To overcome these issues, we propose unbiased
Contrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual
self-supervised loss built upon contrastive learning. We evaluate CVLP on
several down-stream tasks, including VQA, GQA and NLVR2 to validate the
superiority of contrastive learning on multi-modality representation learning.
Our code is available at: https://github.com/ArcherYunDong/CVLP-.
\\ ( https://arxiv.org/abs/2007.13135 ,  2953kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13138
Date: Sun, 26 Jul 2020 14:46:55 GMT   (11215kb,D)

Title: Virtual Multi-view Fusion for 3D Semantic Segmentation
Authors: Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian
  Brewington, Thomas Funkhouser, Caroline Pantofaru
Categories: cs.CV eess.IV
Comments: To appear in ECCV 2020
\\
  Semantic segmentation of 3D meshes is an important problem for 3D scene
understanding. In this paper we revisit the classic multiview representation of
3D meshes and study several techniques that make them effective for 3D semantic
segmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our
method effectively chooses different virtual views of the 3D mesh and renders
multiple 2D channels for training an effective 2D semantic segmentation model.
Features from multiple per view predictions are finally fused on 3D mesh
vertices to predict mesh semantic segmentation labels. Using the large scale
indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual
views enable more effective training of 2D semantic segmentation networks than
previous multiview approaches. When the 2D per pixel predictions are aggregated
on 3D surfaces, our virtual multiview fusion method is able to achieve
significantly better 3D semantic segmentation results compared to all prior
multiview approaches and competitive with recent 3D convolution approaches.
\\ ( https://arxiv.org/abs/2007.13138 ,  11215kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13143
Date: Sun, 26 Jul 2020 15:11:44 GMT   (2884kb,D)

Title: Challenge-Aware RGBT Tracking
Authors: Chenglong Li, Lei Liu, Andong Lu, Qing Ji, and Jin Tang
Categories: cs.CV
Comments: Accepted by ECCV 2020
\\
  RGB and thermal source data suffer from both shared and specific challenges,
and how to explore and exploit them plays a critical role to represent the
target appearance in RGBT tracking. In this paper, we propose a novel
challenge-aware neural network to handle the modality-shared challenges (e.g.,
fast motion, scale variation and occlusion) and the modality-specific ones
(e.g., illumination variation and thermal crossover) for RGBT tracking. In
particular, we design several parameter-shared branches in each layer to model
the target appearance under the modality-shared challenges, and several
parameterindependent branches under the modality-specific ones. Based on the
observation that the modality-specific cues of different modalities usually
contains the complementary advantages, we propose a guidance module to transfer
discriminative features from one modality to another one, which could enhance
the discriminative ability of some weak modality. Moreover, all branches are
aggregated together in an adaptive manner and parallel embedded in the backbone
network to efficiently form more discriminative target representations. These
challenge-aware branches are able to model the target appearance under certain
challenges so that the target representations can be learnt by a few parameters
even in the situation of insufficient training data. From the experimental
results we will show that our method operates at a real-time speed while
performing well against the state-of-the-art methods on three benchmark
datasets.
\\ ( https://arxiv.org/abs/2007.13143 ,  2884kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13145
Date: Sun, 26 Jul 2020 15:20:53 GMT   (8948kb,D)

Title: Deep Photometric Stereo for Non-Lambertian Surfaces
Authors: Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K.
  Wong
Categories: cs.CV
\\
  This paper addresses the problem of photometric stereo, in both calibrated
and uncalibrated scenarios, for non-Lambertian surfaces based on deep learning.
We first introduce a fully convolutional deep network for calibrated
photometric stereo, which we call PS-FCN. Unlike traditional approaches that
adopt simplified reflectance models to make the problem tractable, our method
directly learns the mapping from reflectance observations to surface normal,
and is able to handle surfaces with general and unknown isotropic reflectance.
At test time, PS-FCN takes an arbitrary number of images and their associated
light directions as input and predicts a surface normal map of the scene in a
fast feed-forward pass. To deal with the uncalibrated scenario where light
directions are unknown, we introduce a new convolutional network, named LCNet,
to estimate light directions from input images. The estimated light directions
and the input images are then fed to PS-FCN to determine the surface normals.
Our method does not require a pre-defined set of light directions and can
handle multiple images in an order-agnostic manner. Thorough evaluation of our
approach on both synthetic and real datasets shows that it outperforms
state-of-the-art methods in both calibrated and uncalibrated scenarios.
\\ ( https://arxiv.org/abs/2007.13145 ,  8948kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13172
Date: Sun, 26 Jul 2020 16:30:56 GMT   (7357kb,D)

Title: Learning and aggregating deep local descriptors for instance-level
  recognition
Authors: Giorgos Tolias, Tomas Jenicek, Ond\v{r}ej Chum
Categories: cs.CV
Comments: ECCV 2020
\\
  We propose an efficient method to learn deep local descriptors for
instance-level recognition. The training only requires examples of positive and
negative image pairs and is performed as metric learning of sum-pooled global
image descriptors. At inference, the local descriptors are provided by the
activations of internal components of the network. We demonstrate why such an
approach learns local descriptors that work well for image similarity
estimation with classical efficient match kernel methods. The experimental
validation studies the trade-off between performance and memory requirements of
the state-of-the-art image search approach based on match kernels. Compared to
existing local descriptors, the proposed ones perform better in two
instance-level recognition tasks and keep memory requirements lower. We
experimentally show that global descriptors are not effective enough at large
scale and that local descriptors are essential. We achieve state-of-the-art
performance, in some cases even with a backbone network as small as ResNet18.
\\ ( https://arxiv.org/abs/2007.13172 ,  7357kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13215
Date: Sun, 26 Jul 2020 20:46:41 GMT   (8722kb,D)

Title: OASIS: A Large-Scale Dataset for Single Image 3D in the Wild
Authors: Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton,
  Jia Deng
Categories: cs.CV
Comments: Accepted to CVPR 2020
\\
  Single-view 3D is the task of recovering 3D properties such as depth and
surface normals from a single image. We hypothesize that a major obstacle to
single-image 3D is data. We address this issue by presenting Open Annotations
of Single Image Surfaces (OASIS), a dataset for single-image 3D in the wild
consisting of annotations of detailed 3D geometry for 140,000 images. We train
and evaluate leading models on a variety of single-image 3D tasks. We expect
OASIS to be a useful resource for 3D vision research. Project site:
https://pvl.cs.princeton.edu/OASIS.
\\ ( https://arxiv.org/abs/2007.13215 ,  8722kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13251
Date: Mon, 27 Jul 2020 00:15:13 GMT   (878kb,D)

Title: Point-to-set distance functions for weakly supervised segmentation
Authors: Bas Peters
Categories: cs.CV eess.IV
MSC-class: 68T45
\\
  When pixel-level masks or partial annotations are not available for training
neural networks for semantic segmentation, it is possible to use higher-level
information in the form of bounding boxes, or image tags. In the imaging
sciences, many applications do not have an object-background structure and
bounding boxes are not available. Any available annotation typically comes from
ground truth or domain experts. A direct way to train without masks is using
prior knowledge on the size of objects/classes in the segmentation. We present
a new algorithm to include such information via constraints on the network
output, implemented via projection-based point-to-set distance functions. This
type of distance functions always has the same functional form of the
derivative, and avoids the need to adapt penalty functions to different
constraints, as well as issues related to constraining properties typically
associated with non-differentiable functions. Whereas object size information
is known to enable object segmentation from bounding boxes from datasets with
many general and medical images, we show that the applications extend to the
imaging sciences where data represents indirect measurements, even in the case
of single examples. We illustrate the capabilities in case of a) one or more
classes do not have any annotation; b) there is no annotation at all; c) there
are bounding boxes. We use data for hyperspectral time-lapse imaging, object
segmentation in corrupted images, and sub-surface aquifer mapping from
airborne-geophysical remote-sensing data. The examples verify that the
developed methodology alleviates difficulties with annotating non-visual
imagery for a range of experimental settings.
\\ ( https://arxiv.org/abs/2007.13251 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13262
Date: Mon, 27 Jul 2020 00:54:50 GMT   (4712kb,D)

Title: REXUP: I REason, I EXtract, I UPdate with Structured Compositional
  Reasoning for Visual Question Answering
Authors: Siwen Luo, Soyeon Caren Han, Kaiyuan Sun and Josiah Poon
Categories: cs.CV cs.AI
Comments: 13 pages, 3 figures
\\
  Visual question answering (VQA) is a challenging multi-modal task that
requires not only the semantic understanding of both images and questions, but
also the sound perception of a step-by-step reasoning process that would lead
to the correct answer. So far, most successful attempts in VQA have been
focused on only one aspect, either the interaction of visual pixel features of
images and word features of questions, or the reasoning process of answering
the question in an image with simple objects. In this paper, we propose a deep
reasoning VQA model with explicit visual structure-aware textual information,
and it works well in capturing step-by-step reasoning process and detecting a
complex object-relationship in photo-realistic images. REXUP network consists
of two branches, image object-oriented and scene graph oriented, which jointly
works with super-diagonal fusion compositional attention network. We
quantitatively and qualitatively evaluate REXUP on the GQA dataset and conduct
extensive ablation studies to explore the reasons behind REXUP's effectiveness.
Our best model significantly outperforms the precious state-of-the-art, which
delivers 92.7% on the validation set and 73.1% on the test-dev set.
\\ ( https://arxiv.org/abs/2007.13262 ,  4712kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13264
Date: Mon, 27 Jul 2020 01:21:18 GMT   (5048kb,D)

Title: Learning Task-oriented Disentangled Representations for Unsupervised
  Domain Adaptation
Authors: Pingyang Dai, Peixian Chen, Qiong Wu, Xiaopeng Hong, Qixiang Ye, Qi
  Tian, Rongrong Ji
Categories: cs.CV cs.IR cs.LG
Comments: 9 pages, 6 figures
\\
  Unsupervised domain adaptation (UDA) aims to address the domain-shift problem
between a labeled source domain and an unlabeled target domain. Many efforts
have been made to address the mismatch between the distributions of training
and testing data, but unfortunately, they ignore the task-oriented information
across domains and are inflexible to perform well in complicated open-set
scenarios. Many efforts have been made to eliminate the mismatch between the
distributions of training and testing data by learning domain-invariant
representations. However, the learned representations are usually not
task-oriented, i.e., being class-discriminative and domain-transferable
simultaneously. This drawback limits the flexibility of UDA in complicated
open-set tasks where no labels are shared between domains. In this paper, we
break the concept of task-orientation into task-relevance and task-irrelevance,
and propose a dynamic task-oriented disentangling network (DTDN) to learn
disentangled representations in an end-to-end fashion for UDA. The dynamic
disentangling network effectively disentangles data representations into two
components: the task-relevant ones embedding critical information associated
with the task across domains, and the task-irrelevant ones with the remaining
non-transferable or disturbing information. These two components are
regularized by a group of task-specific objective functions across domains.
Such regularization explicitly encourages disentangling and avoids the use of
generative models or decoders. Experiments in complicated, open-set scenarios
(retrieval tasks) and empirical benchmarks (classification tasks) demonstrate
that the proposed method captures rich disentangled information and achieves
superior performance.
\\ ( https://arxiv.org/abs/2007.13264 ,  5048kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13278
Date: Mon, 27 Jul 2020 02:28:47 GMT   (2099kb,D)

Title: Representation Learning with Video Deep InfoMax
Authors: Hjelm, R Devon and Bachman, Philip
Categories: cs.CV cs.LG
\\
  Self-supervised learning has made unsupervised pretraining relevant again for
difficult computer vision tasks. The most effective self-supervised methods
involve prediction tasks based on features extracted from diverse views of the
data. DeepInfoMax (DIM) is a self-supervised method which leverages the
internal structure of deep networks to construct such views, forming prediction
tasks between local features which depend on small patches in an image and
global features which depend on the whole image. In this paper, we extend DIM
to the video domain by leveraging similar structure in spatio-temporal
networks, producing a method we call Video Deep InfoMax(VDIM). We find that
drawing views from both natural-rate sequences and temporally-downsampled
sequences yields results on Kinetics-pretrained action recognition tasks which
match or outperform prior state-of-the-art methods that use more costly
large-time-scale transformer models. We also examine the effects of data
augmentation and fine-tuning methods, accomplishingSoTA by a large margin when
training only on the UCF-101 dataset.
\\ ( https://arxiv.org/abs/2007.13278 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13284
Date: Mon, 27 Jul 2020 02:45:59 GMT   (285kb)

Title: Research Progress of Convolutional Neural Network and its Application in
  Object Detection
Authors: Wei Zhang and Zuoxiang Zeng
Categories: cs.CV cs.AI
Comments: 11 pages, journal paper
ACM-class: I.2
\\
  With the improvement of computer performance and the increase of data volume,
the object detection based on convolutional neural network (CNN) has become the
main algorithm for object detection. This paper summarizes the research
progress of convolutional neural networks and their applications in object
detection, and focuses on analyzing and discussing a specific idea and method
of applying convolutional neural networks for object detection, pointing out
the current deficiencies and future development direction.
\\ ( https://arxiv.org/abs/2007.13284 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13303
Date: Mon, 27 Jul 2020 04:09:53 GMT   (16227kb,D)

Title: Reconstructing NBA Players
Authors: Luyang Zhu, Konstantinos Rematas, Brian Curless, Steve Seitz, Ira
  Kemelmacher-Shlizerman
Categories: cs.CV
Comments: ECCV 2020
\\
  Great progress has been made in 3D body pose and shape estimation from a
single photo. Yet, state-of-the-art results still suffer from errors due to
challenging body poses, modeling clothing, and self occlusions. The domain of
basketball games is particularly challenging, as it exhibits all of these
challenges. In this paper, we introduce a new approach for reconstruction of
basketball players that outperforms the state-of-the-art. Key to our approach
is a new method for creating poseable, skinned models of NBA players, and a
large database of meshes (derived from the NBA2K19 video game), that we are
releasing to the research community. Based on these models, we introduce a new
method that takes as input a single photo of a clothed player in any basketball
pose and outputs a high resolution mesh and 3D pose for that player. We
demonstrate substantial improvement over state-of-the-art, single-image methods
for body shape reconstruction.
\\ ( https://arxiv.org/abs/2007.13303 ,  16227kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13310
Date: Mon, 27 Jul 2020 04:56:41 GMT   (3242kb,D)

Title: K-Shot Contrastive Learning of Visual Features with Multiple Instance
  Augmentations
Authors: Haohang Xu, Hongkai Xiong, Guo-Jun Qi
Categories: cs.CV cs.AI cs.LG
\\
  In this paper, we propose the $K$-Shot Contrastive Learning (KSCL) of visual
features by applying multiple augmentations to investigate the sample
variations within individual instances. It aims to combine the advantages of
inter-instance discrimination by learning discriminative features to
distinguish between different instances, as well as intra-instance variations
by matching queries against the variants of augmented samples over instances.
Particularly, for each instance, it constructs an instance subspace to model
the configuration of how the significant factors of variations in $K$-shot
augmentations can be combined to form the variants of augmentations. Given a
query, the most relevant variant of instances is then retrieved by projecting
the query onto their subspaces to predict the positive instance class. This
generalizes the existing contrastive learning that can be viewed as a special
one-shot case. An eigenvalue decomposition is performed to configure instance
subspaces, and the embedding network can be trained end-to-end through the
differentiable subspace configuration. Experiment results demonstrate the
proposed $K$-shot contrastive learning achieves superior performances to the
state-of-the-art unsupervised methods.
\\ ( https://arxiv.org/abs/2007.13310 ,  3242kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13312
Date: Mon, 27 Jul 2020 05:03:37 GMT   (2977kb,D)

Title: Split Computing for Complex Object Detectors: Challenges and Preliminary
  Results
Authors: Yoshitomo Matsubara, Marco Levorato
Categories: cs.CV eess.IV
Comments: Accepted to EMDL '20 (4th International Workshop on Embedded and
  Mobile Deep Learning) co-located with ACM MobiCom 2020
\\
  Following the trends of mobile and edge computing for DNN models, an
intermediate option, split computing, has been attracting attentions from the
research community. Previous studies empirically showed that while mobile and
edge computing often would be the best options in terms of total inference
time, there are some scenarios where split computing methods can achieve
shorter inference time. All the proposed split computing approaches, however,
focus on image classification tasks, and most are assessed with small datasets
that are far from the practical scenarios. In this paper, we discuss the
challenges in developing split computing methods for powerful R-CNN object
detectors trained on a large dataset, COCO 2017. We extensively analyze the
object detectors in terms of layer-wise tensor size and model size, and show
that naive split computing methods would not reduce inference time. To the best
of our knowledge, this is the first study to inject small bottlenecks to such
object detectors and unveil the potential of a split computing approach. The
source code and trained models' weights used in this study are available at
https://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors .
\\ ( https://arxiv.org/abs/2007.13312 ,  2977kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13314
Date: Mon, 27 Jul 2020 05:49:44 GMT   (833kb,D)

Title: Rethinking Generative Zero-Shot Learning: An Ensemble Learning
  Perspective for Recognising Visual Patches
Authors: Zhi Chen, Sen Wang, Jingjing Li, Zi Huang
Categories: cs.CV cs.MM
Comments: ACM MM 2020
\\
  Zero-shot learning (ZSL) is commonly used to address the very pervasive
problem of predicting unseen classes in fine-grained image classification and
other tasks. One family of solutions is to learn synthesised unseen visual
samples produced by generative models from auxiliary semantic information, such
as natural language descriptions. However, for most of these models,
performance suffers from noise in the form of irrelevant image backgrounds.
Further, most methods do not allocate a calculated weight to each semantic
patch. Yet, in the real world, the discriminative power of features can be
quantified and directly leveraged to improve accuracy and reduce computational
complexity. To address these issues, we propose a novel framework called
multi-patch generative adversarial nets (MPGAN) that synthesises local patch
features and labels unseen classes with a novel weighted voting strategy. The
process begins by generating discriminative visual features from noisy text
descriptions for a set of predefined local patches using multiple specialist
generative models. The features synthesised from each patch for unseen classes
are then used to construct an ensemble of diverse supervised classifiers, each
corresponding to one local patch. A voting strategy averages the probability
distributions output from the classifiers and, given that some patches are more
discriminative than others, a discrimination-based attention mechanism helps to
weight each patch accordingly. Extensive experiments show that MPGAN has
significantly greater accuracy than state-of-the-art methods.
\\ ( https://arxiv.org/abs/2007.13314 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13332
Date: Mon, 27 Jul 2020 07:13:10 GMT   (2349kb,D)

Title: Few-shot Knowledge Transfer for Fine-grained Cartoon Face Generation
Authors: Nan Zhuang and Cheng Yang
Categories: cs.CV
Comments: Technical Report
\\
  In this paper, we are interested in generating fine-grained cartoon faces for
various groups. We assume that one of these groups consists of sufficient
training data while the others only contain few samples. Although the cartoon
faces of these groups share similar style, the appearances in various groups
could still have some specific characteristics, which makes them differ from
each other. A major challenge of this task is how to transfer knowledge among
groups and learn group-specific characteristics with only few samples. In order
to solve this problem, we propose a two-stage training process. First, a basic
translation model for the basic group (which consists of sufficient data) is
trained. Then, given new samples of other groups, we extend the basic model by
creating group-specific branches for each new group. Group-specific branches
are updated directly to capture specific appearances for each group while the
remaining group-shared parameters are updated indirectly to maintain the
distribution of intermediate feature space. In this manner, our approach is
capable to generate high-quality cartoon faces for various groups.
\\ ( https://arxiv.org/abs/2007.13332 ,  2349kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13344
Date: Mon, 27 Jul 2020 07:58:00 GMT   (12489kb,D)

Title: Self-Prediction for Joint Instance and Semantic Segmentation of Point
  Clouds
Authors: Jinxian Liu, Minghui Yu, Bingbing Ni and Ye Chen
Categories: cs.CV cs.GR
Comments: Accepted to ECCV 2020
\\
  We develop a novel learning scheme named Self-Prediction for 3D instance and
semantic segmentation of point clouds. Distinct from most existing methods that
focus on designing convolutional operators, our method designs a new learning
scheme to enhance point relation exploring for better segmentation. More
specifically, we divide a point cloud sample into two subsets and construct a
complete graph based on their representations. Then we use label propagation
algorithm to predict labels of one subset when given labels of the other
subset. By training with this Self-Prediction task, the backbone network is
constrained to fully explore relational context/geometric/shape information and
learn more discriminative features for segmentation. Moreover, a general
associated framework equipped with our Self-Prediction scheme is designed for
enhancing instance and semantic segmentation simultaneously, where instance and
semantic representations are combined to perform Self-Prediction. Through this
way, instance and semantic segmentation are collaborated and mutually
reinforced. Significant performance improvements on instance and semantic
segmentation compared with baseline are achieved on S3DIS and ShapeNet. Our
method achieves state-of-the-art instance segmentation results on S3DIS and
comparable semantic segmentation results compared with state-of-the-arts on
S3DIS and ShapeNet when we only take PointNet++ as the backbone network.
\\ ( https://arxiv.org/abs/2007.13344 ,  12489kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13354
Date: Mon, 27 Jul 2020 08:15:38 GMT   (573kb)

Title: Feature visualization of Raman spectrum analysis with deep convolutional
  neural network
Authors: Masashi Fukuhara, Kazuhiko Fujiwara, Yoshihiro Maruyama and Hiroyasu
  Itoh
Categories: cs.CV
Journal-ref: Analytica Chimica Acta, Volume 1087, 9 December 2019, Pages 11-19
DOI: 10.1016/j.aca.2019.08.064
\\
  We demonstrate a recognition and feature visualization method that uses a
deep convolutional neural network for Raman spectrum analysis. The
visualization is achieved by calculating important regions in the spectra from
weights in pooling and fully-connected layers. The method is first examined for
simple Lorentzian spectra, then applied to the spectra of pharmaceutical
compounds and numerically mixed amino acids. We investigate the effects of the
size and number of convolution filters on the extracted regions for Raman-peak
signals using the Lorentzian spectra. It is confirmed that the Raman peak
contributes to the recognition by visualizing the extracted features. A
near-zero weight value is obtained at the background level region, which
appears to be used for baseline correction. Common component extraction is
confirmed by an evaluation of numerically mixed amino acid spectra. High weight
values at the common peaks and negative values at the distinctive peaks appear,
even though the model is given one-hot vectors as the training labels (without
a mix ratio). This proposed method is potentially suitable for applications
such as the validation of trained models, ensuring the reliability of common
component extraction from compound samples for spectral analysis.
\\ ( https://arxiv.org/abs/2007.13354 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13373
Date: Mon, 27 Jul 2020 08:47:19 GMT   (1271kb,D)

Title: Part-Aware Data Augmentation for 3D Object Detection in Point Cloud
Authors: Jaeseok Choi, Yeji Song and Nojun Kwak
Categories: cs.CV
\\
  Data augmentation has greatly contributed to improving the performance in
image recognition tasks, and a lot of related studies have been conducted.
However, data augmentation on 3D point cloud data has not been much explored.
3D label has more sophisticated and rich structural information than the 2D
label, so it enables more diverse and effective data augmentation. In this
paper, we propose part-aware data augmentation (PA-AUG) that can better utilize
rich information of 3D label to enhance the performance of 3D object detectors.
PA-AUG divides objects into partitions and stochastically applies five novel
augmentation methods to each local region. It is compatible with existing point
cloud data augmentation methods and can be used universally regardless of the
detector's architecture. PA-AUG has improved the performance of
state-of-the-art 3D object detector for all classes of the KITTI dataset and
has the equivalent effect of increasing the train data by about 2.5$\times$. We
also show that PA-AUG not only increases performance for a given dataset but
also is robust to corrupted data. CODE WILL BE AVAILABLE.
\\ ( https://arxiv.org/abs/2007.13373 ,  1271kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13374
Date: Mon, 27 Jul 2020 08:47:50 GMT   (4020kb,D)

Title: Decomposed Generation Networks with Structure Prediction for Recipe
  Generation from Food Images
Authors: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
Categories: cs.CV
\\
  Recipe generation from food images and ingredients is a challenging task,
which requires the interpretation of the information from another modality.
Different from the image captioning task, where the captions usually have one
sentence, cooking instructions contain multiple sentences and have obvious
structures. To help the model capture the recipe structure and avoid missing
some cooking details, we propose a novel framework: Decomposed Generation
Networks (DGN) with structure prediction, to get more structured and complete
recipe generation outputs. To be specific, we split each cooking instruction
into several phases, and assign different sub-generators to each phase. Our
approach includes two novel ideas: (i) learning the recipe structures with the
global structure prediction component and (ii) producing recipe phases in the
sub-generator output component based on the predicted structure. Extensive
experiments on the challenging large-scale Recipe1M dataset validate the
effectiveness of our proposed model DGN, which improves the performance over
the state-of-the-art results.
\\ ( https://arxiv.org/abs/2007.13374 ,  4020kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13376
Date: Mon, 27 Jul 2020 08:51:55 GMT   (11749kb,D)

Title: NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination
Authors: Penghao Zhou, Chong Zhou, Pai Peng, Junlong Du, Xing Sun, Xiaowei Guo,
  Feiyue Huang
Categories: cs.CV
Comments: Accepted at the ACM International Conference on Multimedia (ACM MM)
  2020
\\
  Greedy-NMS inherently raises a dilemma, where a lower NMS threshold will
potentially lead to a lower recall rate and a higher threshold introduces more
false positives. This problem is more severe in pedestrian detection because
the instance density varies more intensively. However, previous works on NMS
don't consider or vaguely consider the factor of the existent of nearby
pedestrians. Thus, we propose Nearby Objects Hallucinator (NOH), which
pinpoints the objects nearby each proposal with a Gaussian distribution,
together with NOH-NMS, which dynamically eases the suppression for the space
that might contain other objects with a high likelihood. Compared to
Greedy-NMS, our method, as the state-of-the-art, improves by $3.9\%$ AP,
$5.1\%$ Recall, and $0.8\%$ $\text{MR}^{-2}$ on CrowdHuman to $89.0\%$ AP and
$92.9\%$ Recall, and $43.9\%$ $\text{MR}^{-2}$ respectively.
\\ ( https://arxiv.org/abs/2007.13376 ,  11749kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13393
Date: Mon, 27 Jul 2020 09:17:00 GMT   (3636kb,D)

Title: Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D
  Reconstruction with Symmetry
Authors: Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh
Categories: cs.CV
Comments: European Conference on Computer Vision 2020 (ECCV 2020)
\\
  Deep implicit field regression methods are effective for 3D reconstruction
from single-view images. However, the impact of different sampling patterns on
the reconstruction quality is not well-understood. In this work, we first study
the effect of point set discrepancy on the network training. Based on Farthest
Point Sampling algorithm, we propose a sampling scheme that theoretically
encourages better generalization performance, and results in fast convergence
for SGD-based optimization algorithms. Secondly, based on the reflective
symmetry of an object, we propose a feature fusion method that alleviates
issues due to self-occlusions which makes it difficult to utilize local image
features. Our proposed system Ladybird is able to create high quality 3D object
reconstructions from a single input image. We evaluate Ladybird on a large
scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms
of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).
\\ ( https://arxiv.org/abs/2007.13393 ,  3636kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13404
Date: Mon, 27 Jul 2020 09:50:11 GMT   (1645kb,D)

Title: YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart
  Camera Applications
Authors: Christos Kyrkou
Categories: cs.CV
\\
  Deep Learning-based object detectors can enhance the capabilities of smart
camera systems in a wide spectrum of machine vision applications including
video surveillance, autonomous driving, robots and drones, smart factory, and
health monitoring. Pedestrian detection plays a key role in all these
applications and deep learning can be used to construct accurate
state-of-the-art detectors. However, such complex paradigms do not scale easily
and are not traditionally implemented in resource-constrained smart cameras for
on-device processing which offers significant advantages in situations when
real-time monitoring and robustness are vital. Efficient neural networks can
not only enable mobile applications and on-device experiences but can also be a
key enabler of privacy and security allowing a user to gain the benefits of
neural networks without needing to send their data to the server to be
evaluated. This work addresses the challenge of achieving a good trade-off
between accuracy and speed for efficient deployment of deep-learning-based
pedestrian detection in smart camera applications. A computationally efficient
architecture is introduced based on separable convolutions and proposes
integrating dense connections across layers and multi-scale feature fusion to
improve representational capacity while decreasing the number of parameters and
operations. In particular, the contributions of this work are the following: 1)
An efficient backbone combining multi-scale feature operations, 2) a more
elaborate loss function for improved localization, 3) an anchor-less approach
for detection, The proposed approach called YOLOpeds is evaluated using the
PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides
real-time sustained operation of over 30 frames per second with detection rates
in the range of 86% outperforming existing deep learning models.
\\ ( https://arxiv.org/abs/2007.13404 ,  1645kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13406
Date: Mon, 27 Jul 2020 09:53:55 GMT   (692kb)

Title: Contraction Mapping of Feature Norms for Classifier Learning on the Data
  with Different Quality
Authors: Weihua Liu, Xiabi Liu, Murong Wang, Ling Ma and Yunde Jia
Categories: cs.CV cs.LG
\\
  The popular softmax loss and its recent extensions have achieved great
success in the deep learning-based image clas-sification. However, the data for
training image classifiers usually has different quality. Ignoring such
problem, the cor-rect classification of low quality data is hard to be solved.
In this paper, we discover the positive correlation between the feature norm of
an image and its quality through careful ex-periments on various applications
and various deep neural networks. Based on this finding, we propose a
contraction mapping function to compress the range of feature norms of training
images according to their quality and embed this con-traction mapping function
into softmax loss or its extensions to produce novel learning objectives. The
experiments on var-ious classification applications, including handwritten
digit recognition, lung nodule classification, face verification and face
recognition, demonstrate that the proposed approach is promising to effectively
deal with the problem of learning on the data with different quality and leads
to the significant and stable improvements in the classification accuracy.
\\ ( https://arxiv.org/abs/2007.13406 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13428
Date: Mon, 27 Jul 2020 11:04:57 GMT   (449kb,D)

Title: Two-Level Residual Distillation based Triple Network for Incremental
  Object Detection
Authors: Dongbao Yang, Yu Zhou, Dayan Wu, Can Ma, Fei Yang, Weiping Wang
Categories: cs.CV
\\
  Modern object detection methods based on convolutional neural network suffer
from severe catastrophic forgetting in learning new classes without original
data. Due to time consumption, storage burden and privacy of old data, it is
inadvisable to train the model from scratch with both old and new data when new
object classes emerge after the model trained. In this paper, we propose a
novel incremental object detector based on Faster R-CNN to continuously learn
from new object classes without using old data. It is a triple network where an
old model and a residual model as assistants for helping the incremental model
learning on new classes without forgetting the previous learned knowledge. To
better maintain the discrimination of features between old and new classes, the
residual model is jointly trained on new classes in the incremental learning
procedure. In addition, a corresponding distillation scheme is designed to
guide the training process, which consists of a two-level residual distillation
loss and a joint classification distillation loss. Extensive experiments on
VOC2007 and COCO are conducted, and the results demonstrate that the proposed
method can effectively learn to incrementally detect objects of new classes,
and the problem of catastrophic forgetting is mitigated in this context.
\\ ( https://arxiv.org/abs/2007.13428 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13467
Date: Mon, 27 Jul 2020 12:12:27 GMT   (1358kb,D)

Title: Identity-Guided Human Semantic Parsing for Person Re-Identification
Authors: Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, Jinqiao Wang
Categories: cs.CV
Comments: Accepted by ECCV 2020 spotlight
\\
  Existing alignment-based methods have to employ the pretrained human parsing
models to achieve the pixel-level alignment, and cannot identify the personal
belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In
this paper, we propose the identity-guided human semantic parsing approach
(ISP) to locate both the human body parts and personal belongings at
pixel-level for aligned person re-ID only with person identity labels. We
design the cascaded clustering on feature maps to generate the pseudo-labels of
human parts. Specifically, for the pixels of all images of a person, we first
group them to foreground or background and then group the foreground pixels to
human parts. The cluster assignments are subsequently used as pseudo-labels of
human parts to supervise the part estimation and ISP iteratively learns the
feature maps and groups them. Finally, local features of both human body parts
and personal belongings are obtained according to the selflearned part
estimation, and only features of visible parts are utilized for the retrieval.
Extensive experiments on three widely used datasets validate the superiority of
ISP over lots of state-of-the-art methods. Our code is available at
https://github.com/CASIA-IVA-Lab/ISP-reID.
\\ ( https://arxiv.org/abs/2007.13467 ,  1358kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13521
Date: Mon, 27 Jul 2020 13:03:32 GMT   (833kb,D)

Title: The Effect of Wearing a Mask on Face Recognition Performance: an
  Exploratory Study
Authors: Naser Damer, Jonas Henry Grebe, Cong Chen, Fadi Boutros, Florian
  Kirchbuchner and Arjan Kuijper
Categories: cs.CV
Comments: Accepted at BIOSIG2020
\\
  Face recognition has become essential in our daily lives as a convenient and
contactless method of accurate identity verification. Process such as identity
verification at automatic border control gates or the secure login to
electronic devices are increasingly dependant on such technologies. The recent
COVID-19 pandemic have increased the value of hygienic and contactless identity
verification. However, the pandemic led to the wide use of face masks,
essential to keep the pandemic under control. The effect of wearing a mask on
face recognition in a collaborative environment is currently sensitive yet
understudied issue. We address that by presenting a specifically collected
database containing three session, each with three different capture
instructions, to simulate realistic use cases. We further study the effect of
masked face probes on the behaviour of three top-performing face recognition
systems, two academic solutions and one commercial off-the-shelf (COTS) system.
\\ ( https://arxiv.org/abs/2007.13521 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13538
Date: Wed, 22 Jul 2020 15:34:01 GMT   (458kb)

Title: A Novel adaptive optimization of Dual-Tree Complex Wavelet Transform for
  Medical Image Fusion
Authors: T.Deepika, G.Karpaga Kannan
Categories: cs.CV eess.IV
Comments: Conference on Computing Communication and Signal Processing. arXiv
  admin note: text overlap with arXiv:2007.11488
\\
  In recent years, many research achievements are made in the medical image
fusion field. Fusion is basically extraction of best of inputs and conveying it
to the output. Medical Image fusion means that several of various modality
image information is comprehended together to form one image to express its
information. The aim of image fusion is to integrate complementary and
redundant information. In this paper, a multimodal image fusion algorithm based
on the dual-tree complex wavelet transform (DT-CWT) and adaptive particle swarm
optimization (APSO) is proposed. Fusion is achieved through the formation of a
fused pyramid using the DTCWT coefficients from the decomposed pyramids of the
source images. The coefficients are fused by the weighted average method based
on pixels, and the weights are estimated by the APSO to gain optimal fused
images. The fused image is obtained through conventional inverse dual-tree
complex wavelet transform reconstruction process. Experiment results show that
the proposed method based on adaptive particle swarm optimization algorithm is
remarkably better than the method based on particle swarm optimization. The
resulting fused images are compared visually and through benchmarks such as
Entropy (E), Peak Signal to Noise Ratio, (PSNR), Root Mean Square Error (RMSE),
Standard deviation (SD) and Structure Similarity Index Metric (SSIM)
computations.
\\ ( https://arxiv.org/abs/2007.13538 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13547
Date: Mon, 27 Jul 2020 13:28:50 GMT   (3793kb,D)

Title: Reconstruction Regularized Deep Metric Learning for Multi-label Image
  Classification
Authors: Changsheng Li and Chong Liu and Lixin Duan and Peng Gao and Kai Zheng
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by IEEE TNNLS
\\
  In this paper, we present a novel deep metric learning method to tackle the
multi-label image classification problem. In order to better learn the
correlations among images features, as well as labels, we attempt to explore a
latent space, where images and labels are embedded via two unique deep neural
networks, respectively. To capture the relationships between image features and
labels, we aim to learn a \emph{two-way} deep distance metric over the
embedding space from two different views, i.e., the distance between one image
and its labels is not only smaller than those distances between the image and
its labels' nearest neighbors, but also smaller than the distances between the
labels and other images corresponding to the labels' nearest neighbors.
Moreover, a reconstruction module for recovering correct labels is incorporated
into the whole framework as a regularization term, such that the label
embedding space is more representative. Our model can be trained in an
end-to-end manner. Experimental results on publicly available image datasets
corroborate the efficacy of our method compared with the state-of-the-arts.
\\ ( https://arxiv.org/abs/2007.13547 ,  3793kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13551
Date: Mon, 27 Jul 2020 13:31:41 GMT   (9724kb,D)

Title: Differentiable Manifold Reconstruction for Point Cloud Denoising
Authors: Shitong Luo, Wei Hu
Categories: cs.CV
Comments: This work has been accepted to ACM MM 2020
\\
  3D point clouds are often perturbed by noise due to the inherent limitation
of acquisition equipments, which obstructs downstream tasks such as surface
reconstruction, rendering and so on. Previous works mostly infer the
displacement of noisy points from the underlying surface, which however are not
designated to recover the surface explicitly and may lead to sub-optimal
denoising results. To this end, we propose to learn the underlying manifold of
a noisy point cloud from differentiably subsampled points with trivial noise
perturbation and their embedded neighborhood feature, aiming to capture
intrinsic structures in point clouds. Specifically, we present an
autoencoder-like neural network. The encoder learns both local and non-local
feature representations of each point, and then samples points with low noise
via an adaptive differentiable pooling operation. Afterwards, the decoder
infers the underlying manifold by transforming each sampled point along with
the embedded feature of its neighborhood to a local surface centered around the
point. By resampling on the reconstructed manifold, we obtain a denoised point
cloud. Further, we design an unsupervised training loss, so that our network
can be trained in either an unsupervised or supervised fashion. Experiments
show that our method significantly outperforms state-of-the-art denoising
methods under both synthetic noise and real world noise. The code and data are
available at https://github.com/luost26/DMRDenoise
\\ ( https://arxiv.org/abs/2007.13551 ,  9724kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13559
Date: Fri, 24 Jul 2020 14:56:12 GMT   (14691kb,D)

Title: MADGAN: unsupervised Medical Anomaly Detection GAN using multiple
  adjacent brain MRI slice reconstruction
Authors: Changhee Han, Leonardo Rundo, Kohei Murao, Tomoyuki Noguchi, Yuki
  Shimahara, Zoltan Adam Milacski, Saori Koshino, Evis Sala, Hideki Nakayama,
  Shinichi Satoh
Categories: cs.CV cs.LG eess.IV
Comments: 21 pages, 11 figures, submitted to BMC Bioinformatics. arXiv admin
  note: substantial text overlap with arXiv:1906.06114
\\
  Unsupervised learning can discover various unseen diseases, relying on
large-scale unannotated medical images of healthy subjects. Towards this,
unsupervised methods reconstruct a 2D/3D single medical image to detect
outliers either in the learned feature space or from high reconstruction loss.
However, without considering continuity between multiple adjacent slices, they
cannot directly discriminate diseases composed of the accumulation of subtle
anatomical anomalies, such as Alzheimer's Disease (AD). Moreover, no study has
shown how unsupervised anomaly detection is associated with either disease
stages, various (i.e., more than two types of) diseases, or multi-sequence
Magnetic Resonance Imaging (MRI) scans. Therefore, we propose unsupervised
Medical Anomaly Detection Generative Adversarial Network (MADGAN), a novel
two-step method using GAN-based multiple adjacent brain MRI slice
reconstruction to detect various diseases at different stages on multi-sequence
structural MRI: (Reconstruction) Wasserstein loss with Gradient Penalty + 100
L1 loss-trained on 3 healthy brain axial MRI slices to reconstruct the next 3
ones-reconstructs unseen healthy/abnormal scans; (Diagnosis) Average L2 loss
per scan discriminates them, comparing the ground truth/reconstructed slices.
For training, we use 1,133 healthy T1-weighted (T1) and 135 healthy
contrast-enhanced T1 (T1c) brain MRI scans. Our Self-Attention MADGAN can
detect AD on T1 scans at a very early stage, Mild Cognitive Impairment (MCI),
with Area Under the Curve (AUC) 0.727, and AD at a late stage with AUC 0.894,
while detecting brain metastases on T1c scans with AUC 0.921.
\\ ( https://arxiv.org/abs/2007.13559 ,  14691kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13595
Date: Tue, 21 Jul 2020 11:01:36 GMT   (6956kb,D)

Title: SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional
  Neural Networks Training
Authors: Pengcheng Dai, Jianlei Yang, Xucheng Ye, Xingzhou Cheng, Junyu Luo,
  Linghao Song, Yiran Chen, Weisheng Zhao
Categories: cs.CV cs.AR
Comments: published on DAC 2020
\\
  Training Convolutional Neural Networks (CNNs) usually requires a large number
of computational resources. In this paper, \textit{SparseTrain} is proposed to
accelerate CNN training by fully exploiting the sparsity. It mainly involves
three levels of innovations: activation gradients pruning algorithm, sparse
training dataflow, and accelerator architecture. By applying a stochastic
pruning algorithm on each layer, the sparsity of back-propagation gradients can
be increased dramatically without degrading training accuracy and convergence
rate. Moreover, to utilize both \textit{natural sparsity} (resulted from ReLU
or Pooling layers) and \textit{artificial sparsity} (brought by pruning
algorithm), a sparse-aware architecture is proposed for training acceleration.
This architecture supports forward and back-propagation of CNN by adopting
1-Dimensional convolution dataflow. We have built %a simple compiler to map
CNNs topology onto \textit{SparseTrain}, and a cycle-accurate architecture
simulator to evaluate the performance and efficiency based on the synthesized
design with $14nm$ FinFET technologies. Evaluation results on AlexNet/ResNet
show that \textit{SparseTrain} could achieve about $2.7 \times$ speedup and
$2.2 \times$ energy efficiency improvement on average compared with the
original training process.
\\ ( https://arxiv.org/abs/2007.13595 ,  6956kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13632
Date: Mon, 27 Jul 2020 15:17:52 GMT   (9006kb,D)

Title: Towards Accuracy-Fairness Paradox: Adversarial Example-based Data
  Augmentation for Visual Debiasing
Authors: Yi Zhang, Jitao Sang
Categories: cs.CV cs.LG
\\
  Machine learning fairness concerns about the biases towards certain protected
or sensitive group of people when addressing the target tasks. This paper
studies the debiasing problem in the context of image classification tasks. Our
data analysis on facial attribute recognition demonstrates (1) the attribution
of model bias from imbalanced training data distribution and (2) the potential
of adversarial examples in balancing data distribution. We are thus motivated
to employ adversarial example to augment the training data for visual
debiasing. Specifically, to ensure the adversarial generalization as well as
cross-task transferability, we propose to couple the operations of target task
classifier training, bias task classifier training, and adversarial example
generation. The generated adversarial examples supplement the target task
training dataset via balancing the distribution over bias variables in an
online fashion. Results on simulated and real-world debiasing experiments
demonstrate the effectiveness of the proposed solution in simultaneously
improving model accuracy and fairness. Preliminary experiment on few-shot
learning further shows the potential of adversarial attack-based pseudo sample
generation as alternative solution to make up for the training data lackage.
\\ ( https://arxiv.org/abs/2007.13632 ,  9006kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13635
Date: Mon, 27 Jul 2020 15:25:38 GMT   (3692kb,D)

Title: Black-Box Face Recovery from Identity Features
Authors: Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam Tursynbek,
  and Aleksandr Petyushko
Categories: cs.CV
\\
  In this work, we present a novel algorithm based on an it-erative sampling of
random Gaussian blobs for black-box face recovery,given only an output feature
vector of deep face recognition systems. Weattack the state-of-the-art face
recognition system (ArcFace) to test ouralgorithm. Another network with
different architecture (FaceNet) is usedas an independent critic showing that
the target person can be identi-fied with the reconstructed image even with no
access to the attackedmodel. Furthermore, our algorithm requires a
significantly less numberof queries compared to the state-of-the-art solution.
\\ ( https://arxiv.org/abs/2007.13635 ,  3692kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13638
Date: Mon, 27 Jul 2020 15:39:19 GMT   (392kb,D)

Title: Message Passing Least Squares Framework and its Application to Rotation
  Synchronization
Authors: Yunpeng Shi and Gilad Lerman
Categories: cs.CV cs.IT math.IT stat.ML
Comments: To appear in ICML 2020
MSC-class: 90C26, 90C17, 68Q87, 65C20, 90-08, 60-08
ACM-class: G.1.6; I.4.0
\\
  We propose an efficient algorithm for solving group synchronization under
high levels of corruption and noise, while we focus on rotation
synchronization. We first describe our recent theoretically guaranteed message
passing algorithm that estimates the corruption levels of the measured group
ratios. We then propose a novel reweighted least squares method to estimate the
group elements, where the weights are initialized and iteratively updated using
the estimated corruption levels. We demonstrate the superior performance of our
algorithm over state-of-the-art methods for rotation synchronization using both
synthetic and real data.
\\ ( https://arxiv.org/abs/2007.13638 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13640
Date: Mon, 27 Jul 2020 15:40:46 GMT   (8062kb,D)

Title: Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser
Authors: Zahra Kadkhodaie and Eero P. Simoncelli
Categories: cs.CV eess.IV stat.ML
Comments: 15 pages, 13 figures
\\
  Prior probability models are a central component of many image processing
problems, but density estimation is notoriously difficult for high-dimensional
signals such as photographic images. Deep neural networks have provided
state-of-the-art solutions for problems such as denoising, which implicitly
rely on a prior probability model of natural images. Here, we develop a robust
and general methodology for making use of this implicit prior. We rely on a
little-known statistical result due to Miyasawa (1961), who showed that the
least-squares solution for removing additive Gaussian noise can be written
directly in terms of the gradient of the log of the noisy signal density. We
use this fact to develop a stochastic coarse-to-fine gradient ascent procedure
for drawing high-probability samples from the implicit prior embedded within a
CNN trained to perform blind (i.e., unknown noise level) least-squares
denoising. A generalization of this algorithm to constrained sampling provides
a method for using the implicit prior to solve any linear inverse problem, with
no additional training. We demonstrate this general form of transfer learning
in multiple applications, using the same algorithm to produce high-quality
solutions for deblurring, super-resolution, inpainting, and compressive
sensing.
\\ ( https://arxiv.org/abs/2007.13640 ,  8062kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13666
Date: Mon, 27 Jul 2020 16:19:52 GMT   (10423kb,D)

Title: 3D Human Shape and Pose from a Single Low-Resolution Image with
  Self-Supervised Learning
Authors: Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando
  De la Torre
Categories: cs.CV cs.AI cs.LG
Comments: ECCV 2020, project page:
  https://sites.google.com/view/xiangyuxu/3d_eccv20
\\
  3D human shape and pose estimation from monocular images has been an active
area of research in computer vision, having a substantial impact on the
development of new applications, from activity recognition to creating virtual
avatars. Existing deep learning methods for 3D human shape and pose estimation
rely on relatively high-resolution input images; however, high-resolution
visual content is not always available in several practical scenarios such as
video surveillance and sports broadcasting. Low-resolution images in real
scenarios can vary in a wide range of sizes, and a model trained in one
resolution does not typically degrade gracefully across resolutions. Two common
approaches to solve the problem of low-resolution input are applying
super-resolution techniques to the input images which may result in visual
artifacts, or simply training one model for each resolution, which is
impractical in many realistic applications. To address the above issues, this
paper proposes a novel algorithm called RSC-Net, which consists of a
Resolution-aware network, a Self-supervision loss, and a Contrastive learning
scheme. The proposed network is able to learn the 3D body shape and pose across
different resolutions with a single model. The self-supervision loss encourages
scale-consistency of the output, and the contrastive learning scheme enforces
scale-consistency of the deep features. We show that both these new training
losses provide robustness when learning 3D shape and pose in a
weakly-supervised manner. Extensive experiments demonstrate that the RSC-Net
can achieve consistently better results than the state-of-the-art methods for
challenging low-resolution images.
\\ ( https://arxiv.org/abs/2007.13666 ,  10423kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13683
Date: Mon, 27 Jul 2020 16:51:25 GMT   (5687kb,D)

Title: Ordinary Differential Equation and Complex Matrix Exponential for
  Multi-resolution Image Registration
Authors: Abhishek Nan and Matthew Tennant and Uriel Rubin and Nilanjan Ray
Categories: cs.CV
Comments: Software: https://github.com/abnan/ODECME
\\
  Autograd-based software packages have recently renewed interest in image
registration using homography and other geometric models by gradient descent
and optimization, e.g., AirLab and DRMIME. In this work, we emphasize on using
complex matrix exponential (CME) over real matrix exponential to compute
transformation matrices. CME is theoretically more suitable and practically
provides faster convergence as our experiments show. Further, we demonstrate
that the use of an ordinary differential equation (ODE) as an optimizable
dynamical system can adapt the transformation matrix more accurately to the
multi-resolution Gaussian pyramid for image registration. Our experiments
include four publicly available benchmark datasets, two of them 2D and the
other two being 3D. Experiments demonstrate that our proposed method yields
significantly better registration compared to a number of off-the-shelf,
popular, state-of-the-art image registration toolboxes.
\\ ( https://arxiv.org/abs/2007.13683 ,  5687kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13693
Date: Mon, 27 Jul 2020 17:13:14 GMT   (16553kb,D)

Title: A Closer Look at Art Mediums: The MAMe Image Classification Dataset
Authors: Ferran Par\'es, Anna Arias-Duart, Dario Garcia-Gasulla, Gema
  Campo-Franc\'es, Nina Viladrich, Eduard Ayguad\'e, Jes\'us Labarta
Categories: cs.CV cs.LG
\\
  Art is an expression of human creativity, skill and technology. An
exceptionally rich source of visual content. In the context of AI image
processing systems, artworks represent one of the most challenging domains
conceivable: Properly perceiving art requires attention to detail, a huge
generalization capacity, and recognizing both simple and complex visual
patterns. To challenge the AI community, this work introduces a novel image
classification task focused on museum art mediums, the MAMe dataset. Data is
gathered from three different museums, and aggregated by art experts into 29
classes of medium (i.e. materials and techniques). For each class, MAMe
provides a minimum of 850 images (700 for training) of high-resolution and
variable shape. The combination of volume, resolution and shape allows MAMe to
fill a void in current image classification challenges, empowering research in
aspects so far overseen by the research community. After reviewing the
singularity of MAMe in the context of current image classification tasks, a
thorough description of the task is provided, together with dataset statistics.
Baseline experiments are conducted using well-known architectures, to highlight
both the feasibility and complexity of the task proposed. Finally, these
baselines are inspected using explainability methods and expert knowledge, to
gain insight on the challenges that remain ahead.
\\ ( https://arxiv.org/abs/2007.13693 ,  16553kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13704
Date: Mon, 27 Jul 2020 17:31:24 GMT   (1672kb,D)

Title: WGANVO: Monocular Visual Odometry based on Generative Adversarial
  Networks
Authors: Javier Cremona, Lucas Uzal, Taih\'u Pire
Categories: cs.CV cs.RO
\\
  In this work we present WGANVO, a Deep Learning based monocular Visual
Odometry method. In particular, a neural network is trained to regress a pose
estimate from an image pair. The training is performed using a semi-supervised
approach. Unlike geometry based monocular methods, the proposed method can
recover the absolute scale of the scene without neither prior knowledge nor
extra information. The evaluation of the system is carried out on the
well-known KITTI dataset where it is shown to work in real time and the
accuracy obtained is encouraging to continue the development of Deep Learning
based methods.
\\ ( https://arxiv.org/abs/2007.13704 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13712
Date: Mon, 27 Jul 2020 17:45:21 GMT   (12052kb,D)

Title: The Unsupervised Method of Vessel Movement Trajectory Prediction
Authors: Chih-Wei Chen, Charles Harrison, and Hsin-Hsiung Huang
Categories: cs.CV stat.AP
\\
  In real-world application scenarios, it is crucial for marine navigators and
security analysts to predict vessel movement trajectories at sea based on the
Automated Identification System (AIS) data in a given time span. This article
presents an unsupervised method of ship movement trajectory prediction which
represents the data in a three-dimensional space which consists of time
difference between points, the scaled error distance between the tested and its
predicted forward and backward locations, and the space-time angle. The
representation feature space reduces the search scope for the next point to a
collection of candidates which fit the local path prediction well, and
therefore improve the accuracy. Unlike most statistical learning or deep
learning methods, the proposed clustering-based trajectory reconstruction
method does not require computationally expensive model training. This makes
real-time reliable and accurate prediction feasible without using a training
set. Our results show that the most prediction trajectories accurately consist
of the true vessel paths.
\\ ( https://arxiv.org/abs/2007.13712 ,  12052kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13727
Date: Mon, 27 Jul 2020 17:58:53 GMT   (38726kb,D)

Title: Associative3D: Volumetric Reconstruction from Sparse Views
Authors: Shengyi Qian, Linyi Jin, David F. Fouhey
Categories: cs.CV
Comments: ECCV 2020
\\
  This paper studies the problem of 3D volumetric reconstruction from two views
of a scene with an unknown camera. While seemingly easy for humans, this
problem poses many challenges for computers since it requires simultaneously
reconstructing objects in the two views while also figuring out their
relationship. We propose a new approach that estimates reconstructions,
distributions over the camera/object and camera/camera transformations, as well
as an inter-view object affinity matrix. This information is then jointly
reasoned over to produce the most likely explanation of the scene. We train and
test our approach on a dataset of indoor scenes, and rigorously evaluate the
merits of our joint reasoning approach. Our experiments show that it is able to
recover reasonable scenes from sparse views, while the problem is still
challenging. Project site: https://jasonqsy.github.io/Associative3D
\\ ( https://arxiv.org/abs/2007.13727 ,  38726kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13729
Date: Mon, 27 Jul 2020 17:59:08 GMT   (17375kb,D)

Title: Noisy Agents: Self-supervised Exploration by Predicting Auditory Events
Authors: Chuang Gan, Xiaoyu Chen, Phillip Isola, Antonio Torralba, Joshua B.
  Tenenbaum
Categories: cs.CV cs.AI cs.LG cs.RO cs.SD eess.AS
Comments: Project page: http://noisy-agent.csail.mit.edu
\\
  Humans integrate multiple sensory modalities (e.g. visual and audio) to build
a causal understanding of the physical world. In this work, we propose a novel
type of intrinsic motivation for Reinforcement Learning (RL) that encourages
the agent to understand the causal effect of its actions through auditory event
prediction. First, we allow the agent to collect a small amount of acoustic
data and use K-means to discover underlying auditory event clusters. We then
train a neural network to predict the auditory events and use the prediction
errors as intrinsic rewards to guide RL exploration. Experimental results on
Atari games show that our new intrinsic motivation significantly outperforms
several state-of-the-art baselines. We further visualize our noisy agents'
behavior in a physics environment and demonstrate that our newly designed
intrinsic reward leads to the emergence of physical interaction behaviors (e.g.
contact with objects).
\\ ( https://arxiv.org/abs/2007.13729 ,  17375kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13732
Date: Mon, 27 Jul 2020 17:59:49 GMT   (852kb,D)

Title: Learning Lane Graph Representations for Motion Forecasting
Authors: Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, Raquel
  Urtasun
Categories: cs.CV
Comments: ECCV 2020 Oral
\\
  We propose a motion forecasting model that exploits a novel structured map
representation as well as actor-map interactions. Instead of encoding
vectorized maps as raster images, we construct a lane graph from raw map data
to explicitly preserve the map structure. To capture the complex topology and
long range dependencies of the lane graph, we propose LaneGCN which extends
graph convolutions with multiple adjacency matrices and along-lane dilation. To
capture the complex interactions between actors and maps, we exploit a fusion
network consisting of four types of interactions, actor-to-lane, lane-to-lane,
lane-to-actor and actor-to-actor. Powered by LaneGCN and actor-map
interactions, our model is able to predict accurate and realistic multi-modal
trajectories. Our approach significantly outperforms the state-of-the-art on
the large scale Argoverse motion forecasting benchmark.
\\ ( https://arxiv.org/abs/2007.13732 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13132
Date: Sun, 26 Jul 2020 14:11:39 GMT   (4kb)

Title: The Italian domination numbers of some products of digraphs
Authors: Kijung Kim
Categories: cs.DM math.CO
\\
  An Italian dominating function on a digraph $D$ with vertex set $V(D)$ is
defined as a function $f : V(D) \rightarrow \{0, 1, 2\}$ such that every vertex
$v \in V(D)$ with $f(v) = 0$ has at least two in-neighbors assigned $1$ under
$f$ or one in-neighbor $w$ with $f(w) = 2$. In this paper, we determine the
exact values of the Italian domination numbers of some products of digraphs.
\\ ( https://arxiv.org/abs/2007.13132 ,  4kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2007.12764 (*cross-listing*)
Date: Fri, 24 Jul 2020 20:40:10 GMT   (600kb)

Title: Selection of Proper EEG Channels for Subject Intention Classification
  Using Deep Learning
Authors: Ghazale Ghorbanzade, Zahra Nabizadeh-ShahreBabak, Shadrokh Samavi,
  Nader Karimi, Ali Emami, Pejman Khadivi
Categories: eess.SP cs.CV q-bio.NC
Comments: 5 pages 2 figures
\\
  Brain signals could be used to control devices to assist individuals with
disabilities. Signals such as electroencephalograms are complicated and hard to
interpret. A set of signals are collected and should be classified to identify
the intention of the subject. Different approaches have tried to reduce the
number of channels before sending them to a classifier. We are proposing a deep
learning-based method for selecting an informative subset of channels that
produce high classification accuracy. The proposed network could be trained for
an individual subject for the selection of an appropriate set of channels.
Reduction of the number of channels could reduce the complexity of
brain-computer-interface devices. Our method could find a subset of channels.
The accuracy of our approach is comparable with a model trained on all
channels. Hence, our model's temporal and power costs are low, while its
accuracy is kept high.
\\ ( https://arxiv.org/abs/2007.12764 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12813 (*cross-listing*)
Date: Sat, 25 Jul 2020 00:40:46 GMT   (2254kb)

Title: All-Optical Information Processing Capacity of Diffractive Surfaces
Authors: Onur Kulce, Deniz Mengu, Yair Rivenson, Aydogan Ozcan
Categories: eess.IV cs.CV cs.NE physics.optics
Comments: 28 Pages, 6 Figures, 1 Table
\\
  Precise engineering of materials and surfaces has been at the heart of some
of the recent advances in optics and photonics. These advances around the
engineering of materials with new functionalities have also opened up exciting
avenues for designing trainable surfaces that can perform computation and
machine learning tasks through light-matter interaction and diffraction. Here,
we analyze the information processing capacity of coherent optical networks
formed by diffractive surfaces that are trained to perform an all-optical
computational task between a given input and output field-of-view. We prove
that the dimensionality of the all-optical solution space covering the
complex-valued transformations between the input and output fields-of-view is
linearly proportional to the number of diffractive surfaces within the optical
network, up to a limit that is dictated by the extent of the input and output
fields-of-view. Deeper diffractive networks that are composed of larger numbers
of trainable surfaces can cover a higher dimensional subspace of the
complex-valued linear transformations between a larger input field-of-view and
a larger output field-of-view, and exhibit depth advantages in terms of their
statistical inference, learning and generalization capabilities for different
image classification tasks, when compared with a single trainable diffractive
surface. These analyses and conclusions are broadly applicable to various forms
of diffractive surfaces, including e.g., plasmonic and/or dielectric-based
metasurfaces and flat optics that can be used to form all-optical processors.
\\ ( https://arxiv.org/abs/2007.12813 ,  2254kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12829 (*cross-listing*)
Date: Sat, 25 Jul 2020 01:57:57 GMT   (574kb)

Title: Joint Featurewise Weighting and Lobal Structure Learning for Multi-view
  Subspace Clustering
Authors: Shi-Xun Lina, Guo Zhongb, Ting Shu
Categories: cs.LG cs.CV stat.ML
\\
  Multi-view clustering integrates multiple feature sets, which reveal distinct
aspects of the data and provide complementary information to each other, to
improve the clustering performance. It remains challenging to effectively
exploit complementary information across multiple views since the original data
often contain noise and are highly redundant. Moreover, most existing
multi-view clustering methods only aim to explore the consistency of all views
while ignoring the local structure of each view. However, it is necessary to
take the local structure of each view into consideration, because different
views would present different geometric structures while admitting the same
cluster structure. To address the above issues, we propose a novel multi-view
subspace clustering method via simultaneously assigning weights for different
features and capturing local information of data in view-specific
self-representation feature spaces. Especially, a common cluster structure
regularization is adopted to guarantee consistency among different views. An
efficient algorithm based on an augmented Lagrangian multiplier is also
developed to solve the associated optimization problem. Experiments conducted
on several benchmark datasets demonstrate that the proposed method achieves
state-of-the-art performance. We provide the Matlab code on
https://github.com/Ekin102003/JFLMSC.
\\ ( https://arxiv.org/abs/2007.12829 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12858 (*cross-listing*)
Date: Sat, 25 Jul 2020 05:29:34 GMT   (6309kb,D)

Title: Modal Uncertainty Estimation via Discrete Latent Representation
Authors: Di Qiu, Lok Ming Lui
Categories: cs.LG cs.CV stat.ML
\\
  Many important problems in the real world don't have unique solutions. It is
thus important for machine learning models to be capable of proposing different
plausible solutions with meaningful probability measures. In this work we
introduce such a deep learning framework that learns the one-to-many mappings
between the inputs and outputs, together with faithful uncertainty measures. We
call our framework {\it modal uncertainty estimation} since we model the
one-to-many mappings to be generated through a set of discrete latent
variables, each representing a latent mode hypothesis that explains the
corresponding type of input-output relationship. The discrete nature of the
latent representations thus allows us to estimate for any input the conditional
probability distribution of the outputs very effectively. Both the discrete
latent space and its uncertainty estimation are jointly learned during
training. We motivate our use of discrete latent space through the multi-modal
posterior collapse problem in current conditional generative models, then
develop the theoretical background, and extensively validate our method on both
synthetic and realistic tasks. Our framework demonstrates significantly more
accurate uncertainty estimation than the current state-of-the-art methods, and
is informative and convenient for practical use.
\\ ( https://arxiv.org/abs/2007.12858 ,  6309kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12898 (*cross-listing*)
Date: Sat, 25 Jul 2020 10:01:22 GMT   (2701kb,D)

Title: 3D Neural Network for Lung Cancer Risk Prediction on CT Volumes
Authors: Daniel Korat
Categories: eess.IV cs.CV
\\
  With an estimated 160,000 deaths in 2018, lung cancer is the most common
cause of cancer death in the United States. Lung cancer CT screening has been
shown to reduce mortality by up to 40% and is now included in US screening
guidelines. Reducing the high error rates in lung cancer screening is
imperative because of the high clinical and financial costs caused by diagnosis
mistakes. Despite the use of standards for radiological diagnosis, persistent
inter-grader variability and incomplete characterization of comprehensive
imaging findings remain as limitations of current methods. These limitations
suggest opportunities for more sophisticated systems to improve performance and
inter-reader consistency. In this report, we reproduce a state-of-the-art deep
learning algorithm for lung cancer risk prediction. Our model predicts
malignancy probability and risk bucket classification from lung CT studies.
This allows for risk categorization of patients being screened and suggests the
most appropriate surveillance and management. Combining our solution high
accuracy, consistency and fully automated nature, our approach may enable
highly efficient screening procedures and accelerate the adoption of lung
cancer screening.
\\ ( https://arxiv.org/abs/2007.12898 ,  2701kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12909 (*cross-listing*)
Date: Sat, 25 Jul 2020 10:55:04 GMT   (1395kb,D)

Title: CNN Detection of GAN-Generated Face Images based on Cross-Band
  Co-occurrences Analysis
Authors: Mauro Barni, Kassem Kallas, Ehsan Nowroozi, Benedetta Tondi
Categories: cs.CR cs.CV cs.LG eess.IV
Comments: 6 pages, 2 figures, 4 tables, WIFS 2020 NY USA
\\
  Last-generation GAN models allow to generate synthetic images which are
visually indistinguishable from natural ones, raising the need to develop tools
to distinguish fake and natural images thus contributing to preserve the
trustworthiness of digital images. While modern GAN models can generate very
high-quality images with no visible spatial artifacts, reconstruction of
consistent relationships among colour channels is expectedly more difficult. In
this paper, we propose a method for distinguishing GAN-generated from natural
images by exploiting inconsistencies among spectral bands, with specific focus
on the generation of synthetic face images. Specifically, we use cross-band
co-occurrence matrices, in addition to spatial co-occurrence matrices, as input
to a CNN model, which is trained to distinguish between real and synthetic
faces. The results of our experiments confirm the goodness of our approach
which outperforms a similar detection technique based on intra-band spatial
co-occurrences only. The performance gain is particularly significant with
regard to robustness against post-processing, like geometric transformations,
filtering and contrast manipulations.
\\ ( https://arxiv.org/abs/2007.12909 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12911 (*cross-listing*)
Date: Sat, 25 Jul 2020 11:02:16 GMT   (299kb,D)

Title: Tighter risk certificates for neural networks
Authors: Mar\'ia P\'erez-Ortiz and Omar Rivasplata and John Shawe-Taylor and
  Csaba Szepesv\'ari
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: Preprint under review
\\
  This paper presents empirical studies regarding training probabilistic neural
networks using training objectives derived from PAC-Bayes bounds. In the
context of probabilistic neural networks, the output of training is a
probability distribution over network weights. We present two training
objectives, used here for the first time in connection with training neural
networks. These two training objectives are derived from tight PAC-Bayes
bounds, one of which is new. We also re-implement a previously used training
objective based on a classical PAC-Bayes bound, to compare the properties of
the predictors learned using the different training objectives. We compute risk
certificates that are valid on any unseen examples for the learnt predictors.
We further experiment with different types of priors on the weights (both
data-free and data-dependent priors) and neural network architectures. Our
experiments on MNIST and CIFAR-10 show that our training methods produce
competitive test set errors and non-vacuous risk bounds with much tighter
values than previous results in the literature, showing promise not only to
guide the learning algorithm through bounding the risk but also for model
selection. These observations suggest that the methods studied here might be
good candidates for self-bounding learning.
\\ ( https://arxiv.org/abs/2007.12911 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13044 (*cross-listing*)
Date: Sun, 26 Jul 2020 02:36:56 GMT   (540kb)

Title: A Preliminary Exploration into an Alternative CellLineNet: An
  Evolutionary Approach
Authors: Akwarandu Ugo Nwachuku, Xavier Lewis-Palmer, Darlington Ahiale Akogo
Categories: cs.NE cs.CV eess.IV
\\
  Within this paper, the exploration of an evolutionary approach to an
alternative CellLineNet: a convolutional neural network adept at the
classification of epithelial breast cancer cell lines, is presented. This
evolutionary algorithm introduces control variables that guide the search of
architectures in the search space of inverted residual blocks, bottleneck
blocks, residual blocks and a basic 2x2 convolutional block. The promise of
EvoCELL is predicting what combination or arrangement of the feature extracting
blocks that produce the best model architecture for a given task. Therein, the
performance of how the fittest model evolved after each generation is shown.
The final evolved model CellLineNet V2 classifies 5 types of epithelial breast
cell lines consisting of two human cancer lines, 2 normal immortalized lines,
and 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A and HC11). The
Multiclass Cell Line Classification Convolutional Neural Network extends our
earlier work on a Binary Breast Cancer Cell Line Classification model. This
paper presents an on-going exploratory approach to neural network architecture
design and is presented for further study.
\\ ( https://arxiv.org/abs/2007.13044 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13083 (*cross-listing*)
Date: Sun, 26 Jul 2020 08:56:47 GMT   (6086kb)

Title: MACU-Net Semantic Segmentation from High-Resolution Remote Sensing
  Images
Authors: Rui Li and Chenxi Duan and Shunyi Zheng
Categories: eess.IV cs.CV
\\
  Semantic segmentation of remote sensing images plays an important role in
land resource management, yield estimation, and economic assessment. U-Net is a
sophisticated encoder-decoder architecture which has been frequently used in
medical image segmentation and has attained prominent performance. And
asymmetric convolution block can enhance the square convolution kernels using
asymmetric convolutions. In this paper, based on U-Net and asymmetric
convolution block, we incorporate multi-scale features generated by different
layers of U-Net and design a multi-scale skip connected architecture, MACU-Net,
for semantic segmentation using high-resolution remote sensing images. Our
design has the following advantages: (1) The multi-scale skip connections
combine and realign semantic features contained both in low-level and
high-level feature maps with different scales; (2) the asymmetric convolution
block strengthens the representational capacity of a standard convolution
layer. Experiments conducted on two remote sensing image datasets captured by
separate satellites demonstrate that the performance of our MACU-Net transcends
the U-Net, SegNet, DeepLab V3+, and other baseline algorithms.
\\ ( https://arxiv.org/abs/2007.13083 ,  6086kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13101 (*cross-listing*)
Date: Sun, 26 Jul 2020 11:32:52 GMT   (6823kb,D)

Title: Regularized Flexible Activation Function Combinations for Deep Neural
  Networks
Authors: Renlong Jie, Junbin Gao, Andrey Vasnev, Min-ngoc Tran
Categories: cs.NE cs.CV
\\
  Activation in deep neural networks is fundamental to achieving non-linear
mappings. Traditional studies mainly focus on finding fixed activations for a
particular set of learning tasks or model architectures. The research on
flexible activation is quite limited in both designing philosophy and
application scenarios. In this study, three principles of choosing flexible
activation components are proposed and a general combined form of flexible
activation functions is implemented. Based on this, a novel family of flexible
activation functions that can replace sigmoid or tanh in LSTM cells are
implemented, as well as a new family by combining ReLU and ELUs. Also, two new
regularisation terms based on assumptions as prior knowledge are introduced. It
has been shown that LSTM models with proposed flexible activations P-Sig-Ramp
provide significant improvements in time series forecasting, while the proposed
P-E2-ReLU achieves better and more stable performance on lossy image
compression tasks with convolutional auto-encoders. In addition, the proposed
regularization terms improve the convergence, performance and stability of the
models with flexible activation functions.
\\ ( https://arxiv.org/abs/2007.13101 ,  6823kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13118 (*cross-listing*)
Date: Sun, 26 Jul 2020 12:32:34 GMT   (950kb,D)

Title: UIAI System for Short-Duration Speaker Verification Challenge 2020
Authors: Md Sahidullah, Achintya Kumar Sarkar, Ville Vestman, Xuechen Liu,
  Romain Serizel, Tomi Kinnunen, Zheng-Hua Tan, Emmanuel Vincent
Categories: eess.AS cs.CV cs.SD
\\
  In this work, we present the system description of the UIAI entry for the
short-duration speaker verification (SdSV) challenge 2020. Our focus is on Task
1 dedicated to text-dependent speaker verification. We investigate different
feature extraction and modeling approaches for automatic speaker verification
(ASV) and utterance verification (UV). We have also studied different fusion
strategies for combining UV and ASV modules. Our primary submission to the
challenge is the fusion of seven subsystems which yields a normalized minimum
detection cost function (minDCF) of 0.072 and an equal error rate (EER) of
2.14% on the evaluation set. The single system consisting of a pass-phrase
identification based model with phone-discriminative bottleneck features gives
a normalized minDCF of 0.118 and achieves 19% relative improvement over the
state-of-the-art challenge baseline.
\\ ( https://arxiv.org/abs/2007.13118 ,  950kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13224 (*cross-listing*)
Date: Sun, 26 Jul 2020 21:53:47 GMT   (3091kb,D)

Title: Uniformizing Techniques to Process CT scans with 3D CNNs for
  Tuberculosis Prediction
Authors: Hasib Zunair, Aimon Rahman, Nabeel Mohammed, Joseph Paul Cohen
Categories: eess.IV cs.CV
Comments: Accepted for publication at the MICCAI 2020 International Workshop on
  PRedictive Intelligence In MEdicine (PRIME)
\\
  A common approach to medical image analysis on volumetric data uses deep 2D
convolutional neural networks (CNNs). This is largely attributed to the
challenges imposed by the nature of the 3D data: variable volume size, GPU
exhaustion during optimization. However, dealing with the individual slices
independently in 2D CNNs deliberately discards the depth information which
results in poor performance for the intended task. Therefore, it is important
to develop methods that not only overcome the heavy memory and computation
requirements but also leverage the 3D information. To this end, we evaluate a
set of volume uniformizing methods to address the aforementioned issues. The
first method involves sampling information evenly from a subset of the volume.
Another method exploits the full geometry of the 3D volume by interpolating
over the z-axis. We demonstrate performance improvements using controlled
ablation studies as well as put this approach to the test on the ImageCLEF
Tuberculosis Severity Assessment 2019 benchmark. We report 73% area under curve
(AUC) and binary classification accuracy (ACC) of 67.5% on the test set beating
all methods which leveraged only image information (without using clinical
meta-data) achieving 5-th position overall. All codes and models are made
available at https://github.com/hasibzunair/uniformizing-3D.
\\ ( https://arxiv.org/abs/2007.13224 ,  3091kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13249 (*cross-listing*)
Date: Mon, 27 Jul 2020 00:08:07 GMT   (1292kb,D)

Title: Dual Distribution Alignment Network for Generalizable Person
  Re-Identification
Authors: Peixian Chen, Pingyang Dai, Jianzhuang Liu, Feng Zheng, Qi Tian,
  Rongrong Ji
Categories: cs.IR cs.CV cs.LG
Comments: 8 pages, 3 figures
\\
  Domain generalization (DG) serves as a promising solution to handle person
Re-Identification (Re-ID), which trains the model using labels from the source
domain alone, and then directly adopts the trained model to the target domain
without model updating. However, existing DG approaches are usually disturbed
by serious domain variations due to significant dataset variations.
Subsequently, DG highly relies on designing domain-invariant features, which is
however not well exploited, since most existing approaches directly mix
multiple datasets to train DG based models without considering the local
dataset similarities, i.e., examples that are very similar but from different
domains. In this paper, we present a Dual Distribution Alignment Network
(DDAN), which handles this challenge by mapping images into a domain-invariant
feature space by selectively aligning distributions of multiple source domains.
Such an alignment is conducted by dual-level constraints, i.e., the domain-wise
adversarial feature learning and the identity-wise similarity enhancement. We
evaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID)
benchmark. Quantitative results demonstrate that the proposed DDAN can well
align the distributions of various source domains, and significantly
outperforms all existing domain generalization approaches.
\\ ( https://arxiv.org/abs/2007.13249 ,  1292kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13384 (*cross-listing*)
Date: Mon, 27 Jul 2020 09:01:22 GMT   (796kb,D)

Title: ALF: Autoencoder-based Low-rank Filter-sharing for Efficient
  Convolutional Neural Networks
Authors: Alexander Frickenstein, Manoj-Rohit Vemparala, Nael Fasfous, Laura
  Hauenschild, Naveen-Shankar Nagaraja, Christian Unger, Walter Stechele
Categories: cs.LG cs.CV stat.ML
Comments: Accepted by DAC'20
\\
  Closing the gap between the hardware requirements of state-of-the-art
convolutional neural networks and the limited resources constraining embedded
applications is the next big challenge in deep learning research. The
computational complexity and memory footprint of such neural networks are
typically daunting for deployment in resource constrained environments. Model
compression techniques, such as pruning, are emphasized among other
optimization methods for solving this problem. Most existing techniques require
domain expertise or result in irregular sparse representations, which increase
the burden of deploying deep learning applications on embedded hardware
accelerators. In this paper, we propose the autoencoder-based low-rank
filter-sharing technique technique (ALF). When applied to various networks, ALF
is compared to state-of-the-art pruning methods, demonstrating its efficient
compression capabilities on theoretical metrics as well as on an accurate,
deterministic hardware-model. In our experiments, ALF showed a reduction of
70\% in network parameters, 61\% in operations and 41\% in execution time, with
minimal loss in accuracy.
\\ ( https://arxiv.org/abs/2007.13384 ,  796kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13408 (*cross-listing*)
Date: Mon, 27 Jul 2020 10:05:04 GMT   (5660kb,D)

Title: XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on
  Anatomically Variable XCAT Phantoms
Authors: Sina Amirrajab, Samaneh Abbasi-Sureshjani, Yasmina Al Khalil, Cristian
  Lorenz, Juergen Weese, Josien Pluim, and Marcel Breeuwer
Categories: eess.IV cs.CV cs.LG
Comments: Accepted for MICCAI 2020
\\
  Generative adversarial networks (GANs) have provided promising data
enrichment solutions by synthesizing high-fidelity images. However, generating
large sets of labeled images with new anatomical variations remains unexplored.
We propose a novel method for synthesizing cardiac magnetic resonance (CMR)
images on a population of virtual subjects with a large anatomical variation,
introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human
phantom. We investigate two conditional image synthesis approaches grounded on
a semantically-consistent mask-guided image generation technique: 4-class and
8-class XCAT-GANs. The 4-class technique relies on only the annotations of the
heart; while the 8-class technique employs a predicted multi-tissue label map
of the heart-surrounding organs and provides better guidance for our
conditional image synthesis. For both techniques, we train our conditional
XCAT-GAN with real images paired with corresponding labels and subsequently at
the inference time, we substitute the labels with the XCAT derived ones.
Therefore, the trained network accurately transfers the tissue-specific
textures to the new label maps. By creating 33 virtual subjects of synthetic
CMR images at the end-diastolic and end-systolic phases, we evaluate the
usefulness of such data in the downstream cardiac cavity segmentation task
under different augmentation strategies. Results demonstrate that even with
only 20% of real images (40 volumes) seen during training, segmentation
performance is retained with the addition of synthetic CMR images. Moreover,
the improvement in utilizing synthetic images for augmenting the real data is
evident through the reduction of Hausdorff distance up to 28% and an increase
in the Dice score up to 5%, indicating a higher similarity to the ground truth
in all dimensions.
\\ ( https://arxiv.org/abs/2007.13408 ,  5660kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13417 (*cross-listing*)
Date: Mon, 27 Jul 2020 10:36:18 GMT   (14314kb,D)

Title: Image-driven discriminative and generative machine learning algorithms
  for establishing microstructure-processing relationships
Authors: Wufei Ma, Elizabeth Kautz, Arun Baskaran, Aritra Chowdhury, Vineet
  Joshi, B\"ulent Yener, Daniel Lewis
Categories: physics.app-ph cs.CV eess.IV
Comments: 14 pages, 15 figures
\\
  We investigate methods of microstructure representation for the purpose of
predicting processing condition from microstructure image data. A binary alloy
(uranium-molybdenum) that is currently under development as a nuclear fuel was
studied for the purpose of developing an improved machine learning approach to
image recognition, characterization, and building predictive capabilities
linking microstructure to processing conditions. Here, we test different
microstructure representations and evaluate model performance based on the F1
score. A F1 score of 95.1% was achieved for distinguishing between micrographs
corresponding to ten different thermo-mechanical material processing
conditions. We find that our newly developed microstructure representation
describes image data well, and the traditional approach of utilizing area
fractions of different phases is insufficient for distinguishing between
multiple classes using a relatively small, imbalanced original data set of 272
images. To explore the applicability of generative methods for supplementing
such limited data sets, generative adversarial networks were trained to
generate artificial microstructure images. Two different generative networks
were trained and tested to assess performance. Challenges and best practices
associated with applying machine learning to limited microstructure image data
sets is also discussed. Our work has implications for quantitative
microstructure analysis, and development of microstructure-processing
relationships in limited data sets typical of metallurgical process design
studies.
\\ ( https://arxiv.org/abs/2007.13417 ,  14314kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13484 (*cross-listing*)
Date: Thu, 25 Jun 2020 09:29:48 GMT   (1260kb,D)

Title: Attention-based Graph ResNet for Motor Intent Detection from Raw EEG
  signals
Authors: Shuyue Jia, Yimin Hou, Yan Shi, Yang Li
Categories: eess.SP cs.AI cs.CV cs.LG eess.IV
\\
  In previous studies, decoding electroencephalography (EEG) signals has not
considered the topological relationship of EEG electrodes. However, the latest
neuroscience has suggested brain network connectivity. Thus, the exhibited
interaction between EEG channels might not be appropriately measured via
Euclidean distance. To fill the gap, an attention-based graph residual network,
a novel structure of Graph Convolutional Neural Network (GCN), was presented to
detect human motor intents from raw EEG signals, where the topological
structure of EEG electrodes was built as a graph. Meanwhile, deep residual
learning with a full-attention architecture was introduced to address the
degradation problem concerning deeper networks in raw EEG motor imagery (MI)
data. Individual variability, the critical and longstanding challenge
underlying EEG signals, has been successfully handled with the state-of-the-art
performance, 98.08% accuracy at the subject level, 94.28% for 20 subjects.
Numerical results were promising that the implementation of the
graph-structured topology was superior to decode raw EEG data. The innovative
deep learning approach was expected to entail a universal method towards both
neuroscience research and real-world EEG-based practical applications, e.g.,
seizure prediction.
\\ ( https://arxiv.org/abs/2007.13484 ,  1260kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13516 (*cross-listing*)
Date: Mon, 13 Jul 2020 07:11:59 GMT   (223kb)

Title: Hardware Implementation of Hyperbolic Tangent Function using Catmull-Rom
  Spline Interpolation
Authors: Mahesh Chandra
Categories: cs.AR cs.CV
Comments: 4 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:2007.11976
\\
  Deep neural networks yield the state of the art results in many computer
vision and human machine interface tasks such as object recognition, speech
recognition etc. Since, these networks are computationally expensive,
customized accelerators are designed for achieving the required performance at
lower cost and power. One of the key building blocks of these neural networks
is non-linear activation function such as sigmoid, hyperbolic tangent (tanh),
and ReLU. A low complexity accurate hardware implementation of the activation
function is required to meet the performance and area targets of the neural
network accelerators. This paper presents an implementation of tanh function
using the Catmull-Rom spline interpolation. State of the art results are
achieved using this method with comparatively smaller logic area.
\\ ( https://arxiv.org/abs/2007.13516 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13648 (*cross-listing*)
Date: Fri, 24 Jul 2020 14:54:40 GMT   (738kb,D)

Title: Orpheus: A New Deep Learning Framework for Easy Deployment and
  Evaluation of Edge Inference
Authors: Perry Gibson, Jos\'e Cano
Categories: cs.DC cs.CV cs.LG cs.PF stat.ML
Comments: To be published as a poster in 2020 IEEE International Symposium on
  Performance Analysis of Systems and Software
\\
  Optimising deep learning inference across edge devices and optimisation
targets such as inference time, memory footprint and power consumption is a key
challenge due to the ubiquity of neural networks. Today, production deep
learning frameworks provide useful abstractions to aid machine learning
engineers and systems researchers. However, in exchange they can suffer from
compatibility challenges (especially on constrained platforms), inaccessible
code complexity, or design choices that otherwise limit research from a systems
perspective. This paper presents Orpheus, a new deep learning framework for
easy prototyping, deployment and evaluation of inference optimisations. Orpheus
features a small codebase, minimal dependencies, and a simple process for
integrating other third party systems. We present some preliminary evaluation
results.
\\ ( https://arxiv.org/abs/2007.13648 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13657 (*cross-listing*)
Date: Mon, 27 Jul 2020 16:13:13 GMT   (2584kb,D)

Title: Towards Learning Convolutions from Scratch
Authors: Behnam Neyshabur
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: 18 pages, 9 figures, 4 tables
\\
  Convolution is one of the most essential components of architectures used in
computer vision. As machine learning moves towards reducing the expert bias and
learning it from data, a natural next step seems to be learning
convolution-like structures from scratch. This, however, has proven elusive.
For example, current state-of-the-art architecture search algorithms use
convolution as one of the existing modules rather than learning it from data.
In an attempt to understand the inductive bias that gives rise to convolutions,
we investigate minimum description length as a guiding principle and show that
in some settings, it can indeed be indicative of the performance of
architectures. To find architectures with small description length, we propose
$\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on
fully-connected networks for image classification tasks, learns architectures
with local connections and achieves state-of-the-art accuracies for training
fully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%)
bridging the gap between fully-connected and convolutional nets.
\\ ( https://arxiv.org/abs/2007.13657 ,  2584kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13678 (*cross-listing*)
Date: Fri, 10 Jul 2020 20:55:11 GMT   (356kb)

Title: Cloud Detection through Wavelet Transforms in Machine Learning and Deep
  Learning
Authors: Philippe Reiter
Categories: eess.SP cs.AI cs.CV cs.LG eess.IV
Comments: 4 pages, 2 figures
\\
  Cloud detection is a specialized application of image recognition and object
detection using remotely sensed data. The task presents a number of challenges,
including analyzing images obtained in visible, infrared and multi-spectral
frequencies, usually without ground truth data for comparison. Moreover,
machine learning and deep learning (MLDL) algorithms applied to this task are
required to be computationally efficient, as they are typically deployed in
low-power devices and called to operate in real-time.
  This paper explains Wavelet Transform (WT) theory, comparing it to more
widely used image and signal processing transforms, and explores the use of WT
as a powerful signal compressor and feature extractor for MLDL classifiers.
\\ ( https://arxiv.org/abs/2007.13678 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13701 (*cross-listing*)
Date: Mon, 27 Jul 2020 17:27:59 GMT   (26265kb,D)

Title: MMDF: Mobile Microscopy Deep Framework
Authors: Anatasiia Kornilova, Mikhail Salnikov, Olga Novitskaya, Maria
  Begicheva, Egor Sevriugov, Kirill Shcherbakov
Categories: eess.IV cs.CV
\\
  In the last decade, a huge step was done in the field of mobile microscopes
development as well as in the field of mobile microscopy application to
real-life disease diagnostics and a lot of other important areas (air/water
quality pollution, education, agriculture). In current study we applied image
processing techniques from Deep Learning (in-focus/out-of-focus classification,
image deblurring and denoising, multi-focus image fusion) to the data obtained
from the mobile microscope. Overview of significant works for every task is
presented, the most suitable approaches were highlighted. Chosen approaches
were implemented as well as their performance were compared with classical
computer vision techniques.
\\ ( https://arxiv.org/abs/2007.13701 ,  26265kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13715 (*cross-listing*)
Date: Mon, 27 Jul 2020 17:46:59 GMT   (4022kb,D)

Title: Point Cloud Based Reinforcement Learning for Sim-to-Real and Partial
  Observability in Visual Navigation
Authors: Kenzo Lobos-Tsunekawa, Tatsuya Harada
Categories: cs.RO cs.CV cs.LG
Comments: Accepted to IROS'2020
\\
  Reinforcement Learning (RL), among other learning-based methods, represents
powerful tools to solve complex robotic tasks (e.g., actuation, manipulation,
navigation, etc.), with the need for real-world data to train these systems as
one of its most important limitations. The use of simulators is one way to
address this issue, yet knowledge acquired in simulations does not work
directly in the real-world, which is known as the sim-to-real transfer problem.
While previous works focus on the nature of the images used as observations
(e.g., textures and lighting), which has proven useful for a sim-to-sim
transfer, they neglect other concerns regarding said observations, such as
precise geometrical meanings, failing at robot-to-robot, and thus in
sim-to-real transfers. We propose a method that learns on an observation space
constructed by point clouds and environment randomization, generalizing among
robots and simulators to achieve sim-to-real, while also addressing partial
observability. We demonstrate the benefits of our methodology on the point goal
navigation task, in which our method proves to be highly unaffected to unseen
scenarios produced by robot-to-robot transfer, outperforms image-based
baselines in robot-randomized experiments, and presents high performances in
sim-to-sim conditions. Finally, we perform several experiments to validate the
sim-to-real transfer to a physical domestic robot platform, confirming the
out-of-the-box performance of our system.
\\ ( https://arxiv.org/abs/2007.13715 ,  4022kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12819 (*cross-listing*)
Date: Sat, 25 Jul 2020 01:10:33 GMT   (152kb,D)

Title: Support of Closed Walks and Second Eigenvalue Multiplicity of Regular
  Graphs
Authors: Theo McKenzie, Peter M. R. Rasmussen, Nikhil Srivastava
Categories: math.CO cs.DM math.MG math.PR math.SP
Comments: 20pp, 2 figures
\\
  We show that the multiplicity of the second adjacency matrix eigenvalue of
any connected $d-$regular graph is bounded by $O(n d^{7/5}/\log^{1/5-o(1)}n)$
for any $d$, and by $O(n\log^{1/2}d/\log^{1/4-o(1)}n)$ when $d\ge \log^{1/4}n$
and the graph is simple. In fact, the same bounds hold for the number of
eigenvalues in any interval of width $\lambda_2/\log_d^{1-o(1)}n$ containing
the second eigenvalue $\lambda_2$. The main ingredient in the proof is a
polynomial (in $k$) lower bound on the typical support of a random closed walk
of length $2k$ in any connected regular graph, which further relies on new
lower bounds for the entries of the Perron eigenvector of a connected irregular
graph.
\\ ( https://arxiv.org/abs/2007.12819 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13232 (*cross-listing*)
Date: Sun, 26 Jul 2020 22:28:42 GMT   (1196kb,D)

Title: The Pendulum Arrangement: Maximizing the Escape Time of Heterogeneous
  Random Walks
Authors: Asaf Cassel (1), Guy Tennenholtz (2), Shie Mannor (2) ((1) School of
  Computer Science, Tel Aviv University, (2) Faculty of Electrical Engineering,
  Technion Institute of Technology, Israel)
Categories: math.PR cs.DM math.CO math.OC physics.data-an
\\
  We identify a fundamental phenomenon of heterogeneous one dimensional random
walks: the escape (traversal) time is maximized when the heterogeneity in
transition probabilities forms a pyramid-like potential barrier. This barrier
corresponds to a distinct arrangement of transition probabilities, sometimes
referred to as the {\em pendulum arrangement}. We reduce this problem to a sum
over products, combinatorial optimization problem, proving that this unique
structure always maximizes the escape time. This general property may influence
studies in epidemiology, biology, and computer science to better understand
escape time behavior and construct intruder-resilient networks.
\\ ( https://arxiv.org/abs/2007.13232 ,  1196kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13266 (*cross-listing*)
Date: Mon, 27 Jul 2020 01:33:24 GMT   (861kb,D)

Title: Unfolding cubes: nets, packings, partitions, chords
Authors: Kristin DeSplinter, Satyan L. Devadoss, Jordan Readyhough, Bryce
  Wimberly
Categories: math.CO cs.DM
Comments: 17 pages, 18 figures
MSC-class: 52B05 (primary), 05C38, 05A18 (secondary)
\\
  We show that every ridge unfolding of an $n$-cube is without self-overlap,
yielding a valid net. The results are obtained by developing machinery that
translates cube unfolding into combinatorial frameworks. Moreover, the geometry
of the bounding boxes of these cube nets are classified using integer
partitions, as well as the combinatorics of path unfoldings seen through the
lens of chord diagrams.
\\ ( https://arxiv.org/abs/2007.13266 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13601 (*cross-listing*)
Date: Mon, 27 Jul 2020 14:34:12 GMT   (8825kb,D)

Title: Continuous Fuzzy Transform as Integral Operator
Authors: Giuseppe Patan\`e
Categories: math.OC cs.DM cs.GR
\\
  The Fuzzy transform is ubiquitous in different research fields and
applications, such as image and data compression, data mining, knowledge
discovery, and the analysis of linguistic expressions. As a generalisation of
the Fuzzy transform, we introduce the continuous Fuzzy transform and its
inverse, as an integral operator induced by a kernel function. Through the
relation between membership functions and integral kernels, we show that the
main properties (e.g., continuity, symmetry) of the membership functions are
inherited by the continuous Fuzzy transform. Then, the relation between the
continuous Fuzzy transform and integral operators is used to introduce a
data-driven Fuzzy transform, which encodes intrinsic information (e.g.,
structure, geometry, sampling density) about the input data. In this way, we
avoid coarse fuzzy partitions, which group data into large clusters that do not
adapt to their local behaviour, or a too dense fuzzy partition, which generally
has cells that are not covered by the data, thus being redundant and resulting
in a higher computational cost. To this end, the data-driven membership
functions are defined by properly filtering the spectrum of the
Laplace-Beltrami operator associated with the input data. Finally, we introduce
the space of continuous Fuzzy transforms, which is useful for the comparison of
different continuous Fuzzy transforms and for their efficient computation.
\\ ( https://arxiv.org/abs/2007.13601 ,  8825kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13630 (*cross-listing*)
Date: Mon, 27 Jul 2020 15:15:07 GMT   (98kb,D)

Title: High-girth near-Ramanujan graphs with lossy vertex expansion
Authors: Theo McKenzie, Sidhanth Mohanty
Categories: math.CO cs.DM math.SP
Comments: 15 pages, 1 figure
MSC-class: 05C48
ACM-class: G.2.1; G.2.2
\\
  Kahale proved that linear sized sets in $d$-regular Ramanujan graphs have
vertex expansion $\sim\frac{d}{2}$ and complemented this with construction of
near-Ramanujan graphs with vertex expansion no better than $\frac{d}{2}$.
However, the construction of Kahale encounters highly local obstructions to
better vertex expansion. In particular, the poorly expanding sets are
associated with short cycles in the graph. Thus, it is natural to ask whether
high-girth Ramanujan graphs have improved vertex expansion. Our results are
two-fold:
  1. For every $d = p+1$ for prime $p$ and infinitely many $n$, we exhibit an
$n$-vertex $d$-regular graph with girth $\Omega(\log_{d-1} n)$ and vertex
expansion of sublinear sized sets bounded by $\frac{d+1}{2}$ whose nontrivial
eigenvalues are bounded in magnitude by $2\sqrt{d-1}+O\left(\frac{1}{\log
n}\right)$.
  2. In any Ramanujan graph with girth $C\log n$, all sets of size bounded by
$n^{0.99C/4}$ have vertex expansion $(1-o_d(1))d$.
  The tools in analyzing our construction include the nonbacktracking operator
of an infinite graph, the Ihara--Bass formula, a trace moment method inspired
by Bordenave's proof of Friedman's theorem, and a method of Kahale to study
dispersion of eigenvalues of perturbed graphs.
\\ ( https://arxiv.org/abs/2007.13630 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13661 (*cross-listing*)
Date: Mon, 27 Jul 2020 16:15:39 GMT   (595kb)

Title: CARAM: A Content-Aware Hybrid PCM/DRAM Main Memory System Framework
Authors: Yinjin Fu
Categories: cs.AR cs.ET
\\
  The emergence of Phase-Change Memory (PCM) provides opportunities for
directly connecting persistent memory to main memory bus. While PCM achieves
high read throughput and low standby power, the critical concerns are its poor
write performance and limited durability, especially when compared to DRAM. A
naturally inspired design is the hybrid memory architecture that fuses DRAM and
PCM, so as to exploit the positive aspects of both types of memory.
Unfortunately, existing solutions are seriously challenged by the limited main
memory size, which is the primary bottleneck of in-memory computing. In this
paper, we introduce a novel Content Aware hybrid PCM/DRAM main memory system
framework - CARAM, which exploits deduplication to improve line sharing with
high memory efficiency. CARAM effectively reduces write traffic to hybrid
memory by removing unnecessary duplicate line writes. It also substantially
extends available free memory space by coalescing redundant lines in hybrid
memory, thereby further improving the wear-leveling efficiency of PCM. To
obtain high data access performance, we also design a set of acceleration
techniques to minimize the overhead caused by extra computation costs. Our
experiment results show that CARAM effectively reduces 15%~42% of memory usage
and improves I/O bandwidth by 13%~116%, while saving 31%~38% energy
consumption, compared to the state-of-the-art of hybrid systems.
\\ ( https://arxiv.org/abs/2007.13661 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13679 (*cross-listing*)
Date: Wed, 15 Jul 2020 14:18:59 GMT   (240kb)

Title: Llums que no s\'on nom\'es llums
Authors: Jorge Baranda, Pol Henarejos, Ciprian George Gavrincea, Miguel \'Angel
  Lagunas
Categories: eess.SP cs.ET
Comments: 4 pages, in catalan
Journal-ref: Telecos.cat, no. 59, vol. 6, 2013
\\
  Visible Light Communications (VLC) is a new paradigm in wireless
communications. The characteristics of this technology, which uses
light-emitting diode-based lighting devices as transmitting elements, make it
possible to be considered a complement to current wireless radio communication
systems.
  -----
  Les comunicacions per llum visible o 'Visible Light Communications' (VLC)
s\'on un nou paradigma en comunicacions sense fils. Les caracter\'istiques que
presenta aquesta tecnologia, que utilitza els dispositius
d'il{\lgem{}}luminaci\'o basats en d\'iodes emissors de llum com elements
transmissors, fa que es pugui considerar un complement dels actuals sistemes de
comunicaci\'o inal`ambrics.
\\ ( https://arxiv.org/abs/2007.13679 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2007.13371 (*cross-listing*)
Date: Mon, 27 Jul 2020 08:42:07 GMT   (7478kb,D)

Title: Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving
  Simulators in HMI Design
Authors: Lia Morra, Fabrizio Lamberti, F. Gabriele Prattic\'o, Salvatore La
  Rosa, Paolo Montuschi
Categories: cs.HC cs.AI cs.GR
Journal-ref: IEEE Transactions on Vehicular Technology, 68(10), pp.9438-9450,
  2019
DOI: 10.1109/TVT.2019.2933601
\\
  The investigation of factors contributing at making humans trust Autonomous
Vehicles (AVs) will play a fundamental role in the adoption of such technology.
The user's ability to form a mental model of the AV, which is crucial to
establish trust, depends on effective user-vehicle communication; thus, the
importance of Human-Machine Interaction (HMI) is poised to increase. In this
work, we propose a methodology to validate the user experience in AVs based on
continuous, objective information gathered from physiological signals, while
the user is immersed in a Virtual Reality-based driving simulation. We applied
this methodology to the design of a head-up display interface delivering visual
cues about the vehicle' sensory and planning systems. Through this approach, we
obtained qualitative and quantitative evidence that a complete picture of the
vehicle's surrounding, despite the higher cognitive load, is conducive to a
less stressful experience. Moreover, after having been exposed to a more
informative interface, users involved in the study were also more willing to
test a real AV. The proposed methodology could be extended by adjusting the
simulation environment, the HMI and/or the vehicle's Artificial Intelligence
modules to dig into other aspects of the user experience.
\\ ( https://arxiv.org/abs/2007.13371 ,  7478kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1802.02297
replaced with revised version Mon, 27 Jul 2020 00:42:27 GMT   (784kb,D)

Title: 3D Point Cloud Descriptors in Hand-crafted and Deep Learning Age:
  State-of-the-Art
Authors: Xian-Feng Han, Shi-Jie Sun, Xiang-Yu Song, Guo-Qiang Xiao
Categories: cs.CV
\\ ( https://arxiv.org/abs/1802.02297 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:1803.08467
replaced with revised version Sat, 25 Jul 2020 07:04:23 GMT   (55696kb,D)

Title: BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled
  Representation Learning and Image Synthesis
Authors: Zili Yi, Zhiqin Chen, Hao Cai, Wendong Mao, Minglun Gong, Hao Zhang
Categories: cs.CV
Comments: 26 pages, 20 figures, TIP paper
\\ ( https://arxiv.org/abs/1803.08467 ,  55696kb)
------------------------------------------------------------------------------
\\
arXiv:1907.09665
replaced with revised version Sun, 26 Jul 2020 15:02:41 GMT   (0kb,I)

Title: Compact Global Descriptor for Neural Networks
Authors: Xiangyu He, Ke Cheng, Qiang Chen, Qinghao Hu, Peisong Wang, Jian Cheng
Categories: cs.CV
Comments: This paper will be included in our future works as a subsection
\\ ( https://arxiv.org/abs/1907.09665 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:1909.07267
replaced with revised version Sun, 26 Jul 2020 05:38:47 GMT   (2453kb,D)

Title: A Fast and Robust Place Recognition Approach for Stereo Visual Odometry
  Using LiDAR Descriptors
Authors: Jiawei Mo and Junaed Sattar
Categories: cs.CV
Comments: Accepted by IROS2020
\\ ( https://arxiv.org/abs/1909.07267 ,  2453kb)
------------------------------------------------------------------------------
\\
arXiv:1909.07459
replaced with revised version Sun, 26 Jul 2020 11:15:04 GMT   (1425kb,D)

Title: Bridging Visual Perception with Contextual Semantics for Understanding
  Robot Manipulation Tasks
Authors: Chen Jiang, Martin Jagersand
Categories: cs.CV
\\ ( https://arxiv.org/abs/1909.07459 ,  1425kb)
------------------------------------------------------------------------------
\\
arXiv:1909.07558
replaced with revised version Sat, 25 Jul 2020 01:58:48 GMT   (1427kb)

Title: HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial
  Examples
Authors: Wanting Yu, Hongyi Yu, Lingyun Jiang, Mengli Zhang, Kai Qiao, Linyuan
  Wang, Bin Yan
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/1909.07558 ,  1427kb)
------------------------------------------------------------------------------
\\
arXiv:1909.09934
replaced with revised version Sun, 26 Jul 2020 13:40:20 GMT   (491kb,D)

Title: Structured Binary Neural Networks for Image Recognition
Authors: Bohan Zhuang, Chunhua Shen, Mingkui Tan, Peng Chen, Lingqiao Liu, Ian
  Reid
Categories: cs.CV
Comments: 16 pages. Extended version of the conference version arXiv:1811.10413
\\ ( https://arxiv.org/abs/1909.09934 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:1909.11065
replaced with revised version Sat, 25 Jul 2020 08:00:33 GMT   (1342kb,D)

Title: Object-Contextual Representations for Semantic Segmentation
Authors: Yuhui Yuan, Xilin Chen, Jingdong Wang
Categories: cs.CV
Comments: ECCV 2020 Spotlight. Project Page:
  https://github.com/openseg-group/openseg.pytorch
  https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR
Journal-ref: ECCV 2020
\\ ( https://arxiv.org/abs/1909.11065 ,  1342kb)
------------------------------------------------------------------------------
\\
arXiv:1910.07787
replaced with revised version Mon, 27 Jul 2020 15:48:33 GMT   (1268kb,D)

Title: NAMF: A Non-local Adaptive Mean Filter for Salt-and-Pepper Noise Removal
Authors: Houwang Zhang, Yuan Zhu and Hanying Zheng
Categories: cs.CV
\\ ( https://arxiv.org/abs/1910.07787 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:1912.04023
replaced with revised version Mon, 27 Jul 2020 15:53:23 GMT   (41307kb,D)

Title: ShadingNet: Image Intrinsics by Fine-Grained Shading Decomposition
Authors: Anil S. Baslamisli, Partha Das, Hoang-An Le, Sezer Karaoglu, Theo
  Gevers
Categories: cs.CV
Comments: Submitted to International Journal of Computer Vision (IJCV)
\\ ( https://arxiv.org/abs/1912.04023 ,  41307kb)
------------------------------------------------------------------------------
\\
arXiv:1912.08741
replaced with revised version Mon, 27 Jul 2020 11:32:45 GMT   (5288kb,D)

Title: Towards Robust Learning with Different Label Noise Distributions
Authors: Diego Ortego, Eric Arazo, Paul Albert, Noel E. O'Connor and Kevin
  McGuinness
Categories: cs.CV
\\ ( https://arxiv.org/abs/1912.08741 ,  5288kb)
------------------------------------------------------------------------------
\\
arXiv:2002.07705
replaced with revised version Mon, 27 Jul 2020 17:48:08 GMT   (8730kb,D)

Title: Towards Bounding-Box Free Panoptic Segmentation
Authors: Ujwal Bonde and Pablo F. Alcantarilla and Stefan Leutenegger
Categories: cs.CV cs.RO
Comments: 15 pages, 6 figures
\\ ( https://arxiv.org/abs/2002.07705 ,  8730kb)
------------------------------------------------------------------------------
\\
arXiv:2003.02484
replaced with revised version Mon, 27 Jul 2020 12:26:13 GMT   (106kb,D)

Title: Adversarial Vertex Mixup: Toward Better Adversarially Robust
  Generalization
Authors: Saehyung Lee, Hyungyu Lee, Sungroh Yoon
Categories: cs.CV
Comments: To appear in CVPR 2020 (Oral)
\\ ( https://arxiv.org/abs/2003.02484 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2003.05664
replaced with revised version Sun, 26 Jul 2020 02:18:32 GMT   (8309kb,D)

Title: Conditional Convolutions for Instance Segmentation
Authors: Zhi Tian and Chunhua Shen and Hao Chen
Categories: cs.CV
Comments: Accepted to Proc. European Conf. Computer Vision (ECCV) 2020 as oral
  presentation
\\ ( https://arxiv.org/abs/2003.05664 ,  8309kb)
------------------------------------------------------------------------------
\\
arXiv:2003.07289
replaced with revised version Sat, 25 Jul 2020 12:39:06 GMT   (0kb,I)

Title: MVLoc: Multimodal Variational Geometry-Aware Learning for Visual
  Localization
Authors: Rui Zhou, Changhao Chen, Bing Wang, Andrew Markham, Niki Trigoni
Categories: cs.CV eess.IV
Comments: There exist several serious typos in this paper. And some results are
  problematic due to the wrong experiment
\\ ( https://arxiv.org/abs/2003.07289 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2003.12181
replaced with revised version Sun, 26 Jul 2020 02:08:57 GMT   (5062kb,D)

Title: ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds
Authors: Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis,
  Siddhartha Chaudhuri, Radom\'ir M\v{e}ch
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2003.12181 ,  5062kb)
------------------------------------------------------------------------------
\\
arXiv:2004.01347
replaced with revised version Mon, 27 Jul 2020 08:11:32 GMT   (1754kb,D)

Title: Learning Pose-invariant 3D Object Reconstruction from Single-view Images
Authors: Bo Peng, Wei Wang, Jing Dong and Tieniu Tan
Categories: cs.CV
Comments: under review, code available at https://github.com/bomb2peng/learn3D
\\ ( https://arxiv.org/abs/2004.01347 ,  1754kb)
------------------------------------------------------------------------------
\\
arXiv:2004.02803
replaced with revised version Sun, 26 Jul 2020 11:58:50 GMT   (1379kb,D)

Title: Deformable 3D Convolution for Video Super-Resolution
Authors: Xinyi Ying, Longguang Wang, Yingqian Wang, Weidong Sheng, Wei An,
  Yulan Guo
Categories: cs.CV
\\ ( https://arxiv.org/abs/2004.02803 ,  1379kb)
------------------------------------------------------------------------------
\\
arXiv:2004.04398
replaced with revised version Mon, 27 Jul 2020 12:55:37 GMT   (912kb,D)

Title: Online Meta-Learning for Multi-Source and Semi-Supervised Domain
  Adaptation
Authors: Da Li, Timothy Hospedales
Categories: cs.CV cs.LG
Comments: ECCV 2020 CR version
\\ ( https://arxiv.org/abs/2004.04398 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2004.06002
replaced with revised version Sun, 26 Jul 2020 07:28:39 GMT   (1066kb,D)

Title: Dynamic R-CNN: Towards High Quality Object Detection via Dynamic
  Training
Authors: Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, Xilin Chen
Categories: cs.CV
Comments: Accepted by ECCV2020
\\ ( https://arxiv.org/abs/2004.06002 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2004.06130
replaced with revised version Sun, 26 Jul 2020 14:33:04 GMT   (2326kb,D)

Title: SpeedNet: Learning the Speediness in Videos
Authors: Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T.
  Freeman, Michael Rubinstein, Michal Irani and Tali Dekel
Categories: cs.CV
Comments: Accepted to CVPR 2020 (oral). Project webpage:
  http://speednet-cvpr20.github.io
\\ ( https://arxiv.org/abs/2004.06130 ,  2326kb)
------------------------------------------------------------------------------
\\
arXiv:2004.06165
replaced with revised version Sun, 26 Jul 2020 00:46:46 GMT   (11644kb,D)

Title: Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks
Authors: Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei
  Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao
Categories: cs.CV cs.CL cs.IR cs.LG
Comments: ECCV 2020, Code and pre-trained models are released:
  https://github.com/microsoft/Oscar
\\ ( https://arxiv.org/abs/2004.06165 ,  11644kb)
------------------------------------------------------------------------------
\\
arXiv:2005.13402
replaced with revised version Sat, 25 Jul 2020 17:15:52 GMT   (297kb,D)

Title: AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing
  Label Features from Multi-Modal Embeddings
Authors: Pratik Mazumder, Pravendra Singh, Kranti Kumar Parida, Vinay P.
  Namboodiri
Categories: cs.CV cs.SD eess.AS
\\ ( https://arxiv.org/abs/2005.13402 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2006.02659
replaced with revised version Mon, 27 Jul 2020 07:28:11 GMT   (4058kb,D)

Title: MFPP: Morphological Fragmental Perturbation Pyramid for Black-Box Model
  Explanations
Authors: Qing Yang, Xia Zhu, Jong-Kae Fwu, Yun Ye, Ganmei You and Yuan Zhu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.02659 ,  4058kb)
------------------------------------------------------------------------------
\\
arXiv:2006.09920
replaced with revised version Sat, 25 Jul 2020 04:11:42 GMT   (10948kb,D)

Title: Contrastive Learning for Weakly Supervised Phrase Grounding
Authors: Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, and
  Derek Hoiem
Categories: cs.CV cs.CL cs.LG stat.ML
Comments: Project page: http://tanmaygupta.info/info-ground
\\ ( https://arxiv.org/abs/2006.09920 ,  10948kb)
------------------------------------------------------------------------------
\\
arXiv:2006.16956
replaced with revised version Fri, 24 Jul 2020 19:16:37 GMT   (8269kb,D)

Title: ITSELF: Iterative Saliency Estimation fLexible Framework
Authors: Leonardo de Melo Joao, Felipe de Castro Belem, Alexandre Xavier Falcao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.16956 ,  8269kb)
------------------------------------------------------------------------------
\\
arXiv:2007.04269
replaced with revised version Sat, 25 Jul 2020 08:14:04 GMT   (2915kb,D)

Title: SegFix: Model-Agnostic Boundary Refinement for Segmentation
Authors: Yuhui Yuan, Jingyi Xie, Xilin Chen, Jingdong Wang
Categories: cs.CV
Comments: ECCV 2020. Project Page:
  https://github.com/openseg-group/openseg.pytorch
\\ ( https://arxiv.org/abs/2007.04269 ,  2915kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05223
replaced with revised version Mon, 27 Jul 2020 01:21:29 GMT   (3218kb,D)

Title: Distillation Guided Residual Learning for Binary Convolutional Neural
  Networks
Authors: Jianming Ye, Shiliang Zhang, Jingdong Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2007.05223 ,  3218kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05854
replaced with revised version Mon, 27 Jul 2020 11:15:19 GMT   (0kb,I)

Title: Efficient resource management in UAVs for Visual Assistance
Authors: Bapireddy Karri and Sudip Misra
Categories: cs.CV cs.LG eess.IV
Comments: Problem with the results in submitted version
\\ ( https://arxiv.org/abs/2007.05854 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06389
replaced with revised version Sun, 26 Jul 2020 19:24:51 GMT   (4892kb,D)

Title: Term Revealing: Furthering Quantization at Run Time on Quantized DNNs
Authors: H. T. Kung, Bradley McDanel, Sai Qian Zhang
Categories: cs.CV cs.LG
Comments: 13 pages, 19 figures, 4 tables, To appear in Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis (SC), 2020 Update: Revised writing/figures and added more
  references for Section IV Update: Revised Section IV writing/figures and
  added additional references on signed digit representations
\\ ( https://arxiv.org/abs/2007.06389 ,  4892kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06676
replaced with revised version Sun, 26 Jul 2020 14:04:24 GMT   (5978kb,D)

Title: UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a
  Generic Framework for Handling Common Camera Distortion Models
Authors: Varun Ravi Kumar, Senthil Yogamani, Markus Bach, Christian Witt,
  Stefan Milz and Patrick Mader
Categories: cs.CV cs.LG cs.RO
Comments: Camera ready version for publication at IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) 2020
\\ ( https://arxiv.org/abs/2007.06676 ,  5978kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06837
replaced with revised version Mon, 27 Jul 2020 05:16:57 GMT   (9503kb,D)

Title: Top-Related Meta-Learning Method for Few-Shot Detection
Authors: Qian Li, Nan Guo, Duo Wang, Xiaochun Ye, Dongrui Fan and Zhimin Tang
Categories: cs.CV
Comments: meta-learing,few-shot detection, category-based grouping mechanism
\\ ( https://arxiv.org/abs/2007.06837 ,  9503kb)
------------------------------------------------------------------------------
\\
arXiv:2007.07431
replaced with revised version Sat, 25 Jul 2020 23:45:21 GMT   (7758kb,D)

Title: COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content
  Conditioned Style Encoder
Authors: Kuniaki Saito, Kate Saenko, Ming-Yu Liu
Categories: cs.CV
Comments: The paper will be presented at the EUROPEAN Conference on Computer
  Vision (ECCV) 2020
\\ ( https://arxiv.org/abs/2007.07431 ,  7758kb)
------------------------------------------------------------------------------
\\
arXiv:2007.08074
replaced with revised version Mon, 27 Jul 2020 09:34:12 GMT   (4593kb,D)

Title: Suppress and Balance: A Simple Gated Network for Salient Object
  Detection
Authors: Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, Lei Zhang
Categories: cs.CV
Comments: Accepted in ECCV2020(oral). Code:
  https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency
\\ ( https://arxiv.org/abs/2007.08074 ,  4593kb)
------------------------------------------------------------------------------
\\
arXiv:2007.08434
replaced with revised version Mon, 27 Jul 2020 10:57:04 GMT   (668kb,D)

Title: Appearance-Preserving 3D Convolution for Video-based Person
  Re-identification
Authors: Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, Xilin Chen
Categories: cs.CV
Comments: Accepted by ECCV2020 (Oral)
\\ ( https://arxiv.org/abs/2007.08434 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2007.08728
replaced with revised version Mon, 27 Jul 2020 05:42:32 GMT   (943kb,D)

Title: Detecting Human-Object Interactions with Action Co-occurrence Priors
Authors: Dong-Jin Kim, Xiao Sun, Jinsoo Choi, Stephen Lin, In So Kweon
Categories: cs.CV cs.LG
Comments: ECCV 2020. Source code :
  https://github.com/Dong-JinKim/ActionCooccurrencePriors/
\\ ( https://arxiv.org/abs/2007.08728 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09033
replaced with revised version Fri, 24 Jul 2020 22:13:13 GMT   (5494kb,D)

Title: Region-based Non-local Operation for Video Classification
Authors: Guoxi Huang and Adrian G. Bors
Categories: cs.CV
\\ ( https://arxiv.org/abs/2007.09033 ,  5494kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09107
replaced with revised version Sun, 26 Jul 2020 08:27:20 GMT   (1840kb,D)

Title: Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery
Authors: Emanuele Colleoni, Philip Edwards, Danail Stoyanov
Categories: cs.CV cs.LG cs.RO
\\ ( https://arxiv.org/abs/2007.09107 ,  1840kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09162
replaced with revised version Fri, 24 Jul 2020 19:34:54 GMT   (8281kb,D)

Title: Improving Object Detection with Selective Self-supervised Self-training
Authors: Yandong Li, Di Huang, Danfeng Qin, Liqiang Wang, Boqing Gong
Categories: cs.CV
Comments: Accepted to ECCV 2020
\\ ( https://arxiv.org/abs/2007.09162 ,  8281kb)
------------------------------------------------------------------------------
\\
arXiv:2007.09654
replaced with revised version Sat, 25 Jul 2020 09:41:18 GMT   (4703kb,D)

Title: Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed
  Datasets
Authors: Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, Dahua Lin
Categories: cs.CV cs.LG
Comments: To appear in ECCV 2020 as a spotlight presentation. Code and models
  are available at: https://github.com/wutong16/DistributionBalancedLoss
\\ ( https://arxiv.org/abs/2007.09654 ,  4703kb)
------------------------------------------------------------------------------
\\
arXiv:2007.10298
replaced with revised version Sat, 25 Jul 2020 14:50:25 GMT   (4564kb,D)

Title: Landmark Guidance Independent Spatio-channel Attention and Complementary
  Context Information based Facial Expression Recognition
Authors: Darshan Gera and S Balasubramanian
Categories: cs.CV
Comments: A couple of reference citations corrected, few details added and code
  link provided
\\ ( https://arxiv.org/abs/2007.10298 ,  4564kb)
------------------------------------------------------------------------------
\\
arXiv:2007.10323
replaced with revised version Sun, 26 Jul 2020 21:13:04 GMT   (2724kb,D)

Title: Pillar-based Object Detection for Autonomous Driving
Authors: Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline
  Pantofaru, Thomas Funkhouser, Justin Solomon
Categories: cs.CV cs.LG cs.RO
Comments: Accepted to ECCV2020
\\ ( https://arxiv.org/abs/2007.10323 ,  2724kb)
------------------------------------------------------------------------------
\\
arXiv:2007.10361
replaced with revised version Mon, 27 Jul 2020 07:34:46 GMT   (2095kb,D)

Title: Privacy Preserving Visual SLAM
Authors: Mikiya Shibuya, Shinya Sumikura, and Ken Sakurada
Categories: cs.CV
Comments: ECCV2020, Project: https://xdspacelab.github.io/lcvslam/ , Video:
  https://youtu.be/gEtUqnHx83w
\\ ( https://arxiv.org/abs/2007.10361 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2007.11978
replaced with revised version Sun, 26 Jul 2020 01:40:12 GMT   (2961kb,D)

Title: The Devil is in Classification: A Simple Framework for Long-tail
  Instance Segmentation
Authors: Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng Tang,
  Steven Hoi, Jiashi Feng
Categories: cs.CV
Comments: LVIS 2019 challenge winner, performance significantly improved after
  challenge submission, accepted at ECCV 2019
\\ ( https://arxiv.org/abs/2007.11978 ,  2961kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12072
replaced with revised version Sat, 25 Jul 2020 11:20:38 GMT   (8614kb,D)

Title: TSIT: A Simple and Versatile Framework for Image-to-Image Translation
Authors: Liming Jiang, Changxu Zhang, Mingyang Huang, Chunxiao Liu, Jianping
  Shi, Chen Change Loy
Categories: cs.CV cs.LG eess.IV
Comments: ECCV 2020 (Spotlight). Table 2 is updated. GitHub:
  https://github.com/EndlessSora/TSIT
\\ ( https://arxiv.org/abs/2007.12072 ,  8614kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12099
replaced with revised version Mon, 27 Jul 2020 06:10:35 GMT   (245kb,D)

Title: PP-YOLO: An Effective and Efficient Implementation of Object Detector
Authors: Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang,
  Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, Shilei Wen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2007.12099 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12212
replaced with revised version Mon, 27 Jul 2020 11:57:35 GMT   (1538kb,D)

Title: ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions
Authors: Anurag Roy, Vinay Kumar Verma, Kripabandhu Ghosh, Saptarshi Ghosh
Categories: cs.CV cs.CL cs.IR cs.LG
Comments: Accepted in CIKM-2020
\\ ( https://arxiv.org/abs/2007.12212 ,  1538kb)
------------------------------------------------------------------------------
\\
arXiv:1909.03179
replaced with revised version Sat, 25 Jul 2020 03:27:32 GMT   (1379kb)

Title: Perfect Matchings, Rank of Connection Tensors and Graph Homomorphisms
Authors: Jin-Yi Cai and Artem Govorov
Categories: cs.DM math.CO
\\ ( https://arxiv.org/abs/1909.03179 ,  1379kb)
------------------------------------------------------------------------------
\\
arXiv:1909.06658
replaced with revised version Sat, 25 Jul 2020 14:52:13 GMT   (5852kb,D)

Title: Committee Machines -- A Universal Method to Deal with Non-Idealities in
  Memristor-Based Neural Networks
Authors: D. Joksas, P. Freitas, Z. Chai, W. H. Ng, M. Buckwell, C. Li, W. D.
  Zhang, Q. Xia, A. J. Kenyon, A. Mehonic
Categories: cs.ET
Comments: 23 pages, 18 figures, 4 tables
\\ ( https://arxiv.org/abs/1909.06658 ,  5852kb)
------------------------------------------------------------------------------
\\
arXiv:1905.10071
replaced with revised version Wed, 15 Jul 2020 04:39:13 GMT   (4655kb,D)

Title: Flow-based Intrinsic Curiosity Module
Authors: Hsuan-Kung Yang, Po-Han Chiang, Min-Fong Hong, and Chun-Yi Lee
Categories: cs.LG cs.CV stat.ML
Comments: The SOLE copyright holder is IJCAI (International Joint Conferences
  on Artificial Intelligence), all rights reserved. The link is provided as
  follows: https://www.ijcai.org/Proceedings/2020/286
Journal-ref: Proceedings of the Twenty-Ninth International Joint Conference on
  Artificial Intelligence Main track. Pages 2065-2072
DOI: 10.24963/ijcai.2020/286
\\ ( https://arxiv.org/abs/1905.10071 ,  4655kb)
------------------------------------------------------------------------------
\\
arXiv:1905.10710
replaced with revised version Sun, 26 Jul 2020 13:49:41 GMT   (7206kb,D)

Title: Fixing Bias in Reconstruction-based Anomaly Detection with Lipschitz
  Discriminators
Authors: Alexander Tong, Guy Wolf, Smita Krishnaswamy
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: 6 pages, 4 figures, 2 tables, presented at IEEE MLSP
\\ ( https://arxiv.org/abs/1905.10710 ,  7206kb)
------------------------------------------------------------------------------
\\
arXiv:1906.06919
replaced with revised version Sun, 26 Jul 2020 14:00:51 GMT   (2297kb,D)

Title: Improving Black-box Adversarial Attacks with a Transfer-based Prior
Authors: Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu
Categories: cs.LG cs.CR cs.CV stat.ML
Comments: NeurIPS 2019; Code available at
  https://github.com/thu-ml/Prior-Guided-RGF
\\ ( https://arxiv.org/abs/1906.06919 ,  2297kb)
------------------------------------------------------------------------------
\\
arXiv:1910.04256
replaced with revised version Sat, 25 Jul 2020 09:14:35 GMT   (19628kb,D)

Title: Explaining an image classifier's decisions using generative models
Authors: Chirag Agarwal, Anh Nguyen
Categories: cs.LG cs.CV stat.ML
Comments: Preprint. Submission under review
\\ ( https://arxiv.org/abs/1910.04256 ,  19628kb)
------------------------------------------------------------------------------
\\
arXiv:1911.11134
replaced with revised version Sat, 25 Jul 2020 20:13:36 GMT   (627kb,D)

Title: Rigging the Lottery: Making All Tickets Winners
Authors: Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen
Categories: cs.LG cs.CV stat.ML
Comments: Published in Proceedings of the 37th International Conference on
  Machine Learning. Code can be found in github.com/google-research/rigl
Journal-ref: Proceedings of the 37th International Conference on Machine
  Learning (2020) 471-481
\\ ( https://arxiv.org/abs/1911.11134 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2002.04461 (*cross-listing*)
replaced with revised version Sun, 26 Jul 2020 13:42:19 GMT   (6345kb,D)

Title: TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular
  Dynamics
Authors: Alexander Tong, Jessie Huang, Guy Wolf, David van Dijk, Smita
  Krishnaswamy
Categories: stat.ML cs.CV cs.LG q-bio.QM
Comments: Presented at ICML 2020
\\ ( https://arxiv.org/abs/2002.04461 ,  6345kb)
------------------------------------------------------------------------------
\\
arXiv:2004.04668 (*cross-listing*)
replaced with revised version Mon, 27 Jul 2020 12:07:31 GMT   (1961kb,D)

Title: Test-Time Adaptable Neural Networks for Robust Medical Image
  Segmentation
Authors: Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu
Categories: eess.IV cs.CV cs.LG stat.ML
Comments: Pre-print (currently under review at a Journal)
\\ ( https://arxiv.org/abs/2004.04668 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04270
replaced with revised version Sat, 25 Jul 2020 05:53:19 GMT   (3348kb)

Title: EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks
Authors: Hojjat Salehinejad and Shahrokh Valaee
Categories: cs.LG cs.CV cs.NE
\\ ( https://arxiv.org/abs/2006.04270 ,  3348kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05245 (*cross-listing*)
replaced with revised version Sun, 26 Jul 2020 02:20:19 GMT   (495kb)

Title: A Review of Automated Diagnosis of COVID-19 Based on Scanning Images
Authors: Delong Chen, Shunhui Ji, Fan Liu1, Zewen Li, Xinyu Zhou
Categories: eess.IV cs.CV cs.LG
Comments: under review of PRAI2020
\\ ( https://arxiv.org/abs/2006.05245 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2006.10147 (*cross-listing*)
replaced with revised version Sat, 25 Jul 2020 21:52:21 GMT   (1130kb,D)

Title: Are you wearing a mask? Improving mask detection from speech using
  augmentation by cycle-consistent GANs
Authors: Nicolae-C\u{a}t\u{a}lin Ristea, Radu Tudor Ionescu
Categories: eess.AS cs.CV cs.SD
Comments: Accepted at INTERSPEECH 2020
\\ ( https://arxiv.org/abs/2006.10147 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2006.14744
replaced with revised version Fri, 24 Jul 2020 20:04:49 GMT   (16664kb,D)

Title: Graph Optimal Transport for Cross-Domain Alignment
Authors: Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, Jingjing Liu
Categories: cs.CL cs.CV cs.LG
Journal-ref: ICML 2020
\\ ( https://arxiv.org/abs/2006.14744 ,  16664kb)
------------------------------------------------------------------------------
\\
arXiv:2007.10760
replaced with revised version Mon, 27 Jul 2020 09:57:20 GMT   (2137kb,D)

Title: Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive
  Review
Authors: Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin
  Fu, Surya Nepal, and Hyoungshick Kim
Categories: cs.CR cs.CV cs.LG
Comments: 29 pages, 9 figures, 2 tables
\\ ( https://arxiv.org/abs/2007.10760 ,  2137kb)
------------------------------------------------------------------------------
\\
arXiv:2007.10791
replaced with revised version Mon, 27 Jul 2020 01:44:38 GMT   (4650kb,D)

Title: Learning to Match Distributions for Domain Adaptation
Authors: Chaohui Yu, Jindong Wang, Chang Liu, Tao Qin, Renjun Xu, Wenjie Feng,
  Yiqiang Chen, Tie-Yan Liu
Categories: cs.LG cs.CV stat.ML
Comments: Preprint. 20 Pages. Code available at
  https://github.com/jindongwang/transferlearning/tree/master/code/deep/Learning-to-Match
\\ ( https://arxiv.org/abs/2007.10791 ,  4650kb)
------------------------------------------------------------------------------
\\
arXiv:2007.10976
replaced with revised version Fri, 24 Jul 2020 18:26:27 GMT   (159kb)

Title: Interactive Inference under Information Constraints
Authors: Jayadev Acharya, Cl\'ement L. Canonne, Yuhan Liu, Ziteng Sun, and
  Himanshu Tyagi
Categories: cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH
Comments: Clarified the discussion of previous work in Section 1.3
\\ ( https://arxiv.org/abs/2007.10976 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2002.05282
replaced with revised version Sat, 25 Jul 2020 20:33:33 GMT   (8942kb,D)

Title: A Bounded Measure for Estimating the Benefit of Visualization
Authors: Min Chen, Mateu Sbert, Alfie Abdul-Rahman, and Deborah Silver
Categories: cs.AI cs.GR cs.HC cs.IT math.IT
Comments: Comment on version 2: This revised version, which includes a new
  formal proof, many additions, and a detailed revision report, was submitted
  to SciVis 2020. Unexpectedly, our revision effort did not have much influence
  on the SciVis 2020 reviewers who gave an outright rejection with lower scores
  than EuroVis reviews. We will share these reviews after we have completed our
  feedback
\\ ( https://arxiv.org/abs/2002.05282 ,  8942kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
