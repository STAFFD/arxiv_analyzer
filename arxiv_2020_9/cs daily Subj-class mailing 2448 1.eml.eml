Delivered-To: brucelu2013@gmail.com
Received: by 2002:ab4:a06d:0:0:0:0:0 with SMTP id cz13csp4017600ecb;
        Wed, 26 Aug 2020 01:48:43 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJyrDJTyArOijyDfo5PQJ1QEvnC9jIIAgXE5HYD/0/qABku85B08CaTgfCTKbhVijVjn7LEa
X-Received: by 2002:a37:9987:: with SMTP id b129mr12769256qke.315.1598431723172;
        Wed, 26 Aug 2020 01:48:43 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1598431723; cv=none;
        d=google.com; s=arc-20160816;
        b=dETbsItreW+KN+KlEwADoVBSl4hBWEO98KVGtycRmxrX63ctypwCYkR+Ts2n4B3Gb3
         8OnYsdd8ES1GiVox6XhZ0P4YtRT90heynPc49PAUB+i7E0Z6sLSYJOANuUPpEdx3QaDI
         M/zsDrcqf/kXYDKkou2qIl/tPUJ+O5uc8yPICWjSDPe5+82qcjVQx2yar4sHLmnyQvvY
         myvI0CWGeJyt5dKKpD5EQLuf/4zRfXmgq3QO5bndVCTSj1uk03mzL/P1j8oZUG9lSmK+
         hkGbHRXIddUFXkWcOR1Xg+/8p5O36crcY/sjmLiYvpADw4WXQ7OLrbMnWsFwQcbNkEiC
         T27g==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=Xj6KYTEgiBLYdT0YYeOZmVFfiCTSzeJLQ4tk45/R3Ls=;
        b=xbVDzlLAFtNfB3c8vvDszspVlo6dt1kK2vSe1ABnGH3sK8UN3eabFtcEbPCxZI8Ley
         W+DlFs+u/X6pgYYRrd55JzQtZnD+mp+o1zliYvBHbO0/+Z5OQDZl6s7h8hOvutnT212b
         sbOQZ8+ZilaxYCgnEbJJsoNuzbbtyg0Y8RPAzGXbEvV61VS2gYQHaju9PECWXrXM6etL
         KNaF9YL0OoGKicDLrlPX2NYlyiWx6fJWpC17FN9n6NI3Onf7NmZd6+e41KYfx3mnN0kI
         8uYpO9uhVxiy8FW0bzderkRrLtKiijvMoXszTZEigsHP3KrmOcFJZQ41sPzuk9kRoGKu
         qM3Q==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id 62si1020085qkm.126.2020.08.26.01.48.42
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 26 Aug 2020 01:48:43 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07Q8mgPw016206;
	Wed, 26 Aug 2020 04:48:42 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 07Q8mg1j039614;
	Wed, 26 Aug 2020 04:48:42 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 07Q8mgFD039613;
	Wed, 26 Aug 2020 04:48:42 -0400
Date: Wed, 26 Aug 2020 04:48:42 -0400
Message-Id: <202008260848.07Q8mgFD039613@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 2448 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Graphics
 received from  Mon 24 Aug 20 18:00:00 GMT  to  Tue 25 Aug 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2008.10794
Date: Tue, 25 Aug 2020 03:11:28 GMT   (148kb,D)

Title: Simple Topological Drawings of $k$-Planar Graphs
Authors: Michael Hoffmann and Chih-Hung Liu and Meghana M. Reddy and Csaba D.
  T\'oth
Categories: cs.CG cs.DM math.CO
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\
  Every finite graph admits a \emph{simple (topological) drawing}, that is, a
drawing where every pair of edges intersects in at most one point. However, in
combination with other restrictions simple drawings do not universally exist.
For instance, \emph{$k$-planar graphs} are those graphs that can be drawn so
that every edge has at most $k$ crossings (i.e., they admit a \emph{$k$-plane
drawing}). It is known that for $k\le 3$, every $k$-planar graph admits a
$k$-plane simple drawing. But for $k\ge 4$, there exist $k$-planar graphs that
do not admit a $k$-plane simple drawing. Answering a question by Schaefer, we
show that there exists a function $f : \mathbb{N}\rightarrow\mathbb{N}$ such
that every $k$-planar graph admits an $f(k)$-plane simple drawing, for all
$k\in\mathbb{N}$. Note that the function $f$ depends on $k$ only and is
independent of the size of the graph. Furthermore, we develop an algorithm to
show that every $4$-planar graph admits an $8$-plane simple drawing.
\\ ( https://arxiv.org/abs/2008.10794 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11058
Date: Tue, 25 Aug 2020 14:34:30 GMT   (282kb,D)

Title: On the Maximum Number of Crossings in Star-Simple Drawings of $K_n$ with
  No Empty Lens
Authors: Stefan Felsner, Michael Hoffmann, Kristin Knorr, Irene Parada
Categories: cs.CG cs.DM math.CO
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\
  A star-simple drawing of a graph is a drawing in which adjacent edges do not
cross. In contrast, there is no restriction on the number of crossings between
two independent edges. When allowing empty lenses (a face in the arrangement
induced by two edges that is bounded by a 2-cycle), two independent edges may
cross arbitrarily many times in a star-simple drawing. We consider star-simple
drawings of $K_n$ with no empty lens. In this setting we prove an upper bound
of $3((n-4)!)$ on the maximum number of crossings between any pair of edges. It
follows that the total number of crossings is finite and upper bounded by $n!$.
\\ ( https://arxiv.org/abs/2008.11058 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10634
Date: Mon, 24 Aug 2020 18:12:49 GMT   (10665kb,D)

Title: DiverseNet: When One Right Answer is not Enough
Authors: Michael Firman, Neill D. F. Campbell, Lourdes Agapito, Gabriel J.
  Brostow
Categories: cs.CV
Comments: Presented at CVPR 2018
\\
  Many structured prediction tasks in machine vision have a collection of
acceptable answers, instead of one definitive ground truth answer. Segmentation
of images, for example, is subject to human labeling bias. Similarly, there are
multiple possible pixel values that could plausibly complete occluded image
regions. State-of-the art supervised learning methods are typically optimized
to make a single test-time prediction for each query, failing to find other
modes in the output space. Existing methods that allow for sampling often
sacrifice speed or accuracy.
  We introduce a simple method for training a neural network, which enables
diverse structured predictions to be made for each test-time query. For a
single input, we learn to predict a range of possible answers. We compare
favorably to methods that seek diversity through an ensemble of networks. Such
stochastic multiple choice learning faces mode collapse, where one or more
ensemble members fail to receive any training signal. Our best performing
solution can be deployed for various tasks, and just involves small
modifications to the existing single-mode architecture, loss function, and
training regime. We demonstrate that our method results in quantitative
improvements across three challenging tasks: 2D image completion, 3D volume
estimation, and flow prediction.
\\ ( https://arxiv.org/abs/2008.10634 ,  10665kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10678
Date: Mon, 24 Aug 2020 19:51:48 GMT   (3418kb,D)

Title: Probabilistic Deep Learning for Instance Segmentation
Authors: Josef Lorenz Rumberger, Lisa Mais, Dagmar Kainmueller
Categories: cs.CV cs.LG
Comments: ECCV 2020 BioImage Computing Workshop
\\
  Probabilistic convolutional neural networks, which predict distributions of
predictions instead of point estimates, led to recent advances in many areas of
computer vision, from image reconstruction to semantic segmentation. Besides
state of the art benchmark results, these networks made it possible to quantify
local uncertainties in the predictions. These were used in active learning
frameworks to target the labeling efforts of specialist annotators or to assess
the quality of a prediction in a safety-critical environment. However, for
instance segmentation problems these methods are not frequently used so far. We
seek to close this gap by proposing a generic method to obtain model-inherent
uncertainty estimates within proposal-free instance segmentation models.
Furthermore, we analyze the quality of the uncertainty estimates with a metric
adapted from semantic segmentation. We evaluate our method on the BBBC010 C.\
elegans dataset, where it yields competitive performance while also predicting
uncertainty estimates that carry information about object-level inaccuracies
like false splits and false merges. We perform a simulation to show the
potential use of such uncertainty estimates in guided proofreading.
\\ ( https://arxiv.org/abs/2008.10678 ,  3418kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10680
Date: Mon, 24 Aug 2020 20:00:39 GMT   (16415kb,D)

Title: Video Interpolation via Generalized Deformable Convolution
Authors: Zhihao Shi, Xiaohong Liu, Kangdi Shi, Linhui Dai, Jun Chen
Categories: cs.CV
Comments: 11pages, journal
\\
  Video interpolation aims at increasing the frame rate of a given video by
synthesizing intermediate frames. The existing video interpolation methods can
be roughly divided into two categories: flow-based methods and kernel-based
methods. The performance of flow-based methods is often jeopardized by the
inaccuracy of flow map estimation due to oversimplified motion models while
that of kernel-based methods tends to be constrained by the rigidity of kernel
shape. To address these performance-limiting issues, a novel mechanism named
generalized deformable convolution is proposed, which can effectively learn
motion information in a data-driven manner and freely select sampling points in
space-time. We further develop a new video interpolation method based on this
mechanism. Our extensive experiments demonstrate that the new method performs
favorably against the state-of-the-art, especially when dealing with complex
motions.
\\ ( https://arxiv.org/abs/2008.10680 ,  16415kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10719
Date: Mon, 24 Aug 2020 21:51:29 GMT   (24292kb,D)

Title: Interactive Annotation of 3D Object Geometry using 2D Scribbles
Authors: Tianchang Shen, Jun Gao, Amlan Kar, Sanja Fidler
Categories: cs.CV cs.LG
Comments: Accepted to ECCV 2020
\\
  Inferring detailed 3D geometry of the scene is crucial for robotics
applications, simulation, and 3D content creation. However, such information is
hard to obtain, and thus very few datasets support it. In this paper, we
propose an interactive framework for annotating 3D object geometry from both
point cloud data and RGB imagery. The key idea behind our approach is to
exploit strong priors that humans have about the 3D world in order to
interactively annotate complete 3D shapes. Our framework targets naive users
without artistic or graphics expertise. We introduce two simple-to-use
interaction modules. First, we make an automatic guess of the 3D shape and
allow the user to provide feedback about large errors by drawing scribbles in
desired 2D views. Next, we aim to correct minor errors, in which users drag and
drop mesh vertices, assisted by a neural interactive module implemented as a
Graph Convolutional Network. Experimentally, we show that only a few user
interactions are needed to produce good quality 3D shapes on popular benchmarks
such as ShapeNet, Pix3D and ScanNet. We implement our framework as a web
service and conduct a user study, where we show that user annotated data using
our method effectively facilitates real-world learning tasks. Web service:
http://www.cs.toronto.edu/~shenti11/scribble3d.
\\ ( https://arxiv.org/abs/2008.10719 ,  24292kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10736
Date: Mon, 24 Aug 2020 22:32:30 GMT   (498kb)

Title: LULC Segmentation of RGB Satellite Image Using FCN-8
Authors: Abu Bakar Siddik Nayem, Anis Sarker, Ovi Paul, Amin Ali, Md. Ashraful
  Amin and AKM Mahbubur Rahman
Categories: cs.CV cs.LG eess.IV
Comments: Accepted paper at 3rd SLAAI-International Conference on Artificial
  Intelligence; 13 pages, 7 figures, 3 tables
\\
  This work presents use of Fully Convolutional Network (FCN-8) for semantic
segmentation of high-resolution RGB earth surface satel-lite images into land
use land cover (LULC) categories. Specically, we propose a non-overlapping
grid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16
weights to segment satellite im-ages into four (forest, built-up, farmland and
water) classes. The FCN-8 semantically projects the discriminating features in
lower resolution learned by the encoder onto the pixel space in higher
resolution to get a dense classi cation. We experimented the proposed system
with Gaofen-2 image dataset, that contains 150 images of over 60 di erent
cities in china. For comparison, we used available ground-truth along with
images segmented using a widely used commeriial GIS software called
eCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8
obtains signi cantly improved performance, than the eCognition soft-ware. Our
model achieves average accuracy of 91.0% and average Inter-section over Union
(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is
0.60. This paper also reports a detail analysis of errors occurred at the LULC
boundary.
\\ ( https://arxiv.org/abs/2008.10736 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10774
Date: Tue, 25 Aug 2020 01:22:52 GMT   (3443kb,D)

Title: Image Colorization: A Survey and Dataset
Authors: Saeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz
  Khan, Abdul Wahab Muzaffar
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  Image colorization is an essential image processing and computer vision
branch to colorize images and videos. Recently, deep learning techniques
progressed notably for image colorization. This article presents a
comprehensive survey of recent state-of-the-art colorization using deep
learning algorithms, describing their fundamental block architectures in terms
of skip connections, input \etc as well as optimizers, loss functions, training
protocols, and training data \etc Generally, we can roughly categorize the
existing colorization techniques into seven classes. Besides, we also provide
some additional essential issues, such as benchmark datasets and evaluation
metrics. We also introduce a new dataset specific to colorization and perform
an experimental evaluation of the publicly available methods. In the last
section, we discuss the limitations, possible solutions, and future research
directions of the rapidly evolving topic of deep image colorization that the
community should further address. Dataset and Codes for evaluation will be
publicly available at https://github.com/saeed-anwar/ColorSurvey
\\ ( https://arxiv.org/abs/2008.10774 ,  3443kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10785
Date: Tue, 25 Aug 2020 02:28:24 GMT   (10945kb,D)

Title: Learning Target Domain Specific Classifier for Partial Domain Adaptation
Authors: Chuan-Xian Ren, Pengfei Ge, Peiyi Yang, Shuicheng Yan
Categories: cs.CV
DOI: 10.1109/TNNLS.2020.2995648
\\
  Unsupervised domain adaptation~(UDA) aims at reducing the distribution
discrepancy when transferring knowledge from a labeled source domain to an
unlabeled target domain. Previous UDA methods assume that the source and target
domains share an identical label space, which is unrealistic in practice since
the label information of the target domain is agnostic. This paper focuses on a
more realistic UDA scenario, i.e. partial domain adaptation (PDA), where the
target label space is subsumed to the source label space. In the PDA scenario,
the source outliers that are absent in the target domain may be wrongly matched
to the target domain (technically named negative transfer), leading to
performance degradation of UDA methods. This paper proposes a novel Target
Domain Specific Classifier Learning-based Domain Adaptation (TSCDA) method.
TSCDA presents a soft-weighed maximum mean discrepancy criterion to partially
align feature distributions and alleviate negative transfer. Also, it learns a
target-specific classifier for the target domain with pseudo-labels and
multiple auxiliary classifiers, to further address classifier shift. A module
named Peers Assisted Learning is used to minimize the prediction difference
between multiple target-specific classifiers, which makes the classifiers more
discriminant for the target domain. Extensive experiments conducted on three
PDA benchmark datasets show that TSCDA outperforms other state-of-the-art
methods with a large margin, e.g. $4\%$ and $5.6\%$ averagely on Office-31 and
Office-Home, respectively.
\\ ( https://arxiv.org/abs/2008.10785 ,  10945kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10786
Date: Tue, 25 Aug 2020 02:33:33 GMT   (12324kb,D)

Title: Data Science for Motion and Time Analysis with Modern Motion Sensor Data
Authors: Chiwoo Park, Sang Do Noh and Anuj Srivastava
Categories: cs.CV cs.LG math.OC
Comments: Keywords: motion and time study, motion sensors, Riemannian manifold,
  probability distribution on manifold, temporal evolution of probability
  distributions
\\
  The motion-and-time analysis has been a popular research topic in operations
research, especially for analyzing work performances in manufacturing and
service operations. It is regaining attention as continuous improvement tools
for lean manufacturing and smart factory. This paper develops a framework for
data-driven analysis of work motions and studies their correlations to work
speeds or execution rates, using data collected from modern motion sensors. The
past analyses largely relied on manual steps involving time-consuming
stop-watching and video-taping, followed by manual data analysis. While modern
sensing devices have automated the collection of motion data, the motion
analytics that transform the new data into knowledge are largely
underdeveloped. Unsolved technical questions include: How the motion and time
information can be extracted from the motion sensor data, how work motions and
execution rates are statistically modeled and compared, and what are the
statistical correlations of motions to the rates? In this paper, we develop a
novel mathematical framework for motion and time analysis with motion sensor
data, by defining new mathematical representation spaces of human motions and
execution rates and by developing statistical tools on these new spaces. This
methodological research is demonstrated using five use cases applied to
manufacturing motion data.
\\ ( https://arxiv.org/abs/2008.10786 ,  12324kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10824
Date: Tue, 25 Aug 2020 05:30:37 GMT   (1961kb)

Title: A Critical Analysis of Patch Similarity Based Image Denoising Algorithms
Authors: Varuna De Silva
Categories: cs.CV cs.GR eess.IV
\\
  Image denoising is a classical signal processing problem that has received
significant interest within the image processing community during the past two
decades. Most of the algorithms for image denoising has focused on the paradigm
of non-local similarity, where image blocks in the neighborhood that are
similar, are collected to build a basis for reconstruction. Through rigorous
experimentation, this paper reviews multiple aspects of image denoising
algorithm development based on non-local similarity. Firstly, the concept of
non-local similarity as a foundational quality that exists in natural images
has not received adequate attention. Secondly, the image denoising algorithms
that are developed are a combination of multiple building blocks, making
comparison among them a tedious task. Finally, most of the work surrounding
image denoising presents performance results based on Peak-Signal-to-Noise
Ratio (PSNR) between a denoised image and a reference image (which is perturbed
with Additive White Gaussian Noise). This paper starts with a statistical
analysis on non-local similarity and its effectiveness under various noise
levels, followed by a theoretical comparison of different state-of-the-art
image denoising algorithms. Finally, we argue for a methodological overhaul to
incorporate no-reference image quality measures and unprocessed images (raw)
during performance evaluation of image denoising algorithms.
\\ ( https://arxiv.org/abs/2008.10824 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10831
Date: Tue, 25 Aug 2020 05:53:59 GMT   (7985kb,D)

Title: CDeC-Net: Composite Deformable Cascade Network for Table Detection in
  Document Images
Authors: Madhav Agarwal and Ajoy Mondal and C. V. Jawahar
Categories: cs.CV
Comments: 12
\\
  Localizing page elements/objects such as tables, figures, equations, etc. is
the primary step in extracting information from document images. We propose a
novel end-to-end trainable deep network, (CDeC-Net) for detecting tables
present in the documents. The proposed network consists of a multistage
extension of Mask R-CNN with a dual backbone having deformable convolution for
detecting tables varying in scale with high detection accuracy at higher IoU
threshold. We empirically evaluate CDeC-Net on all the publicly available
benchmark datasets - ICDAR-2013, ICDAR-2017, ICDAR-2019,UNLV, Marmot,
PubLayNet, and TableBank - with extensive experiments.
  Our solution has three important properties: (i) a single trained model
CDeC-Net{\ddag} performs well across all the popular benchmark datasets; (ii)
we report excellent performances across multiple, including higher, thresholds
of IoU; (iii) by following the same protocol of the recent papers for each of
the benchmarks, we consistently demonstrate the superior quantitative
performance. Our code and models will be publicly released for enabling the
reproducibility of the results.
\\ ( https://arxiv.org/abs/2008.10831 ,  7985kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10833
Date: Tue, 25 Aug 2020 06:00:06 GMT   (30190kb,D)

Title: Adaptive Context-Aware Multi-Modal Network for Depth Completion
Authors: Shanshan Zhao, Mingming Gong, Huan Fu, and Dacheng Tao
Categories: cs.CV
\\
  Depth completion aims to recover a dense depth map from the sparse depth data
and the corresponding single RGB image. The observed pixels provide the
significant guidance for the recovery of the unobserved pixels' depth. However,
due to the sparsity of the depth data, the standard convolution operation,
exploited by most of existing methods, is not effective to model the observed
contexts with depth values. To address this issue, we propose to adopt the
graph propagation to capture the observed spatial contexts. Specifically, we
first construct multiple graphs at different scales from observed pixels. Since
the graph structure varies from sample to sample, we then apply the attention
mechanism on the propagation, which encourages the network to model the
contextual information adaptively. Furthermore, considering the mutli-modality
of input data, we exploit the graph propagation on the two modalities
respectively to extract multi-modal representations. Finally, we introduce the
symmetric gated fusion strategy to exploit the extracted multi-modal features
effectively. The proposed strategy preserves the original information for one
modality and also absorbs complementary information from the other through
learning the adaptive gating weights. Our model, named Adaptive Context-Aware
Multi-Modal Network (ACMNet), achieves the state-of-the-art performance on two
benchmarks, {\it i.e.}, KITTI and NYU-v2, and at the same time has fewer
parameters than latest models. Our code is available at:
\url{https://github.com/sshan-zhao/ACMNet}.
\\ ( https://arxiv.org/abs/2008.10833 ,  30190kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10843
Date: Tue, 25 Aug 2020 06:35:57 GMT   (2779kb)

Title: Graphical Object Detection in Document Images
Authors: Ranajit Saha and Ajoy Mondal and C. V. Jawahar
Categories: cs.CV
Comments: 8
Journal-ref: ICDAR 2019
\\
  Graphical elements: particularly tables and figures contain a visual summary
of the most valuable information contained in a document. Therefore,
localization of such graphical objects in the document images is the initial
step to understand the content of such graphical objects or document images. In
this paper, we present a novel end-to-end trainable deep learning based
framework to localize graphical objects in the document images called as
Graphical Object Detection (GOD). Our framework is data-driven and does not
require any heuristics or meta-data to locate graphical objects in the document
images. The GOD explores the concept of transfer learning and domain adaptation
to handle scarcity of labeled training images for graphical object detection
task in the document images. Performance analysis carried out on the various
public benchmark data sets: ICDAR-2013, ICDAR-POD2017,and UNLV shows that our
model yields promising results as compared to state-of-the-art techniques.
\\ ( https://arxiv.org/abs/2008.10843 ,  2779kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10850
Date: Tue, 25 Aug 2020 07:15:51 GMT   (488kb,D)

Title: Discriminability Distillation in Group Representation Learning
Authors: Manyuan Zhang, Guanglu Song, Hang Zhou, Yu Liu
Categories: cs.CV
Comments: To appear in Proceedings of the European Conference on Computer
  Vision (ECCV), 2020
\\
  Learning group representation is a commonly concerned issue in tasks where
the basic unit is a group, set, or sequence. Previously, the research community
tries to tackle it by aggregating the elements in a group based on an indicator
either defined by humans such as the quality and saliency, or generated by a
black box such as the attention score. This article provides a more essential
and explicable view. We claim the most significant indicator to show whether
the group representation can be benefited from one of its element is not the
quality or an inexplicable score, but the discriminability w.r.t. the model. We
explicitly design the discrimiability using embedded class centroids on a proxy
set. We show the discrimiability knowledge has good properties that can be
distilled by a light-weight distillation network and can be generalized on the
unseen target set. The whole procedure is denoted as discriminability
distillation learning (DDL). The proposed DDL can be flexibly plugged into many
group-based recognition tasks without influencing the original training
procedures. Comprehensive experiments on various tasks have proven the
effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes
forward the state-of-the-art results on these tasks by an impressive margin.
\\ ( https://arxiv.org/abs/2008.10850 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10869
Date: Tue, 25 Aug 2020 07:59:15 GMT   (2618kb,D)

Title: Two-Stream Networks for Lane-Change Prediction of Surrounding Vehicles
Authors: David Fern\'andez-Llorca, Mahdi Biparva, Rub\'en Izquierdo-Gonzalo and
  John K. Tsotsos
Categories: cs.CV cs.AI cs.RO
Comments: This work has been accepted at the IEEE Intelligent Transportation
  Systems Conference 2020
\\
  In highway scenarios, an alert human driver will typically anticipate early
cut-in and cut-out maneuvers of surrounding vehicles using only visual cues. An
automated system must anticipate these situations at an early stage too, to
increase the safety and the efficiency of its performance. To deal with
lane-change recognition and prediction of surrounding vehicles, we pose the
problem as an action recognition/prediction problem by stacking visual cues
from video cameras. Two video action recognition approaches are analyzed:
two-stream convolutional networks and spatiotemporal multiplier networks.
Different sizes of the regions around the vehicles are analyzed, evaluating the
importance of the interaction between vehicles and the context information in
the performance. In addition, different prediction horizons are evaluated. The
obtained results demonstrate the potential of these methodologies to serve as
robust predictors of future lane-changes of surrounding vehicles in time
horizons between 1 and 2 seconds.
\\ ( https://arxiv.org/abs/2008.10869 ,  2618kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10902
Date: Tue, 25 Aug 2020 09:15:48 GMT   (14641kb,D)

Title: Confidence-aware Adversarial Learning for Self-supervised Semantic
  Matching
Authors: Shuaiyi Huang, Qiuyue Wang, Xuming He
Categories: cs.CV
Comments: PRCV 2020
\\
  In this paper, we aim to address the challenging task of semantic matching
where matching ambiguity is difficult to resolve even with learned deep
features. We tackle this problem by taking into account the confidence in
predictions and develop a novel refinement strategy to correct partial matching
errors. Specifically, we introduce a Confidence-Aware Semantic Matching Network
(CAMNet) which instantiates two key ideas of our approach. First, we propose to
estimate a dense confidence map for a matching prediction through
self-supervised learning. Second, based on the estimated confidence, we refine
initial predictions by propagating reliable matching to the rest of locations
on the image plane. In addition, we develop a new hybrid loss in which we
integrate a semantic alignment loss with a confidence loss, and an adversarial
loss that measures the quality of semantic correspondence. We are the first
that exploit confidence during refinement to improve semantic matching accuracy
and develop an end-to-end self-supervised adversarial learning procedure for
the entire matching network. We evaluate our method on two public benchmarks,
on which we achieve top performance over the prior state of the art. We will
release our source code at https://github.com/ShuaiyiHuang/CAMNet.
\\ ( https://arxiv.org/abs/2008.10902 ,  14641kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10913
Date: Tue, 25 Aug 2020 09:47:58 GMT   (12461kb,D)

Title: MonStereo: When Monocular and Stereo Meet at the Tail of 3D Human
  Localization
Authors: Lorenzo Bertoni, Sven Kreiss, Taylor Mordan, Alexandre Alahi
Categories: cs.CV
\\
  Monocular and stereo vision are cost-effective solutions for 3D human
localization in the context of self-driving cars or social robots. However,
they are usually developed independently and have their respective strengths
and limitations. We propose a novel unified learning framework that leverages
the strengths of both monocular and stereo cues for 3D human localization. Our
method jointly (i) associates humans in left-right images, (ii) deals with
occluded and distant cases in stereo settings by relying on the robustness of
monocular cues, and (iii) tackles the intrinsic ambiguity of monocular
perspective projection by exploiting prior knowledge of human height
distribution. We achieve state-of-the-art quantitative results for the 3D
localization task on KITTI dataset and estimate confidence intervals that
account for challenging instances. We show qualitative examples for the long
tail challenges such as occluded, far-away, and children instances.
\\ ( https://arxiv.org/abs/2008.10913 ,  12461kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10916
Date: Tue, 25 Aug 2020 09:51:33 GMT   (1565kb,D)

Title: Towards End-to-end Car License Plate Location and Recognition in
  Unconstrained Scenarios
Authors: Shuxin Qin and Sijiang Liu
Categories: cs.CV cs.AI cs.LG
\\
  Benefiting from the rapid development of convolutional neural networks, the
performance of car license plate detection and recognition has been largely
improved. Nonetheless, challenges still exist especially for real-world
applications. In this paper, we present an efficient and accurate framework to
solve the license plate detection and recognition tasks simultaneously. It is a
lightweight and unified deep neural network, that can be optimized end-to-end
and work in real-time. Specifically, for unconstrained scenarios, an
anchor-free method is adopted to efficiently detect the bounding box and four
corners of a license plate, which are used to extract and rectify the target
region features. Then, a novel convolutional neural network branch is designed
to further extract features of characters without segmentation. Finally,
recognition task is treated as sequence labelling problems, which are solved by
Connectionist Temporal Classification (CTC) directly. Several public datasets
including images collected from different scenarios under various conditions
are chosen for evaluation. A large number of experiments indicate that the
proposed method significantly outperforms the previous state-of-the-art methods
in both speed and precision.
\\ ( https://arxiv.org/abs/2008.10916 ,  1565kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10924
Date: Tue, 25 Aug 2020 10:14:13 GMT   (3001kb,D)

Title: Think about boundary: Fusing multi-level boundary information for
  landmark heatmap regression
Authors: Jinheng Xie, Jun Wan, Linlin Shen, Zhihui Lai
Categories: cs.CV
Comments: 12 pages, 9 figures
\\
  Although current face alignment algorithms have obtained pretty good
performances at predicting the location of facial landmarks, huge challenges
remain for faces with severe occlusion and large pose variations, etc. On the
contrary, semantic location of facial boundary is more likely to be reserved
and estimated on these scenes. Therefore, we study a two-stage but end-to-end
approach for exploring the relationship between the facial boundary and
landmarks to get boundary-aware landmark predictions, which consists of two
modules: the self-calibrated boundary estimation (SCBE) module and the
boundary-aware landmark transform (BALT) module. In the SCBE module, we modify
the stem layers and employ intermediate supervision to help generate
high-quality facial boundary heatmaps. Boundary-aware features inherited from
the SCBE module are integrated into the BALT module in a multi-scale fusion
framework to better model the transformation from boundary to landmark heatmap.
Experimental results conducted on the challenging benchmark datasets
demonstrate that our approach outperforms state-of-the-art methods in the
literature.
\\ ( https://arxiv.org/abs/2008.10924 ,  3001kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10960
Date: Tue, 25 Aug 2020 12:35:48 GMT   (15967kb,D)

Title: AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with
  Spatially-Aware Conditional GANs
Authors: Julien Despois, Frederic Flament, Matthieu Perrot
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://despoisj.github.io/AgingMapGAN/
\\
  Existing approaches and datasets for face aging produce results skewed
towards the mean, with individual variations and expression wrinkles often
invisible or overlooked in favor of global patterns such as the fattening of
the face. Moreover, they offer little to no control over the way the faces are
aged and can difficultly be scaled to large images, thus preventing their usage
in many real-world applications. To address these limitations, we present an
approach to change the appearance of a high-resolution image using
ethnicity-specific aging information and weak spatial supervision to guide the
aging process. We demonstrate the advantage of our proposed method in terms of
quality, control, and how it can be used on high-definition images while
limiting the computational overhead.
\\ ( https://arxiv.org/abs/2008.10960 ,  15967kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10966
Date: Tue, 25 Aug 2020 12:45:39 GMT   (3165kb,D)

Title: In-Home Daily-Life Captioning Using Radio Signals
Authors: Lijie Fan, Tianhong Li, Yuan Yuan, Dina Katabi
Categories: cs.CV
Comments: ECCV 2020. The first two authors contributed equally to this paper
\\
  This paper aims to caption daily life --i.e., to create a textual description
of people's activities and interactions with objects in their homes. Addressing
this problem requires novel methods beyond traditional video captioning, as
most people would have privacy concerns about deploying cameras throughout
their homes. We introduce RF-Diary, a new model for captioning daily life by
analyzing the privacy-preserving radio signal in the home with the home's
floormap. RF-Diary can further observe and caption people's life through walls
and occlusions and in dark settings. In designing RF-Diary, we exploit the
ability of radio signals to capture people's 3D dynamics, and use the floormap
to help the model learn people's interactions with objects. We also use a
multi-modal feature alignment training scheme that leverages existing
video-based captioning datasets to improve the performance of our radio-based
captioning model. Extensive experimental results demonstrate that RF-Diary
generates accurate captions under visible conditions. It also sustains its good
performance in dark or occluded settings, where video-based captioning
approaches fail to generate meaningful captions. For more information, please
visit our project webpage: http://rf-diary.csail.mit.edu
\\ ( https://arxiv.org/abs/2008.10966 ,  3165kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10968
Date: Tue, 25 Aug 2020 12:47:09 GMT   (122kb,D)

Title: Active Class Incremental Learning for Imbalanced Datasets
Authors: Eden Belouadah, Adrian Popescu, Umang Aggarwal, L\'eo Saci
Categories: cs.CV
Comments: Accepted in IPCV workshop from ECCV2020
\\
  Incremental Learning (IL) allows AI systems to adapt to streamed data. Most
existing algorithms make two strong hypotheses which reduce the realism of the
incremental scenario: (1) new data are assumed to be readily annotated when
streamed and (2) tests are run with balanced datasets while most real-life
datasets are actually imbalanced. These hypotheses are discarded and the
resulting challenges are tackled with a combination of active and imbalanced
learning. We introduce sample acquisition functions which tackle imbalance and
are compatible with IL constraints. We also consider IL as an imbalanced
learning problem instead of the established usage of knowledge distillation
against catastrophic forgetting. Here, imbalance effects are reduced during
inference through class prediction scaling. Evaluation is done with four visual
datasets and compares existing and proposed sample acquisition functions.
Results indicate that the proposed contributions have a positive effect and
reduce the gap between active and standard IL performance.
\\ ( https://arxiv.org/abs/2008.10968 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11009
Date: Tue, 25 Aug 2020 13:48:35 GMT   (1780kb,D)

Title: Protect, Show, Attend and Tell: Image Captioning Model with Ownership
  Protection
Authors: Jian Han Lim, Chee Seng Chan, Kam Woh Ng, Lixin Fan, Qiang Yang
Categories: cs.CV cs.CR
Comments: 9 pages
\\
  By and large, existing Intellectual Property Right (IPR) protection on deep
neural networks typically i) focus on image classification task only, and ii)
follow a standard digital watermarking framework that were conventionally used
to protect the ownership of multimedia and video content. This paper
demonstrates that current digital watermarking framework is insufficient to
protect image captioning task that often regarded as one of the frontier A.I.
problems. As a remedy, this paper studies and proposes two different embedding
schemes in the hidden memory state of a recurrent neural network to protect
image captioning model. From both theoretically and empirically points, we
prove that a forged key will yield an unusable image captioning model,
defeating the purpose on infringement. To the best of our knowledge, this work
is the first to propose ownership protection on image captioning task. Also,
extensive experiments show that the proposed method does not compromise the
original image captioning performance on all common captioning metrics on
Flickr30k and MS-COCO datasets, and at the same time it is able to withstand
both removal and ambiguity attacks.
\\ ( https://arxiv.org/abs/2008.11009 ,  1780kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11014
Date: Wed, 5 Aug 2020 08:28:18 GMT   (8980kb,D)

Title: Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet
  Transform and Markov Random Field
Authors: Haixia Bi, Lin Xu, Xiangyong Cao, Yong Xue, Zongben Xu
Categories: cs.CV
Journal-ref: IEEE Transactions on Image Processing (2020)
DOI: 10.1109/TIP.2020.2992177
\\
  Polarimetric synthetic aperture radar (PolSAR) image segmentation is
currently of great importance in image processing for remote sensing
applications. However, it is a challenging task due to two main reasons.
Firstly, the label information is difficult to acquire due to high annotation
costs. Secondly, the speckle effect embedded in the PolSAR imaging process
remarkably degrades the segmentation performance. To address these two issues,
we present a contextual PolSAR image semantic segmentation method in this
paper.With a newly defined channelwise consistent feature set as input, the
three-dimensional discrete wavelet transform (3D-DWT) technique is employed to
extract discriminative multi-scale features that are robust to speckle noise.
Then Markov random field (MRF) is further applied to enforce label smoothness
spatially during segmentation. By simultaneously utilizing 3D-DWT features and
MRF priors for the first time, contextual information is fully integrated
during the segmentation to ensure accurate and smooth segmentation. To
demonstrate the effectiveness of the proposed method, we conduct extensive
experiments on three real benchmark PolSAR image data sets. Experimental
results indicate that the proposed method achieves promising segmentation
accuracy and preferable spatial consistency using a minimal number of labeled
pixels.
\\ ( https://arxiv.org/abs/2008.11014 ,  8980kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11048
Date: Tue, 25 Aug 2020 14:23:38 GMT   (933kb,D)

Title: Label Decoupling Framework for Salient Object Detection
Authors: Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang, Qi Tian
Categories: cs.CV
Comments: Accepted by CVPR2020, https://github.com/weijun88/LDF
\\
  To get more accurate saliency maps, recent methods mainly focus on
aggregating multi-level features from fully convolutional network (FCN) and
introducing edge information as auxiliary supervision. Though remarkable
progress has been achieved, we observe that the closer the pixel is to the
edge, the more difficult it is to be predicted, because edge pixels have a very
imbalance distribution. To address this problem, we propose a label decoupling
framework (LDF) which consists of a label decoupling (LD) procedure and a
feature interaction network (FIN). LD explicitly decomposes the original
saliency map into body map and detail map, where body map concentrates on
center areas of objects and detail map focuses on regions around edges. Detail
map works better because it involves much more pixels than traditional edge
supervision. Different from saliency map, body map discards edge pixels and
only pays attention to center areas. This successfully avoids the distraction
from edge pixels during training. Therefore, we employ two branches in FIN to
deal with body map and detail map respectively. Feature interaction (FI) is
designed to fuse the two complementary branches to predict the saliency map,
which is then used to refine the two branches again. This iterative refinement
is helpful for learning better representations and more precise saliency maps.
Comprehensive experiments on six benchmark datasets demonstrate that LDF
outperforms state-of-the-art approaches on different evaluation metrics.
\\ ( https://arxiv.org/abs/2008.11048 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11055
Date: Tue, 25 Aug 2020 14:29:05 GMT   (5920kb,D)

Title: On estimating gaze by self-attention augmented convolutions
Authors: Gabriel Lefundes, Luciano Oliveira
Categories: cs.CV
\\
  Estimation of 3D gaze is highly relevant to multiple fields, including but
not limited to interactive systems, specialized human-computer interfaces, and
behavioral research. Although recently deep learning methods have boosted the
accuracy of appearance-based gaze estimation, there is still room for
improvement in the network architectures for this particular task. Therefore we
propose here a novel network architecture grounded on self-attention augmented
convolutions to improve the quality of the learned features during the training
of a shallower residual network. The rationale is that self-attention mechanism
can help outperform deeper architectures by learning dependencies between
distant regions in full-face images. This mechanism can also create better and
more spatially-aware feature representations derived from the face and eye
images before gaze regression. We dubbed our framework ARes-gaze, which
explores our Attention-augmented ResNet (ARes-14) as twin convolutional
backbones. In our experiments, results showed a decrease of the average angular
error by 2.38% when compared to state-of-the-art methods on the MPIIFaceGaze
data set, and a second-place on the EyeDiap data set. It is noteworthy that our
proposed framework was the only one to reach high accuracy simultaneously on
both data sets.
\\ ( https://arxiv.org/abs/2008.11055 ,  5920kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11073
Date: Tue, 25 Aug 2020 14:44:58 GMT   (3542kb,D)

Title: Mask-guided sample selection for Semi-Supervised Instance Segmentation
Authors: Miriam Bellver, Amaia Salvador, Jordi Torres, Xavier Giro-i-Nieto
Categories: cs.CV
Comments: Preprint submitted to Multimedia Tools and Applications
\\
  Image segmentation methods are usually trained with pixel-level annotations,
which require significant human effort to collect. The most common solution to
address this constraint is to implement weakly-supervised pipelines trained
with lower forms of supervision, such as bounding boxes or scribbles. Another
option are semi-supervised methods, which leverage a large amount of unlabeled
data and a limited number of strongly-labeled samples. In this second setup,
samples to be strongly-annotated can be selected randomly or with an active
learning mechanism that chooses the ones that will maximize the model
performance. In this work, we propose a sample selection approach to decide
which samples to annotate for semi-supervised instance segmentation. Our method
consists in first predicting pseudo-masks for the unlabeled pool of samples,
together with a score predicting the quality of the mask. This score is an
estimate of the Intersection Over Union (IoU) of the segment with the ground
truth mask. We study which samples are better to annotate given the quality
score, and show how our approach outperforms a random selection, leading to
improved performance for semi-supervised instance segmentation with low
annotation budgets.
\\ ( https://arxiv.org/abs/2008.11073 ,  3542kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11083
Date: Tue, 25 Aug 2020 14:55:53 GMT   (166kb,D)

Title: Using the discrete radon transformation for grayscale image moments
Authors: William Diggin and Michael Diggin
Categories: cs.CV
\\
  Image moments are weighted sums over pixel values in a given image and are
used in object detection and localization. Raw image moments are derived
directly from the image and are fundamental in deriving moment invariants
quantities. The current general algorithm for raw image moments is
computationally expensive and the number of multiplications needed scales with
the number of pixels in the image. For an image of size (N,M), it has O(NM)
multiplications. In this paper we outline an algorithm using the Discrete Radon
Transformation for computing the raw image moments of a grayscale image. It
reduces two dimensional moment calculations to linear combinations of one
dimensional moment calculations. We show that the number of multiplications
needed scales as O(N + M), making it faster then the most widely used algorithm
of raw image moments.
\\ ( https://arxiv.org/abs/2008.11083 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11098
Date: Tue, 25 Aug 2020 15:24:02 GMT   (5378kb,D)

Title: Improving Deep Stereo Network Generalization with Geometric Priors
Authors: Jialiang Wang, Varun Jampani, Deqing Sun, Charles Loop, Stan
  Birchfield, Jan Kautz
Categories: cs.CV
\\
  End-to-end deep learning methods have advanced stereo vision in recent years
and obtained excellent results when the training and test data are similar.
However, large datasets of diverse real-world scenes with dense ground truth
are difficult to obtain and currently not publicly available to the research
community. As a result, many algorithms rely on small real-world datasets of
similar scenes or synthetic datasets, but end-to-end algorithms trained on such
datasets often generalize poorly to different images that arise in real-world
applications. As a step towards addressing this problem, we propose to
incorporate prior knowledge of scene geometry into an end-to-end stereo network
to help networks generalize better. For a given network, we explicitly add a
gradient-domain smoothness prior and occlusion reasoning into the network
training, while the architecture remains unchanged during inference.
Experimentally, we show consistent improvements if we train on synthetic
datasets and test on the Middlebury (real images) dataset. Noticeably, we
improve PSM-Net accuracy on Middlebury from 5.37 MAE to 3.21 MAE without
sacrificing speed.
\\ ( https://arxiv.org/abs/2008.11098 ,  5378kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11104
Date: Tue, 25 Aug 2020 15:33:59 GMT   (10992kb,D)

Title: Masked Face Recognition for Secure Authentication
Authors: Aqeel Anwar, Arijit Raychowdhury
Categories: cs.CV cs.LG eess.IV
Comments: 8 pages, 5 figures
\\
  With the recent world-wide COVID-19 pandemic, using face masks have become an
important part of our lives. People are encouraged to cover their faces when in
public area to avoid the spread of infection. The use of these face masks has
raised a serious question on the accuracy of the facial recognition system used
for tracking school/office attendance and to unlock phones. Many organizations
use facial recognition as a means of authentication and have already developed
the necessary datasets in-house to be able to deploy such a system.
Unfortunately, masked faces make it difficult to be detected and recognized,
thereby threatening to make the in-house datasets invalid and making such
facial recognition systems inoperable. This paper addresses a methodology to
use the current facial datasets by augmenting it with tools that enable masked
faces to be recognized with low false-positive rates and high overall accuracy,
without requiring the user dataset to be recreated by taking new pictures for
authentication. We present an open-source tool, MaskTheFace to mask faces
effectively creating a large dataset of masked faces. The dataset generated
with this tool is then used towards training an effective facial recognition
system with target accuracy for masked faces. We report an increase of 38% in
the true positive rate for the Facenet system. We also test the accuracy of
re-trained system on a custom real-world dataset MFR2 and report similar
accuracy.
\\ ( https://arxiv.org/abs/2008.11104 ,  10992kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11149
Date: Tue, 25 Aug 2020 16:30:01 GMT   (413kb,D)

Title: Spatiotemporal Action Recognition in Restaurant Videos
Authors: Akshat Gupta, Milan Desai, Wusheng Liang, Magesh Kannan
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  Spatiotemporal action recognition is the task of locating and classifying
actions in videos. Our project applies this task to analyzing video footage of
restaurant workers preparing food, for which potential applications include
automated checkout and inventory management. Such videos are quite different
from the standardized datasets that researchers are used to, as they involve
small objects, rapid actions, and notoriously unbalanced data classes. We
explore two approaches. The first approach involves the familiar object
detector You Only Look Once, and another applying a recently proposed analogue
for action recognition, You Only Watch Once. In the first, we design and
implement a novel, recurrent modification of YOLO using convolutional LSTMs and
explore the various subtleties in the training of such a network. In the
second, we study the ability of YOWOs three dimensional convolutions to capture
the spatiotemporal features of our unique dataset
\\ ( https://arxiv.org/abs/2008.11149 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11151
Date: Tue, 25 Aug 2020 16:32:33 GMT   (4026kb,D)

Title: FastSal: a Computationally Efficient Network for Visual Saliency
  Prediction
Authors: Feiyan Hu and Kevin McGuinness
Categories: cs.CV cs.LG
\\
  This paper focuses on the problem of visual saliency prediction, predicting
regions of an image that tend to attract human visual attention, under a
constrained computational budget. We modify and test various recent efficient
convolutional neural network architectures like EfficientNet and MobileNetV2
and compare them with existing state-of-the-art saliency models such as SalGAN
and DeepGaze II both in terms of standard accuracy metrics like AUC and NSS,
and in terms of the computational complexity and model size. We find that
MobileNetV2 makes an excellent backbone for a visual saliency model and can be
effective even without a complex decoder. We also show that knowledge transfer
from a more computationally expensive model like DeepGaze II can be achieved
via pseudo-labelling an unlabelled dataset, and that this approach gives result
on-par with many state-of-the-art algorithms with a fraction of the
computational cost and model size. Source code is available at
https://github.com/feiyanhu/FastSal.
\\ ( https://arxiv.org/abs/2008.11151 ,  4026kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11170
Date: Tue, 25 Aug 2020 17:04:39 GMT   (1464kb,D)

Title: Boundary Uncertainty in a Single-Stage Temporal Action Localization
  Network
Authors: Ting-Ting Xie, Christos Tzelepis, Ioannis Patras
Categories: cs.CV
Comments: Tech report
\\
  In this paper, we address the problem of temporal action localization with a
single stage neural network. In the proposed architecture we model the boundary
predictions as uni-variate Gaussian distributions in order to model their
uncertainties, which is the first in this area to the best of our knowledge. We
use two uncertainty-aware boundary regression losses: first, the
Kullback-Leibler divergence between the ground truth location of the boundary
and the Gaussian modeling the prediction of the boundary and second, the
expectation of the $\ell_1$ loss under the same Gaussian. We show that with
both uncertainty modeling approaches improve the detection performance by more
than $1.5\%$ in mAP@tIoU=0.5 and that the proposed simple one-stage network
performs closely to more complex one and two stage networks.
\\ ( https://arxiv.org/abs/2008.11170 ,  1464kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11185
Date: Tue, 25 Aug 2020 17:38:40 GMT   (1110kb,D)

Title: Bias-Awareness for Zero-Shot Learning the Seen and Unseen
Authors: William Thong and Cees G.M. Snoek
Categories: cs.CV
Comments: Accepted at British Machine Vision Conference (BMVC) 2020
\\
  Generalized zero-shot learning recognizes inputs from both seen and unseen
classes. Yet, existing methods tend to be biased towards the classes seen
during training. In this paper, we strive to mitigate this bias. We propose a
bias-aware learner to map inputs to a semantic embedding space for generalized
zero-shot learning. During training, the model learns to regress to real-valued
class prototypes in the embedding space with temperature scaling, while a
margin-based bidirectional entropy term regularizes seen and unseen
probabilities. Relying on a real-valued semantic embedding space provides a
versatile approach, as the model can operate on different types of semantic
information for both seen and unseen classes. Experiments are carried out on
four benchmarks for generalized zero-shot learning and demonstrate the benefits
of the proposed bias-aware classifier, both as a stand-alone method or in
combination with generated features.
\\ ( https://arxiv.org/abs/2008.11185 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11200
Date: Tue, 25 Aug 2020 17:57:55 GMT   (12783kb,D)

Title: GRAB: A Dataset of Whole-Body Human Grasping of Objects
Authors: Omid Taheri, Nima Ghorbani, Michael J. Black, and Dimitrios Tzionas
Categories: cs.CV
Comments: ECCV 2020
DOI: 10.1007/978-3-030-58548-8_34
\\
  Training computers to understand, model, and synthesize human grasping
requires a rich dataset containing complex 3D object shapes, detailed contact
information, hand pose and shape, and the 3D body motion over time. While
"grasping" is commonly thought of as a single hand stably lifting an object, we
capture the motion of the entire body and adopt the generalized notion of
"whole-body grasps". Thus, we collect a new dataset, called GRAB (GRasping
Actions with Bodies), of whole-body grasps, containing full 3D shape and pose
sequences of 10 subjects interacting with 51 everyday objects of varying shape
and size. Given MoCap markers, we fit the full 3D body shape and pose,
including the articulated face and hands, as well as the 3D object pose. This
gives detailed 3D meshes over time, from which we compute contact between the
body and object. This is a unique dataset, that goes well beyond existing ones
for modeling and understanding how humans grasp and manipulate objects, how
their full body is involved, and how interaction varies with the task. We
illustrate the practical value of GRAB with an example application; we train
GrabNet, a conditional generative network, to predict 3D hand grasps for unseen
3D object shapes. The dataset and code are available for research purposes at
https://grab.is.tue.mpg.de.
\\ ( https://arxiv.org/abs/2008.11200 ,  12783kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11201
Date: Tue, 25 Aug 2020 17:58:17 GMT   (805kb,D)

Title: Deep Active Learning in Remote Sensing for data efficient Change
  Detection
Authors: V\'it R\r{u}\v{z}i\v{c}ka, Stefano D'Aronco, Jan Dirk Wegner, Konrad
  Schindler
Categories: cs.CV
Comments: 10 pages, 5 figures, ECML/PKDD Workshop on Machine Learning for Earth
  Observation, 2020
\\
  We investigate active learning in the context of deep neural network models
for change detection and map updating. Active learning is a natural choice for
a number of remote sensing tasks, including the detection of local surface
changes: changes are on the one hand rare and on the other hand their
appearance is varied and diffuse, making it hard to collect a representative
training set in advance. In the active learning setting, one starts from a
minimal set of training examples and progressively chooses informative samples
that are annotated by a user and added to the training set. Hence, a core
component of an active learning system is a mechanism to estimate model
uncertainty, which is then used to pick uncertain, informative samples. We
study different mechanisms to capture and quantify this uncertainty when
working with deep networks, based on the variance or entropy across explicit or
implicit model ensembles. We show that active learning successfully finds
highly informative samples and automatically balances the training
distribution, and reaches the same performance as a model supervised with a
large, pre-annotated training set, with $\approx$99% fewer annotated samples.
\\ ( https://arxiv.org/abs/2008.11201 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11203
Date: Tue, 25 Aug 2020 17:59:53 GMT   (9002kb,D)

Title: Learning to Learn in a Semi-Supervised Fashion
Authors: Yun-Chun Chen, Chao-Te Chou, Yu-Chiang Frank Wang
Categories: cs.CV cs.LG
Comments: ECCV 2020
\\
  To address semi-supervised learning from both labeled and unlabeled data, we
present a novel meta-learning scheme. We particularly consider that labeled and
unlabeled data share disjoint ground truth label sets, which can be seen tasks
like in person re-identification or image retrieval. Our learning scheme
exploits the idea of leveraging information from labeled to unlabeled data.
Instead of fitting the associated class-wise similarity scores as most
meta-learning algorithms do, we propose to derive semantics-oriented similarity
representations from labeled data, and transfer such representation to
unlabeled ones. Thus, our strategy can be viewed as a self-supervised learning
scheme, which can be applied to fully supervised learning tasks for improved
performance. Our experiments on various tasks and settings confirm the
effectiveness of our proposed approach and its superiority over the
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2008.11203 ,  9002kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2008.10631 (*cross-listing*)
Date: Mon, 24 Aug 2020 18:04:50 GMT   (10213kb,D)

Title: OpenBot: Turning Smartphones into Robots
Authors: Matthias M\"uller, Vladlen Koltun
Categories: cs.RO cs.CV cs.LG
\\
  Current robots are either expensive or make significant compromises on
sensory richness, computational power, and communication capabilities. We
propose to leverage smartphones to equip robots with extensive sensor suites,
powerful computational abilities, state-of-the-art communication channels, and
access to a thriving software ecosystem. We design a small electric vehicle
that costs $50 and serves as a robot body for standard Android smartphones. We
develop a software stack that allows smartphones to use this body for mobile
operation and demonstrate that the system is sufficiently powerful to support
advanced robotics workloads such as person following and real-time autonomous
navigation in unstructured environments. Controlled experiments demonstrate
that the presented approach is robust across different smartphones and robot
bodies. A video of our work is available at
https://www.youtube.com/watch?v=qc8hFLyWDOM
\\ ( https://arxiv.org/abs/2008.10631 ,  10213kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10652 (*cross-listing*)
Date: Mon, 24 Aug 2020 18:50:30 GMT   (313kb,D)

Title: Robust Pancreatic Ductal Adenocarcinoma Segmentation with
  Multi-Institutional Multi-Phase Partially-Annotated CT Scans
Authors: Ling Zhang, Yu Shi, Jiawen Yao, Yun Bian, Kai Cao, Dakai Jin, Jing
  Xiao, Le Lu
Categories: eess.IV cs.CV
Comments: 10 pages, 2 figures; MICCAI 2020
\\
  Accurate and automated tumor segmentation is highly desired since it has the
great potential to increase the efficiency and reproducibility of computing
more complete tumor measurements and imaging biomarkers, comparing to (often
partial) human measurements. This is probably the only viable means to enable
the large-scale clinical oncology patient studies that utilize medical imaging.
Deep learning approaches have shown robust segmentation performances for
certain types of tumors, e.g., brain tumors in MRI imaging, when a training
dataset with plenty of pixel-level fully-annotated tumor images is available.
However, more than often, we are facing the challenge that only (very) limited
annotations are feasible to acquire, especially for hard tumors. Pancreatic
ductal adenocarcinoma (PDAC) segmentation is one of the most challenging tumor
segmentation tasks, yet critically important for clinical needs. Previous work
on PDAC segmentation is limited to the moderate amounts of annotated patient
images (n<300) from venous or venous+arterial phase CT scans. Based on a new
self-learning framework, we propose to train the PDAC segmentation model using
a much larger quantity of patients (n~=1,000), with a mix of annotated and
un-annotated venous or multi-phase CT images. Pseudo annotations are generated
by combining two teacher models with different PDAC segmentation specialties on
unannotated images, and can be further refined by a teaching assistant model
that identifies associated vessels around the pancreas. A student model is
trained on both manual and pseudo annotated multi-phase images. Experiment
results show that our proposed method provides an absolute improvement of 6.3%
Dice score over the strong baseline of nnUNet trained on annotated images,
achieving the performance (Dice = 0.71) similar to the inter-observer
variability between radiologists.
\\ ( https://arxiv.org/abs/2008.10652 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10710 (*cross-listing*)
Date: Mon, 24 Aug 2020 21:14:13 GMT   (109894kb,D)

Title: Exploit Camera Raw Data for Video Super-Resolution via Hidden Markov
  Model Inference
Authors: Xiaohong Liu, Kangdi Shi, Zhe Wang, Jun Chen
Categories: eess.IV cs.CV
Comments: 13 pages, 14 figures, submitted to IEEE Transactions on Image
  Processing
\\
  To the best of our knowledge, the existing deep-learning-based Video
Super-Resolution (VSR) methods exclusively make use of videos produced by the
Image Signal Processor (ISP) of the camera system as inputs. Such methods are
1) inherently suboptimal due to information loss incurred by non-invertible
operations in ISP, and 2) inconsistent with the real imaging pipeline where VSR
in fact serves as a pre-processing unit of ISP. To address this issue, we
propose a new VSR method that can directly exploit camera sensor data,
accompanied by a carefully built Raw Video Dataset (RawVD) for training,
validation, and testing. This method consists of a Successive Deep Inference
(SDI) module and a reconstruction module, among others. The SDI module is
designed according to the architectural principle suggested by a canonical
decomposition result for Hidden Markov Model (HMM) inference; it estimates the
target high-resolution frame by repeatedly performing pairwise feature fusion
using deformable convolutions. The reconstruction module, built with
elaborately designed Attention-based Residual Dense Blocks (ARDBs), serves the
purpose of 1) refining the fused feature and 2) learning the color information
needed to generate a spatial-specific transformation for accurate color
correction. Extensive experiments demonstrate that owing to the informativeness
of the camera raw data, the effectiveness of the network architecture, and the
separation of super-resolution and color correction processes, the proposed
method achieves superior VSR results compared to the state-of-the-art and can
be adapted to any specific camera-ISP.
\\ ( https://arxiv.org/abs/2008.10710 ,  109894kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10766 (*cross-listing*)
Date: Tue, 25 Aug 2020 00:44:09 GMT   (8468kb,D)

Title: Channel-Directed Gradients for Optimization of Convolutional Neural
  Networks
Authors: Dong Lao, Peihao Zhu, Peter Wonka, Ganesh Sundaramoorthi
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  We introduce optimization methods for convolutional neural networks that can
be used to improve existing gradient-based optimization in terms of
generalization error. The method requires only simple processing of existing
stochastic gradients, can be used in conjunction with any optimizer, and has
only a linear overhead (in the number of parameters) compared to computation of
the stochastic gradient. The method works by computing the gradient of the loss
function with respect to output-channel directed re-weighted L2 or Sobolev
metrics, which has the effect of smoothing components of the gradient across a
certain direction of the parameter tensor. We show that defining the gradients
along the output channel direction leads to a performance boost, while other
directions can be detrimental. We present the continuum theory of such
gradients, its discretization, and application to deep networks. Experiments on
benchmark datasets, several networks and baseline optimizers show that
optimizers can be improved in generalization error by simply computing the
stochastic gradient with respect to output-channel directed metrics.
\\ ( https://arxiv.org/abs/2008.10766 ,  8468kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10796 (*cross-listing*)
Date: Tue, 25 Aug 2020 03:30:53 GMT   (10173kb,D)

Title: Variational Image Restoration Network
Authors: Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang, Deyu Meng
Categories: eess.IV cs.CV
ACM-class: I.4.4
\\
  Deep neural networks (DNNs) have achieved significant success in image
restoration tasks by directly learning a powerful non-linear mapping from
corrupted images to their latent clean ones. However, there still exist two
major limitations for these deep learning (DL)-based methods. Firstly, the
noises contained in real corrupted images are very complex, usually neglected
and largely under-estimated in most current methods. Secondly, existing DL
methods are mostly trained on one pre-assumed degradation process for all of
the training image pairs, such as the widely used bicubic downsampling
assumption in the image super-resolution task, inevitably leading to poor
generalization performance when the true degradation does not match with such
assumed one. To address these issues, we propose a unified generative model for
the image restoration, which elaborately configures the degradation process
from the latent clean image to the observed corrupted one. Specifically,
different from most of current methods, the pixel-wisely non-i.i.d. Gaussian
distribution, being with more flexibility, is adopted in our method to fit the
complex real noises. Furthermore, the method is built on the general image
degradation process, making it capable of adapting diverse degradations under
one single model. Besides, we design a variational inference algorithm to learn
all parameters involved in the proposed model with explicit form of objective
loss. Specifically, beyond traditional variational methodology, two DNNs are
employed to parameterize the posteriori distributions, one to infer the
distribution of the latent clean image, and another to infer the distribution
of the image noise. Extensive experiments demonstrate the superiority of the
proposed method on three classical image restoration tasks, including image
denoising, image super-resolution and JPEG image deblocking.
\\ ( https://arxiv.org/abs/2008.10796 ,  10173kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10805 (*cross-listing*)
Date: Tue, 25 Aug 2020 04:08:10 GMT   (1636kb,D)

Title: New Directions in Distributed Deep Learning: Bringing the Network at
  Forefront of IoT Design
Authors: Kartikeya Bhardwaj, Wei Chen, Radu Marculescu
Categories: stat.ML cs.CV cs.LG eess.SP
Comments: This preprint is for personal use only. The official article will
  appear in proceedings of Design Automation Conference (DAC), 2020. This work
  was presented at the DAC 2020 special session on Edge-to-Cloud Neural
  Networks for Machine Learning Applications in Future IoT Systems
\\
  In this paper, we first highlight three major challenges to large-scale
adoption of deep learning at the edge: (i) Hardware-constrained IoT devices,
(ii) Data security and privacy in the IoT era, and (iii) Lack of network-aware
deep learning algorithms for distributed inference across multiple IoT devices.
We then provide a unified view targeting three research directions that
naturally emerge from the above challenges: (1) Federated learning for training
deep networks, (2) Data-independent deployment of learning algorithms, and (3)
Communication-aware distributed inference. We believe that the above research
directions need a network-centric approach to enable the edge intelligence and,
therefore, fully exploit the true potential of IoT.
\\ ( https://arxiv.org/abs/2008.10805 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11010 (*cross-listing*)
Date: Tue, 25 Aug 2020 13:48:40 GMT   (573kb,D)

Title: Efficient Blind-Spot Neural Network Architecture for Image Denoising
Authors: David Honz\'atko, Siavash A. Bigdeli, Engin T\"uretken, L. Andrea
  Dunbar
Categories: eess.IV cs.CV cs.LG
ACM-class: I.4.3; I.2.10
Journal-ref: 2020 7th Swiss Conference on Data Science (SDS), Luzern,
  Switzerland, 2020, pp. 59-60
DOI: 10.1109/SDS49233.2020.00022
\\
  Image denoising is an essential tool in computational photography. Standard
denoising techniques, which use deep neural networks at their core, require
pairs of clean and noisy images for its training. If we do not possess the
clean samples, we can use blind-spot neural network architectures, which
estimate the pixel value based on the neighbouring pixels only. These networks
thus allow training on noisy images directly, as they by-design avoid trivial
solutions. Nowadays, the blind-spot is mostly achieved using shifted
convolutions or serialization. We propose a novel fully convolutional network
architecture that uses dilations to achieve the blind-spot property. Our
network improves the performance over the prior work and achieves
state-of-the-art results on established datasets.
\\ ( https://arxiv.org/abs/2008.11010 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11062 (*cross-listing*)
Date: Tue, 25 Aug 2020 14:39:42 GMT   (23122kb,D)

Title: GAN Slimming: All-in-One GAN Compression by A Unified Optimization
  Framework
Authors: Haotao Wang, Shupeng Gui, Haichuan Yang, Ji Liu, Zhangyang Wang
Categories: cs.LG cs.CV eess.IV stat.ML
Comments: ECCV 2020 spotlight
\\
  Generative adversarial networks (GANs) have gained increasing popularity in
various computer vision applications, and recently start to be deployed to
resource-constrained mobile devices. Similar to other deep models,
state-of-the-art GANs suffer from high parameter complexities. That has
recently motivated the exploration of compressing GANs (usually generators).
Compared to the vast literature and prevailing success in compressing deep
classifiers, the study of GAN compression remains in its infancy, so far
leveraging individual compression techniques instead of more sophisticated
combinations. We observe that due to the notorious instability of training
GANs, heuristically stacking different compression techniques will result in
unsatisfactory results. To this end, we propose the first unified optimization
framework combining multiple compression means for GAN compression, dubbed GAN
Slimming (GS). GS seamlessly integrates three mainstream compression
techniques: model distillation, channel pruning and quantization, together with
the GAN minimax objective, into one unified optimization form, that can be
efficiently optimized from end to end. Without bells and whistles, GS largely
outperforms existing options in compressing image-to-image translation GANs.
Specifically, we apply GS to compress CartoonGAN, a state-of-the-art style
transfer network, by up to 47 times, with minimal visual quality degradation.
Codes and pre-trained models can be found at
https://github.com/TAMU-VITA/GAN-Slimming.
\\ ( https://arxiv.org/abs/2008.11062 ,  23122kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11109 (*cross-listing*)
Date: Tue, 25 Aug 2020 15:37:57 GMT   (5428kb,D)

Title: Measure Anatomical Thickness from Cardiac MRI with Deep Neural Networks
Authors: Qiaoying Huang, Eric Z. Chen, Hanchao Yu, Yimo Guo, Terrence Chen,
  Dimitris Metaxas, Shanhui Sun
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by STACOM 2020
\\
  Accurate estimation of shape thickness from medical images is crucial in
clinical applications. For example, the thickness of myocardium is one of the
key to cardiac disease diagnosis. While mathematical models are available to
obtain accurate dense thickness estimation, they suffer from heavy
computational overhead due to iterative solvers. To this end, we propose novel
methods for dense thickness estimation, including a fast solver that estimates
thickness from binary annular shapes and an end-to-end network that estimates
thickness directly from raw cardiac images.We test the proposed models on three
cardiac datasets and one synthetic dataset, achieving impressive results and
generalizability on all. Thickness estimation is performed without iterative
solvers or manual correction, which is 100 times faster than the mathematical
model. We also analyze thickness patterns on different cardiac pathologies with
a standard clinical model and the results demonstrate the potential clinical
value of our method for thickness based cardiac disease diagnosis.
\\ ( https://arxiv.org/abs/2008.11109 ,  5428kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10709 (*cross-listing*)
Date: Mon, 24 Aug 2020 21:07:39 GMT   (24kb)

Title: Algorithms and Lower Bounds for the Worker-Task Assignment Problem
Authors: Aaron Berger, William Kuszmaul, Adam Polak, Jonathan Tidor, Nicole
  Wein
Categories: cs.DS cs.DM
Comments: 19 pages
\\
  We study the problem of assigning workers to tasks where each task has demand
for a particular number of workers, and the demands are dynamically changing
over time. Specifically, a worker-task assignment function $\phi$ takes a
multiset of $w$ tasks $T \subseteq [t]$ and produces an assignment $\phi(T)$
from the workers $1, 2, \ldots, w$ to the tasks $T$. The assignment function
$\phi$ is said to have switching cost at most $k$ if, for all task multisets
$T$, changing the contents of $T$ by one task changes $\phi(T)$ by at most $k$
worker assignments. The goal of the worker-task assignment problem is to
produce an assignment function $\phi$ with the minimum possible switching cost.
  Prior work on this problem (SSS'17, ICALP'20) observed a simple assignment
function $\phi$ with switching cost $\min(w, t - 1)$, but there has been no
success in constructing $\phi$ with sublinear switching cost. We construct the
first assignment function $\phi$ with sublinear, and in fact polylogarithmic,
switching cost. We give a probabilistic construction for $\phi$ that achieves
switching cost $O(\log w \log (wt))$ and an explicit construction that achieves
switching cost $\operatorname{polylog} (wt)$.
  From the lower bounds side, prior work has used involved arguments to prove
constant lower bounds on switching cost, but no super-constant lower bounds are
known. We prove the first super-constant lower bound on switching cost. In
particular, we show that for any value of $w$ there exists a value of $t$ for
which the optimal switching cost is $w$. That is, when $w \ll t$, the trivial
bound on switching cost is optimal.
  We also consider an application of the worker-task assignment problem to a
metric embeddings problem. In particular, we use our results to give the first
low-distortion embedding from sparse binary vectors into low-dimensional
Hamming space.
\\ ( https://arxiv.org/abs/2008.10709 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2008.11103 (*cross-listing*)
Date: Sat, 22 Aug 2020 10:00:20 GMT   (48kb)

Title: On Cycles of Generalized Collatz Sequences
Authors: Anant Gupta
Categories: math.NT cs.DM
Comments: 23 pages, 10 figures, 6 tables
MSC-class: 11D45
\\
  We explore the cycles and convergence of Generalized Collatz Sequence, where
$3n+1$ in original collatz function is replaced with $3n+k$. We present a
generating function for cycles of GCS and show a particular inheritance
structure of cycles across such sequences. The cycle structure is invariant
across such inheritance and appears more fundamental than cycle elements. A
consequence is that there can be arbitrarily large number of cycles in some
sequences. GCS can also be seen as an integer space partition function and such
partitions along with collatz graphs are inherited across sequences. An
interesting connection between cycles of GCS and certain exponential
Diophantine equations is also presented.
\\ ( https://arxiv.org/abs/2008.11103 ,  48kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1904.08746
replaced with revised version Tue, 25 Aug 2020 09:41:46 GMT   (138kb)

Title: Advancing Through Terrains
Authors: Vincent Froese and Malte Renken
Categories: cs.CG cs.DM
\\ ( https://arxiv.org/abs/1904.08746 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10373
replaced with revised version Tue, 25 Aug 2020 14:32:47 GMT   (3343kb,D)

Title: An Integer-Linear Program for Bend-Minimization in Ortho-Radial Drawings
Authors: Benjamin Niedermann, Ignaz Rutter
Categories: cs.CG
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\ ( https://arxiv.org/abs/2008.10373 ,  3343kb)
------------------------------------------------------------------------------
\\
arXiv:1811.09567
replaced with revised version Tue, 25 Aug 2020 07:39:24 GMT   (67066kb,D)

Title: How does Lipschitz Regularization Influence GAN Training?
Authors: Yipeng Qin, Niloy Mitra, Peter Wonka
Categories: cs.CV cs.LG
Comments: Accepted at ECCV 2020
\\ ( https://arxiv.org/abs/1811.09567 ,  67066kb)
------------------------------------------------------------------------------
\\
arXiv:1906.01166
replaced with revised version Tue, 25 Aug 2020 13:22:34 GMT   (6152kb,D)

Title: Interpretable Neural Network Decoupling
Authors: Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang, Chenqian Yan,
  Yongjian Wu, Feiyue Huang, Ling Shao
Categories: cs.CV
Comments: 20 pages, 12 figures
\\ ( https://arxiv.org/abs/1906.01166 ,  6152kb)
------------------------------------------------------------------------------
\\
arXiv:1906.02238
replaced with revised version Mon, 24 Aug 2020 23:38:03 GMT   (10198kb,D)

Title: Adaptation Across Extreme Variations using Unlabeled Domain Bridges
Authors: Shuyang Dai, Kihyuk Sohn, Yi-Hsuan Tsai, Lawrence Carin, Manmohan
  Chandraker
Categories: cs.CV
\\ ( https://arxiv.org/abs/1906.02238 ,  10198kb)
------------------------------------------------------------------------------
\\
arXiv:1907.01341
replaced with revised version Tue, 25 Aug 2020 09:37:24 GMT   (12209kb,D)

Title: Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot
  Cross-dataset Transfer
Authors: Ren\'e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler,
  Vladlen Koltun
Categories: cs.CV
Comments: To appear in TPAMI (accepted August 2020)
\\ ( https://arxiv.org/abs/1907.01341 ,  12209kb)
------------------------------------------------------------------------------
\\
arXiv:1908.08918
replaced with revised version Tue, 25 Aug 2020 17:54:25 GMT   (5952kb,D)

Title: DefSLAM: Tracking and Mapping of Deforming Scenes from Monocular
  Sequences
Authors: Jose Lamarca, Shaifali Parashar, Adrien Bartoli and J.M.M. Montiel
Categories: cs.CV eess.IV
Comments: Experiments results: https://www.youtube.com/watch?v=6mmhD2_t6Gs ;
  More Results:
  https://www.youtube.com/playlist?list=PLKBuKNhAV30SlKGJ9eaMlAExdWRypUy-K
\\ ( https://arxiv.org/abs/1908.08918 ,  5952kb)
------------------------------------------------------------------------------
\\
arXiv:1910.00324
replaced with revised version Mon, 24 Aug 2020 21:33:51 GMT   (2790kb,D)

Title: Graph convolutional networks for learning with few clean and many noisy
  labels
Authors: Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum, Cordelia
  Schmid
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/1910.00324 ,  2790kb)
------------------------------------------------------------------------------
\\
arXiv:1911.09943
replaced with revised version Tue, 25 Aug 2020 04:47:37 GMT   (16597kb,D)

Title: DLGAN: Disentangling Label-Specific Fine-Grained Features for Image
  Manipulation
Authors: Guanqi Zhan, Yihao Zhao, Bingchan Zhao, Haoqi Yuan, Baoquan Chen, Hao
  Dong
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/1911.09943 ,  16597kb)
------------------------------------------------------------------------------
\\
arXiv:1912.11853
replaced with revised version Tue, 25 Aug 2020 09:08:08 GMT   (423kb,D)

Title: Domain Adaptation Regularization for Spectral Pruning
Authors: Laurent Dillard, Yosuke Shinya, Taiji Suzuki
Categories: cs.CV cs.LG stat.ML
Comments: BMVC 2020
\\ ( https://arxiv.org/abs/1912.11853 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2001.02905
replaced with revised version Tue, 25 Aug 2020 09:24:50 GMT   (19147kb,D)

Title: Fast Adaptation to Super-Resolution Networks via Meta-Learning
Authors: Seobin Park, Jinsu Yoo, Donghyeon Cho, Jiwon Kim and Tae Hyun Kim
Categories: cs.CV
\\ ( https://arxiv.org/abs/2001.02905 ,  19147kb)
------------------------------------------------------------------------------
\\
arXiv:2001.10331
replaced with revised version Tue, 25 Aug 2020 11:52:26 GMT   (3240kb,D)

Title: f-BRS: Rethinking Backpropagating Refinement for Interactive
  Segmentation
Authors: Konstantin Sofiiuk, Ilia Petrov, Olga Barinova and Anton Konushin
Categories: cs.CV
\\ ( https://arxiv.org/abs/2001.10331 ,  3240kb)
------------------------------------------------------------------------------
\\
arXiv:2003.00255
replaced with revised version Tue, 25 Aug 2020 14:35:13 GMT   (7435kb,D)

Title: Joint Face Completion and Super-resolution using Multi-scale Feature
  Relation Learning
Authors: Zhilei Liu, Yunpeng Wu, Le Li, Cuicui Zhang, Baoyuan Wu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2003.00255 ,  7435kb)
------------------------------------------------------------------------------
\\
arXiv:2003.08933
replaced with revised version Tue, 25 Aug 2020 17:59:12 GMT   (8349kb,D)

Title: DELTAS: Depth Estimation by Learning Triangulation And densification of
  Sparse points
Authors: Ayan Sinha, Zak Murez, James Bartolozzi, Vijay Badrinarayanan and
  Andrew Rabinovich
Categories: cs.CV cs.LG eess.IV
Comments: ECCV 2020
\\ ( https://arxiv.org/abs/2003.08933 ,  8349kb)
------------------------------------------------------------------------------
\\
arXiv:2003.12039
replaced with revised version Tue, 25 Aug 2020 15:49:48 GMT   (4636kb,D)

Title: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow
Authors: Zachary Teed and Jia Deng
Categories: cs.CV
Comments: fixed a formatting issue, Eq 7. no change in content
\\ ( https://arxiv.org/abs/2003.12039 ,  4636kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13042
replaced with revised version Tue, 25 Aug 2020 06:36:16 GMT   (4780kb,D)

Title: Omni-sourced Webly-supervised Learning for Video Recognition
Authors: Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, Dahua Lin
Categories: cs.CV
\\ ( https://arxiv.org/abs/2003.13042 ,  4780kb)
------------------------------------------------------------------------------
\\
arXiv:2004.00713
replaced with revised version Mon, 24 Aug 2020 21:44:38 GMT   (357kb,D)

Title: Memory-Efficient Incremental Learning Through Feature Adaptation
Authors: Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, Cordelia Schmid
Categories: cs.CV
\\ ( https://arxiv.org/abs/2004.00713 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2005.10420
replaced with revised version Tue, 25 Aug 2020 01:16:43 GMT   (6476kb,D)

Title: Towards Streaming Perception
Authors: Mengtian Li, Yu-Xiong Wang, Deva Ramanan
Categories: cs.CV
Comments: ECCV 2020 (Oral). Code and data can be found on the project page at
  https://www.cs.cmu.edu/~mengtial/proj/streaming/
\\ ( https://arxiv.org/abs/2005.10420 ,  6476kb)
------------------------------------------------------------------------------
\\
arXiv:2005.10544
replaced with revised version Tue, 25 Aug 2020 15:45:55 GMT   (88kb,D)

Title: Cross-Domain Few-Shot Learning with Meta Fine-Tuning
Authors: John Cai, Sheng Mei Shen
Categories: cs.CV cs.LG
Comments: CVPR 2020 Workshop on Visual Learning with Limited Labels (VL3)
\\ ( https://arxiv.org/abs/2005.10544 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2006.01320
replaced with revised version Tue, 25 Aug 2020 09:54:24 GMT   (30974kb,D)

Title: Two-hand Global 3D Pose Estimation Using Monocular RGB
Authors: Fanqing Lin, Connor Wilhelm, Tony Martinez
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.01320 ,  30974kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05338
replaced with revised version Tue, 25 Aug 2020 14:00:58 GMT   (1181kb,D)

Title: On Data Augmentation for GAN Training
Authors: Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen,
  Ngai-Man Cheung
Categories: cs.CV eess.IV
Comments: The updated version of "Towards Good Practices for Data Augmentation
  in GAN Training"
\\ ( https://arxiv.org/abs/2006.05338 ,  1181kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02662
replaced with revised version Tue, 25 Aug 2020 11:11:31 GMT   (6526kb,D)

Title: Toward unsupervised, multi-object discovery in large-scale image
  collections
Authors: Huy V. Vo, Patrick P\'erez and Jean Ponce
Categories: cs.CV
Comments: Accepted for publication in European Conference on Computer Vision
  (ECCV) 2020
\\ ( https://arxiv.org/abs/2007.02662 ,  6526kb)
------------------------------------------------------------------------------
\\
arXiv:2007.14560
replaced with revised version Tue, 25 Aug 2020 09:42:26 GMT   (422kb,D)

Title: Realistic Video Summarization through VISIOCITY: A New Benchmark and
  Evaluation Framework
Authors: Vishal Kaushal, Suraj Kothawade, Rishabh Iyer, Ganesh Ramakrishnan
Categories: cs.CV cs.IR cs.LG cs.MM
Comments: 19 pages, 1 figure, 14 tables
\\ ( https://arxiv.org/abs/2007.14560 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04722
replaced with revised version Tue, 25 Aug 2020 15:37:50 GMT   (13209kb,D)

Title: Robust Long-Term Object Tracking via Improved Discriminative Model
  Prediction
Authors: Seokeon Choi, Junhyun Lee, Yunsung Lee, Alexander Hauptmann
Categories: cs.CV
Comments: Accepted to ECCV 2020 Workshop
\\ ( https://arxiv.org/abs/2008.04722 ,  13209kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05826
replaced with revised version Tue, 25 Aug 2020 12:30:01 GMT   (14152kb,D)

Title: Localizing the Common Action Among a Few Videos
Authors: Pengwan Yang, Vincent Tao Hu, Pascal Mettes, Cees G. M. Snoek
Categories: cs.CV cs.LG eess.IV
Comments: ECCV 2020
\\ ( https://arxiv.org/abs/2008.05826 ,  14152kb)
------------------------------------------------------------------------------
\\
arXiv:2008.09285
replaced with revised version Tue, 25 Aug 2020 16:36:11 GMT   (5422kb,D)

Title: Occupancy Anticipation for Efficient Exploration and Navigation
Authors: Santhosh K. Ramakrishnan, Ziad Al-Halah, Kristen Grauman
Categories: cs.CV
Comments: Accepted in ECCV 2020. 19 pages, 6 figures, appendix at end
\\ ( https://arxiv.org/abs/2008.09285 ,  5422kb)
------------------------------------------------------------------------------
\\
arXiv:2008.09965
replaced with revised version Tue, 25 Aug 2020 02:36:33 GMT   (1406kb,D)

Title: Neighbourhood-Insensitive Point Cloud Normal Estimation Network
Authors: Zirui Wang, Victor Adrian Prisacariu
Categories: cs.CV
Comments: Accepted in BMVC 2020 as oral presentation. Code available at
  https://code.active.vision and project page at http://ninormal.active.vision
\\ ( https://arxiv.org/abs/2008.09965 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10329
replaced with revised version Tue, 25 Aug 2020 09:31:23 GMT   (609kb)

Title: Cascade Convolutional Neural Network for Image Super-Resolution
Authors: Jianwei Zhang and zhenxing Wang and yuhui Zheng and Guoqing Zhang
Categories: cs.CV
Comments: 12 page,5 figures
\\ ( https://arxiv.org/abs/2008.10329 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10351
replaced with revised version Tue, 25 Aug 2020 02:04:42 GMT   (7483kb,D)

Title: Model Generalization in Deep Learning Applications for Land Cover
  Mapping
Authors: Lucas Hu, Caleb Robinson, Bistra Dilkina
Categories: cs.CV cs.LG stat.ML
Comments: 7 pages, 8 figures, 5 tables
ACM-class: I.4.6; I.2.10
\\ ( https://arxiv.org/abs/2008.10351 ,  7483kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10480
replaced with revised version Tue, 25 Aug 2020 03:44:05 GMT   (2233kb,D)

Title: 3rd Place Solution to "Google Landmark Retrieval 2020"
Authors: Ke Mei, Lei li, Jinchang Xu, Yanhua Cheng, Yugeng Lin
Categories: cs.CV
\\ ( https://arxiv.org/abs/2008.10480 ,  2233kb)
------------------------------------------------------------------------------
\\
arXiv:1808.07558
replaced with revised version Tue, 25 Aug 2020 16:34:40 GMT   (24kb)

Title: Crossing Numbers and Stress of Random Graphs
Authors: Markus Chimani and Hanna D\"oring and Matthias Reitzner
Categories: cs.DM math.CO
Comments: Extended Version (compared to conference version @ GD)
\\ ( https://arxiv.org/abs/1808.07558 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10475
replaced with revised version Tue, 25 Aug 2020 08:06:34 GMT   (504kb,D)

Title: On Mixed Linear Layouts of Series-Parallel Graphs
Authors: Patrizio Angelini and Michael A. Bekos and Philipp Kindermann and
  Tamara Mchedlidze
Categories: cs.DM
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\ ( https://arxiv.org/abs/2008.10475 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:1905.10763
replaced with revised version Mon, 24 Aug 2020 20:44:21 GMT   (12212kb,D)

Title: ENIGMA: Evolutionary Non-Isometric Geometry Matching
Authors: Michal Edelstein, Danielle Ezuz, Mirela Ben-Chen
Categories: cs.GR
\\ ( https://arxiv.org/abs/1905.10763 ,  12212kb)
------------------------------------------------------------------------------
\\
arXiv:2006.02027
replaced with revised version Tue, 25 Aug 2020 07:54:29 GMT   (3079kb,D)

Title: Sampling-Based Motion Planning on Manifold Sequences
Authors: Peter Englert, Isabel M. Rayas Fern\'andez, Ragesh K. Ramachandran,
  Gaurav S. Sukhatme
Categories: cs.RO cs.CG
\\ ( https://arxiv.org/abs/2006.02027 ,  3079kb)
------------------------------------------------------------------------------
\\
arXiv:2008.10431 (*cross-listing*)
replaced with revised version Tue, 25 Aug 2020 07:34:35 GMT   (835kb)

Title: Geometric and statistical techniques for projective mapping of chocolate
  chip cookies with a large number of consumers
Authors: David Orden, Encarnaci\'on Fern\'andez-Fern\'andez, Marino
  Tejedor-Romero, Alejandra Mart\'inez-Moraian
Categories: stat.AP cs.CG
Comments: 21 pages, 16 figures, 1 table
\\ ( https://arxiv.org/abs/2008.10431 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02267 (*cross-listing*)
replaced with revised version Tue, 25 Aug 2020 16:10:15 GMT   (0kb,I)

Title: Deep Learning based Dimple Detection for Quantitative Fractography
Authors: Ashish Sinha, KS Suresh
Categories: eess.IV cs.CV cs.LG
Comments: There is a major flaw with the paper's dataset section, and also the
  author.This needs a major revision so we can't wait for the replacement
\\ ( https://arxiv.org/abs/2007.02267 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03995
replaced with revised version Tue, 25 Aug 2020 11:12:16 GMT   (698kb,D)

Title: MCU-Net: A framework towards uncertainty representations for decision
  support system patient referrals in healthcare contexts
Authors: Nabeel Seedat
Categories: cs.LG cs.CV stat.ML
Comments: 4 pages, 4 figures,Spotlight Talk at KDD 2020 - Applied Data Science
  for Healthcare Workshop & presented at ICML 2020: Uncertainty and Robustness
  in Deep Learning
\\ ( https://arxiv.org/abs/2007.03995 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2008.04488 (*cross-listing*)
replaced with revised version Mon, 24 Aug 2020 23:12:59 GMT   (658kb)

Title: ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images
Authors: Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Weixiong Zhang, Baozhou
  Sun
Categories: eess.IV cs.CV
Comments: 16 pages, 7 figures; under review as a journal article at Medical
  Physics; abstract presented at AAPM 2020
MSC-class: 68T07(Primary), 68T45(Secondary)
\\ ( https://arxiv.org/abs/2008.04488 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2008.09381
replaced with revised version Mon, 24 Aug 2020 19:54:19 GMT   (2415kb,D)

Title: A Survey on Assessing the Generalization Envelope of Deep Neural
  Networks at Inference Time for Image Classification
Authors: Julia Lust and Alexandru Paul Condurache
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/2008.09381 ,  2415kb)
------------------------------------------------------------------------------
\\
arXiv:1809.09724 (*cross-listing*)
replaced with revised version Mon, 24 Aug 2020 18:41:04 GMT   (79kb)

Title: A big data based method for pass rates optimization in mathematics
  university lower division courses
Authors: Fernando A Morales and Cristian C Chica and Carlos A Osorio and Daniel
  Cabarcas J
Categories: econ.GN cs.DM math.OC q-fin.EC
Comments: 30 pages, 8 figures
MSC-class: 97B10, 68U01, 68R05, 65C05
\\ ( https://arxiv.org/abs/1809.09724 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:1908.05976
replaced with revised version Tue, 25 Aug 2020 07:01:16 GMT   (2488kb,D)

Title: HOTVis: Higher-Order Time-Aware Visualisation of Dynamic Graphs
Authors: Vincenzo Perri, Ingo Scholtes
Categories: cs.SI cs.GR cs.LG physics.data-an
Comments: Appears in the Proceedings of the 28th International Symposium on
  Graph Drawing and Network Visualization (GD 2020)
\\ ( https://arxiv.org/abs/1908.05976 ,  2488kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
