Delivered-To: brucelu2013@gmail.com
Received: by 2002:a67:d80b:0:0:0:0:0 with SMTP id e11csp3304158vsj;
        Tue, 14 Jul 2020 01:33:14 -0700 (PDT)
X-Google-Smtp-Source: ABdhPJy+X5WdXOef0rD900egqeVmUVeNHf60Twwm+ztQ6S9YSuSus78Dj0gR6Ry/X9FXskMDsro8
X-Received: by 2002:ac8:197b:: with SMTP id g56mr3479838qtk.105.1594715593695;
        Tue, 14 Jul 2020 01:33:13 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1594715593; cv=none;
        d=google.com; s=arc-20160816;
        b=PG+zeIIY+hVOdEV41VPrTNkIEKB5jEZC8J1N4HNxWOnwaWfCCkmgYLjWiVsB4wVOoH
         QGM31EWomZOKrtSfs5N9LXxh+6zYAJ88kPeXmeb0UpTNwk1s3lxZoOwoW3WLuMYdEpQ6
         ZVo2N+I8AqOhrqXgQaFYJ5BkFhnrxWWcKpoPtxKEFp/czHY8StQxEFlySYVG51k7wpbk
         QztP4fMXZob6OlO/3P8qOAITG2N+Soy3oYaQW/ILbmpKuMtH3l1tv8g96fVk91GXp9eM
         0ng768/WL37bDjdE5Dvl7gmae4xBVL0GADRqtDt5SPdxPUXuYDw7ELvklRFblpyfbOfg
         mW8g==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=subject:to:reply-to:from:precedence:message-id:date;
        bh=U9l/vgLsP6pN6NFamhUjziAPk02i/Qz93vftY/GVjW8=;
        b=0QS6zRPGBqdZ+5jIMI/CVAIvXcG4IHf8aB3noUVwVuzj6BAZjdDlIxX0XDGHCeMq9Z
         F4RdQvkYTKHTypElJ4EUzXcNUjnVTkH8zMjjNGRb9CbG41nT8Zft8BWpTkj/vDUvAxFh
         eoDLZgmP07dnfEP1FZtcT0VkdH14c+Vg07khpYBWpDAL+tfpf1ZTaGUmyPOGihkCRDkU
         N6NQlvohsFy4AFW4AVuVVvmIr3xbpNANYmB84hCTfjgyGSuTmCNp3HuS2vMHgS0HrUuh
         WAG5YH7p8e7IA9YzBslurSBwfmes9BkyKc9w17bBcYSnlqHkgrJGbR/XEZPNB3HUKTy6
         7f4g==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Return-Path: <no-reply@arxiv.org>
Received: from lib-arxiv-015.serverfarm.cornell.edu (mail.arxiv.org. [128.84.4.11])
        by mx.google.com with ESMTPS id n19si10305748qtl.99.2020.07.14.01.33.13
        for <brucelu2013@gmail.com>
        (version=TLS1_2 cipher=ECDHE-ECDSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 14 Jul 2020 01:33:13 -0700 (PDT)
Received-SPF: pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) client-ip=128.84.4.11;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of no-reply@arxiv.org designates 128.84.4.11 as permitted sender) smtp.mailfrom=no-reply@arxiv.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=arxiv.org
Received: from lib-arxiv-007.serverfarm.cornell.edu (lib-arxiv-007.serverfarm.cornell.edu [128.84.4.12])
	by lib-arxiv-015.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 06E8XDM8063866;
	Tue, 14 Jul 2020 04:33:13 -0400
Received: from lib-arxiv-007.serverfarm.cornell.edu (localhost [127.0.0.1])
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4) with ESMTP id 06E8XDXe002216;
	Tue, 14 Jul 2020 04:33:13 -0400
Received: (from e-prints@localhost)
	by lib-arxiv-007.serverfarm.cornell.edu (8.14.4/8.14.4/Submit) id 06E8XDB7002215;
	Tue, 14 Jul 2020 04:33:13 -0400
Date: Tue, 14 Jul 2020 04:33:13 -0400
Message-Id: <202007140833.06E8XDB7002215@lib-arxiv-007.serverfarm.cornell.edu>
X-Authentication-Warning: lib-arxiv-007.serverfarm.cornell.edu: e-prints set sender to no-reply@arXiv.org using -f
Precedence: bulk
From: no-reply@arXiv.org (send mail ONLY to cs)
Reply-To: cs@arXiv.org
To: rabble@arXiv.org (cs daily title/abstract distribution)
Subject: cs daily Subj-class mailing 15090 1
Content-Type: text/plain
MIME-Version: 1.0

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computational Geometry
Computer Vision and Pattern Recognition
Discrete Mathematics
Emerging Technologies
Graphics
 received from  Fri 10 Jul 20 18:00:00 GMT  to  Mon 13 Jul 20 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2007.06052
Date: Sun, 12 Jul 2020 18:07:48 GMT   (179kb)

Title: City Guarding with Limited Field of View
Authors: Ovidiu Daescu and Hemant Malik
Categories: cs.CG
\\
  Drones and other small unmanned aerial vehicles are starting to get
permission to fly within city limits. While video cameras are easily available
in most cities, their purpose is to guard the streets at ground level. Guarding
the aerial space of a city with video cameras is a problem that so far has been
largely ignored.
  In this paper, we present bounds on the number of cameras needed to guard the
city's aerial space (roofs, walls, and ground) using cameras with 180-degree
range of vision (the region in front of the guard), which is common for most
commercial cameras. We assume all buildings are vertical and have a rectangular
base. Each camera is placed at a top corner of a building.
  We considered the following two versions: (i) buildings have an axis-aligned
ground base and, (ii) buildings have an arbitrary orientation. We give
necessary and sufficient results for (i), necessary results for (ii), and
conjecture sufficiency results for (ii). Specifically, for (i) we prove a
sufficiency bound of 2k + k/4 +4 on the number of vertex guards, while for (ii)
we show that 3k + 1 vertex guards are sometimes necessary, where k is the total
number of buildings in the city.
\\ ( https://arxiv.org/abs/2007.06052 ,  179kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05533
Date: Fri, 10 Jul 2020 16:20:56 GMT   (3065kb,D)

Title: ISINet: An Instance-Based Approach for Surgical Instrument Segmentation
Authors: Cristina Gonz\'alez (1), Laura Bravo-S\'anchez (1), Pablo Arbelaez (1)
  ((1) Center for Research and Formation in Artificial Intelligence,
  Universidad de los Andes, Colombia)
Categories: cs.CV
Comments: Accepted at MICCAI2020
\\
  We study the task of semantic segmentation of surgical instruments in
robotic-assisted surgery scenes. We propose the Instance-based Surgical
Instrument Segmentation Network (ISINet), a method that addresses this task
from an instance-based segmentation perspective. Our method includes a temporal
consistency module that takes into account the previously overlooked and
inherent temporal information of the problem. We validate our approach on the
existing benchmark for the task, the Endoscopic Vision 2017 Robotic Instrument
Segmentation Dataset, and on the 2018 version of the dataset, whose annotations
we extended for the fine-grained version of instrument segmentation. Our
results show that ISINet significantly outperforms state-of-the-art methods,
with our baseline version duplicating the Intersection over Union (IoU) of
previous methods and our complete model triplicating the IoU.
\\ ( https://arxiv.org/abs/2007.05533 ,  3065kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05534
Date: Fri, 10 Jul 2020 16:38:48 GMT   (21025kb,D)

Title: Multi-Domain Image Completion for Random Missing Input Data
Authors: Liyue Shen, Wentao Zhu, Xiaosong Wang, Lei Xing, John M. Pauly, Baris
  Turkbey, Stephanie Anne Harmon, Thomas Hogue Sanford, Sherif Mehralivand,
  Peter Choyke, Bradford Wood, Daguang Xu
Categories: cs.CV cs.LG eess.IV
\\
  Multi-domain data are widely leveraged in vision applications taking
advantage of complementary information from different modalities, e.g., brain
tumor segmentation from multi-parametric magnetic resonance imaging (MRI).
However, due to possible data corruption and different imaging protocols, the
availability of images for each domain could vary amongst multiple data sources
in practice, which makes it challenging to build a universal model with a
varied set of input data. To tackle this problem, we propose a general approach
to complete the random missing domain(s) data in real applications.
Specifically, we develop a novel multi-domain image completion method that
utilizes a generative adversarial network (GAN) with a representational
disentanglement scheme to extract shared skeleton encoding and separate flesh
encoding across multiple domains. We further illustrate that the learned
representation in multi-domain image completion could be leveraged for
high-level tasks, e.g., segmentation, by introducing a unified framework
consisting of image completion and segmentation with a shared content encoder.
The experiments demonstrate consistent performance improvement on three
datasets for brain tumor segmentation, prostate segmentation, and facial
expression image completion respectively.
\\ ( https://arxiv.org/abs/2007.05534 ,  21025kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05573
Date: Fri, 10 Jul 2020 19:02:24 GMT   (538kb)

Title: Improved Detection of Adversarial Images Using Deep Neural Networks
Authors: Yutong Gao, Yi Pan
Categories: cs.CV cs.CR
\\
  Machine learning techniques are immensely deployed in both industry and
academy. Recent studies indicate that machine learning models used for
classification tasks are vulnerable to adversarial examples, which limits the
usage of applications in the fields with high precision requirements. We
propose a new approach called Feature Map Denoising to detect the adversarial
inputs and show the performance of detection on the mixed dataset consisting of
adversarial examples generated by different attack algorithms, which can be
used to associate with any pre-trained DNNs at a low cost. Wiener filter is
also introduced as the denoise algorithm to the defense model, which can
further improve performance. Experimental results indicate that good accuracy
of detecting the adversarial examples can be achieved through our Feature Map
Denoising algorithm.
\\ ( https://arxiv.org/abs/2007.05573 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05593
Date: Fri, 10 Jul 2020 20:11:43 GMT   (1912kb,D)

Title: Attention-guided Quality Assessment for Automated Cryo-EM Grid Screening
Authors: Hong Xu, Shireen Y. Elhabian, David E. Timm
Categories: cs.CV cs.LG
Comments: Accepted for publication in MICCAI 2020, the 23rd International
  Conference on Medical Image Computing and Computer Assisted Intervention
\\
  Cryogenic electron microscopy (cryo-EM) has become an enabling technology in
drug discovery and in understanding molecular bases of disease by producing
near-atomic resolution (less than 0.4 nm) 3D reconstructions of biological
macromolecules. The imaging process required for 3D reconstructions involves a
highly iterative and empirical screening process, starting with the acquisition
of low magnification images of the cryo-EM grids. These images are inspected
for squares that are likely to contain useful molecular signals. Potentially
useful squares within the grid are then imaged at progressively higher
magnifications, with the goal of identifying sub-micron areas within circular
holes (bounded by the squares) for imaging at high magnification. This arduous,
multi-step data acquisition process represents a bottleneck for obtaining a
high throughput data collection. Here, we focus on automating the early
decision making for the microscope operator, scoring low magnification images
of squares, and proposing the first deep learning framework, XCryoNet, for
automated cryo-EM grid screening. XCryoNet is a semi-supervised,
attention-guided deep learning approach that provides explainable scoring of
automatically extracted square images using limited amounts of labeled data.
Results show up to 8% and 37% improvements over a fully supervised and a
no-attention solution, respectively, when labeled data is scarce.
\\ ( https://arxiv.org/abs/2007.05593 ,  1912kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05608
Date: Fri, 10 Jul 2020 20:58:04 GMT   (7674kb,D)

Title: Image Captioning with Compositional Neural Module Networks
Authors: Junjiao Tian and Jean Oh
Categories: cs.CV cs.AI
Comments: International Joint Conference on Artificial Intelligence (IJCAI-19)
\\
  In image captioning where fluency is an important factor in evaluation, e.g.,
$n$-gram metrics, sequential models are commonly used; however, sequential
models generally result in overgeneralized expressions that lack the details
that may be present in an input image. Inspired by the idea of the
compositional neural module networks in the visual question answering task, we
introduce a hierarchical framework for image captioning that explores both
compositionality and sequentiality of natural language. Our algorithm learns to
compose a detail-rich sentence by selectively attending to different modules
corresponding to unique aspects of each object detected in an input image to
include specific descriptions such as counts and color. In a set of experiments
on the MSCOCO dataset, the proposed model outperforms a state-of-the art model
across multiple evaluation metrics, more importantly, presenting visually
interpretable results. Furthermore, the breakdown of subcategories $f$-scores
of the SPICE metric and human evaluation on Amazon Mechanical Turk show that
our compositional module networks effectively generate accurate and detailed
captions.
\\ ( https://arxiv.org/abs/2007.05608 ,  7674kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05615
Date: Fri, 10 Jul 2020 21:25:27 GMT   (3602kb,D)

Title: PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision
Authors: R. Austin McEver and B.S. Manjunath
Categories: cs.CV
\\
  Current state of the art methods for generating semantic segmentation rely
heavily on a large set of images that have each pixel labeled with a class of
interest label or background. Coming up with such labels, especially in domains
that require an expert to do annotations, comes at a heavy cost in time and
money. Several methods have shown that we can learn semantic segmentation from
less expensive image-level labels, but the effectiveness of point level labels,
a healthy compromise between all pixels labelled and none, still remains
largely unexplored. This paper presents a novel procedure for producing
semantic segmentation from images given some point level annotations. This
method includes point annotations in the training of a convolutional neural
network (CNN) for producing improved localization and class activation maps.
Then, we use another CNN for predicting semantic affinities in order to
propagate rough class labels and create pseudo semantic segmentation labels.
Finally, we propose training a CNN that is normally fully supervised using our
pseudo labels in place of ground truth labels, which further improves
performance and simplifies the inference process by requiring just one CNN
during inference rather than two. Our method achieves state of the art results
for point supervised semantic segmentation on the PASCAL VOC 2012 dataset
\cite{everingham2010pascal}, even outperforming state of the art methods for
stronger bounding box and squiggle supervision.
\\ ( https://arxiv.org/abs/2007.05615 ,  3602kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05617
Date: Fri, 10 Jul 2020 21:33:06 GMT   (5414kb)

Title: Quantization in Relative Gradient Angle Domain For Building Polygon
  Estimation
Authors: Yuhao Chen and Yifan Wu and Linlin Xu and Alexander Wong
Categories: cs.CV
\\
  Building footprint extraction in remote sensing data benefits many important
applications, such as urban planning and population estimation. Recently, rapid
development of Convolutional Neural Networks (CNNs) and open-sourced high
resolution satellite building image datasets have pushed the performance
boundary further for automated building extractions. However, CNN approaches
often generate imprecise building morphologies including noisy edges and round
corners. In this paper, we leverage the performance of CNNs, and propose a
module that uses prior knowledge of building corners to create angular and
concise building polygons from CNN segmentation outputs. We describe a new
transform, Relative Gradient Angle Transform (RGA Transform) that converts
object contours from time vs. space to time vs. angle. We propose a new shape
descriptor, Boundary Orientation Relation Set (BORS), to describe angle
relationship between edges in RGA domain, such as orthogonality and
parallelism. Finally, we develop an energy minimization framework that makes
use of the angle relationship in BORS to straighten edges and reconstruct sharp
corners, and the resulting corners create a polygon. Experimental results
demonstrate that our method refines CNN output from a rounded approximation to
a more clear-cut angular shape of the building footprint.
\\ ( https://arxiv.org/abs/2007.05617 ,  5414kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05643
Date: Fri, 10 Jul 2020 23:18:01 GMT   (873kb,D)

Title: Learning Local Complex Features using Randomized Neural Networks for
  Texture Analysis
Authors: Lucas C. Ribas, Leonardo F. S. Scabini, Jarbas Joaci de Mesquita S\'a
  Junior and Odemir M. Bruno
Categories: cs.CV
\\
  Texture is a visual attribute largely used in many problems of image
analysis. Currently, many methods that use learning techniques have been
proposed for texture discrimination, achieving improved performance over
previous handcrafted methods. In this paper, we present a new approach that
combines a learning technique and the Complex Network (CN) theory for texture
analysis. This method takes advantage of the representation capacity of CN to
model a texture image as a directed network and uses the topological
information of vertices to train a randomized neural network. This neural
network has a single hidden layer and uses a fast learning algorithm, which is
able to learn local CN patterns for texture characterization. Thus, we use the
weighs of the trained neural network to compose a feature vector. These feature
vectors are evaluated in a classification experiment in four widely used image
databases. Experimental results show a high classification performance of the
proposed method when compared to other methods, indicating that our approach
can be used in many image analysis problems.
\\ ( https://arxiv.org/abs/2007.05643 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05655
Date: Sat, 11 Jul 2020 00:21:05 GMT   (5890kb,D)

Title: Evolving Graphical Planner: Contextual Global Planning for
  Vision-and-Language Navigation
Authors: Zhiwei Deng, Karthik Narasimhan, Olga Russakovsky
Categories: cs.CV cs.AI cs.RO
\\
  The ability to perform effective planning is crucial for building an
instruction-following agent. When navigating through a new environment, an
agent is challenged with (1) connecting the natural language instructions with
its progressively growing knowledge of the world; and (2) performing long-range
planning and decision making in the form of effective exploration and error
correction. Current methods are still limited on both fronts despite extensive
efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a
model that performs global planning for navigation based on raw sensory input.
The model dynamically constructs a graphical representation, generalizes the
action space to allow for more flexible decision making, and performs efficient
planning on a proxy graph representation. We evaluate our model on a
challenging Vision-and-Language Navigation (VLN) task with photorealistic
images and achieve superior performance compared to previous navigation
architectures. For instance, we achieve a 53% success rate on the test split of
the Room-to-Room navigation task through pure imitation learning, outperforming
previous navigation architectures by up to 5%.
\\ ( https://arxiv.org/abs/2007.05655 ,  5890kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05661
Date: Sat, 11 Jul 2020 01:51:23 GMT   (6852kb,D)

Title: Deep Patch-based Human Segmentation
Authors: Dongbo Zhang, Zheng Fang, Xuequan Lu, Hong Qin, Antonio Robles-Kelly,
  Chao Zhang, Ying He
Categories: cs.CV cs.GR
Comments: submitted for review
\\
  3D human segmentation has seen noticeable progress in re-cent years. It,
however, still remains a challenge to date. In this paper, weintroduce a deep
patch-based method for 3D human segmentation. Wefirst extract a local surface
patch for each vertex and then parameterizeit into a 2D grid (or image). We
then embed identified shape descriptorsinto the 2D grids which are further fed
into the powerful 2D Convolu-tional Neural Network for regressing corresponding
semantic labels (e.g.,head, torso). Experiments demonstrate that our method is
effective inhuman segmentation, and achieves state-of-the-art accuracy.
\\ ( https://arxiv.org/abs/2007.05661 ,  6852kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05667
Date: Sat, 11 Jul 2020 02:51:40 GMT   (242kb,D)

Title: To filter prune, or to layer prune, that is the question
Authors: Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang and
  Nilanjan Ray
Categories: cs.CV
\\
  Recent advances in pruning of neural networks have made it possible to remove
a large number of filters or weights without any perceptible drop in accuracy.
The number of parameters and that of FLOPs are usually the reported metrics to
measure the quality of the pruned models. However, the gain in speed for these
pruned methods is often overlooked in the literature due to the complex nature
of latency measurements. In this paper, we show the limitation of filter
pruning methods in terms of latency reduction and propose LayerPrune framework.
LayerPrune presents set of layer pruning methods based on different criteria
that achieve higher latency reduction than filter pruning methods on similar
accuracy. The advantage of layer pruning over filter pruning in terms of
latency reduction is a result of the fact that the former is not constrained by
the original model's depth and thus allows for a larger range of latency
reduction. For each filter pruning method we examined, we use the same filter
importance criterion to calculate a per-layer importance score in one-shot. We
then prune the least important layers and fine-tune the shallower model which
obtains comparable or better accuracy than its filter-based pruning
counterpart. This one-shot process allows to remove layers from single path
networks like VGG before fine-tuning, unlike in iterative filter pruning, a
minimum number of filters per layer is required to allow for data flow which
constraint the search space. To the best of our knowledge, we are the first to
examine the effect of pruning methods on latency metric instead of FLOPs for
multiple networks, datasets and hardware targets. LayerPrune also outperforms
handcrafted architectures such as Shufflenet, MobileNet, MNASNet and ResNet18
by 7.3%, 4.6%, 2.8% and 0.5% respectively on similar latency budget on ImageNet
dataset.
\\ ( https://arxiv.org/abs/2007.05667 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05675
Date: Sat, 11 Jul 2020 03:44:21 GMT   (5305kb,D)

Title: Coarse-to-Fine Pseudo-Labeling Guided Meta-Learning for Few-Shot
  Classification
Authors: Jinhai Yang, Hua Yang, Lin Chen
Categories: cs.CV cs.LG stat.ML
\\
  To endow neural networks with the potential to learn rapidly from a handful
of samples, meta-learning blazes a trail to acquire across-task knowledge from
a variety of few-shot learning tasks. However, most existing meta-learning
algorithms retain the requirement of fine-grained supervision, which is
expensive in many applications. In this paper, we show that meta-learning
models can extract transferable knowledge from coarse-grained supervision for
few-shot classification. We propose a weakly-supervised framework, namely
Coarse-to-fine Pseudo-labeling Guided Meta-Learning (CPGML), to alleviate the
need for data annotation. In our framework, the coarse-categories are grouped
into pseudo sub-categories to construct a task distribution for meta-training,
following the cosine distance between the corresponding embedding vectors of
images. For better feature representation in this process, we develop
Dual-level Discriminative Embedding (DDE) aiming to keep the distance between
learned embeddings consistent with the visual similarity and semantic relation
of input images simultaneously. Moreover, we propose a task-attention mechanism
to reduce the weight of the training tasks with potentially higher label noises
based on the observation of task-nonequivalence. Extensive experiments
conducted on two hierarchical meta-learning benchmarks demonstrate that, under
the proposed framework, meta-learning models can effectively extract
task-independent knowledge from the roughly-generated tasks and generalize well
to unseen tasks.
\\ ( https://arxiv.org/abs/2007.05675 ,  5305kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05676
Date: Sat, 11 Jul 2020 03:50:57 GMT   (5006kb,D)

Title: Learning Object Depth from Camera Motion and Video Object Segmentation
Authors: Brent A. Griffin and Jason J. Corso
Categories: cs.CV
\\
  Video object segmentation, i.e., the separation of a target object from
background in video, has made significant progress on real and challenging
videos in recent years. To leverage this progress in 3D applications, this
paper addresses the problem of learning to estimate the depth of segmented
objects given some measurement of camera motion (e.g., from robot kinematics or
vehicle odometry). We achieve this by, first, introducing a diverse, extensible
dataset and, second, designing a novel deep network that estimates the depth of
objects using only segmentation masks and uncalibrated camera movement. Our
data-generation framework creates artificial object segmentations that are
scaled for changes in distance between the camera and object, and our network
learns to estimate object depth even with segmentation errors. We demonstrate
our approach across domains using a robot camera to locate objects from the YCB
dataset and a vehicle camera to locate obstacles while driving.
\\ ( https://arxiv.org/abs/2007.05676 ,  5006kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05684
Date: Sat, 11 Jul 2020 05:29:28 GMT   (347kb,D)

Title: Fast Real-time Counterfactual Explanations
Authors: Yunxia Zhao
Categories: cs.CV
Comments: This paper has been accepted by ICML workshop 2020
\\
  Counterfactual explanations are considered, which is to answer {\it why the
prediction is class A but not B.} Different from previous optimization based
methods, an optimization-free Fast ReAl-time Counterfactual Explanation (FRACE)
algorithm is proposed benefiting from the development of multi-domain image to
image translation algorithms. Built from starGAN, a transformer is trained as a
residual generator conditional on a classifier constrained under a proposal
perturbation loss which maintains the content information of the query image,
but just the class-specific semantic information is changed. The transformer
can transfer the query image to any counterfactual class, and during inference,
our explanation can be generated by it only within a forward time. It is fast
and can satisfy the real-time practical application. Because of the adversarial
training of GAN, our explanation is also more realistic compared to other
counterparts. The experimental results demonstrate that our proposal is better
than the existing state of the art in terms of quality and speed.
\\ ( https://arxiv.org/abs/2007.05684 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05687
Date: Sat, 11 Jul 2020 05:44:16 GMT   (6509kb,D)

Title: Fast Video Object Segmentation With Temporal Aggregation Network and
  Dynamic Template Matching
Authors: Xuhua Huang, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang
Categories: cs.CV
Comments: CVPR2020
\\
  Significant progress has been made in Video Object Segmentation (VOS), the
video object tracking task in its finest level. While the VOS task can be
naturally decoupled into image semantic segmentation and video object tracking,
significantly much more research effort has been made in segmentation than
tracking. In this paper, we introduce "tracking-by-detection" into VOS which
can coherently integrate segmentation into tracking, by proposing a new
temporal aggregation network and a novel dynamic time-evolving template
matching mechanism to achieve significantly improved performance. Notably, our
method is entirely online and thus suitable for one-shot learning, and our
end-to-end trainable model allows multiple object segmentation in one forward
pass. We achieve new state-of-the-art performance on the DAVIS benchmark
without complicated bells and whistles in both speed and accuracy, with a speed
of 0.14 second per frame and J&F measure of 75.9% respectively.
\\ ( https://arxiv.org/abs/2007.05687 ,  6509kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05706
Date: Sat, 11 Jul 2020 07:44:04 GMT   (535kb,D)

Title: Cascade Network with Guided Loss and Hybrid Attention for Two-view
  Geometry
Authors: Zhi Chen and Fan Yang Wenbing Tao
Categories: cs.CV
\\
  In this paper, we are committed to designing a high-performance network for
two-view geometry. We first propose a Guided Loss and theoretically establish
the direct negative correlation between the loss and Fn-measure by dynamically
adjusting the weights of positive and negative classes during training, so that
the network is always trained towards the direction of increasing Fn-measure.
By this way, the network can maintain the advantage of the cross-entropy loss
while maximizing the Fn-measure. We then propose a hybrid attention block to
extract feature, which integrates the bayesian attentive context normalization
(BACN) and channel-wise attention (CA). BACN can mine the prior information to
better exploit global context and CA can capture complex channel context to
enhance the channel awareness of the network. Finally, based on our Guided Loss
and hybrid attention block, a cascade network is designed to gradually optimize
the result for more superior performance. Experiments have shown that our
network achieves the state-of-the-art performance on benchmark datasets.
\\ ( https://arxiv.org/abs/2007.05706 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05719
Date: Sat, 11 Jul 2020 08:43:34 GMT   (6755kb,D)

Title: AutoTrajectory: Label-free Trajectory Extraction and Prediction from
  Videos using Dynamic Points
Authors: Yuexin Ma, Xinge ZHU, Xinjing Cheng, Ruigang Yang, Jiming Liu, Dinesh
  Manocha
Categories: cs.CV
Journal-ref: ECCV 2020
\\
  Current methods for trajectory prediction operate in supervised manners, and
therefore require vast quantities of corresponding ground truth data for
training. In this paper, we present a novel, label-free algorithm,
AutoTrajectory, for trajectory extraction and prediction to use raw videos
directly. To better capture the moving objects in videos, we introduce dynamic
points. We use them to model dynamic motions by using a forward-backward
extractor to keep temporal consistency and using image reconstruction to keep
spatial consistency in an unsupervised manner. Then we aggregate dynamic points
to instance points, which stand for moving objects such as pedestrians in
videos. Finally, we extract trajectories by matching instance points for
prediction training. To the best of our knowledge, our method is the first to
achieve unsupervised learning of trajectory extraction and prediction. We
evaluate the performance on well-known trajectory datasets and show that our
method is effective for real-world videos and can use raw videos to further
improve the performance of existing models.
\\ ( https://arxiv.org/abs/2007.05719 ,  6755kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05720
Date: Sat, 11 Jul 2020 08:47:07 GMT   (1446kb,D)

Title: ECML: An Ensemble Cascade Metric Learning Mechanism towards Face
  Verification
Authors: Fu Xiong, Yang Xiao, Zhiguo Cao, Yancheng Wang, Joey Tianyi Zhou and
  Jianxi Wu
Categories: cs.CV
Comments: Accepted to IEEE Transaction on Cybernetics
\\
  Face verification can be regarded as a 2-class fine-grained visual
recognition problem. Enhancing the feature's discriminative power is one of the
key problems to improve its performance. Metric learning technology is often
applied to address this need, while achieving a good tradeoff between
underfitting and overfitting plays the vital role in metric learning. Hence, we
propose a novel ensemble cascade metric learning (ECML) mechanism. In
particular, hierarchical metric learning is executed in the cascade way to
alleviate underfitting. Meanwhile, at each learning level, the features are
split into non-overlapping groups. Then, metric learning is executed among the
feature groups in the ensemble manner to resist overfitting. Considering the
feature distribution characteristics of faces, a robust Mahalanobis metric
learning method (RMML) with closed-form solution is additionally proposed. It
can avoid the computation failure issue on inverse matrix faced by some
well-known metric learning approaches (e.g., KISSME). Embedding RMML into the
proposed ECML mechanism, our metric learning paradigm (EC-RMML) can run in the
one-pass learning manner. Experimental results demonstrate that EC-RMML is
superior to state-of-the-art metric learning methods for face verification.
And, the proposed ensemble cascade metric learning mechanism is also applicable
to other metric learning approaches.
\\ ( https://arxiv.org/abs/2007.05720 ,  1446kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05722
Date: Sat, 11 Jul 2020 08:57:58 GMT   (9382kb,D)

Title: Do We Need Sound for Sound Source Localization?
Authors: Takashi Oya, Shohei Iwase, Ryota Natsume, Takahiro Itazuri, Shugo
  Yamaguchi, Shigeo Morishima
Categories: cs.CV cs.SD eess.AS
Comments: Paper: 14 pages, 6 figures. Supplementary Material: 6 pages, 3
  figures. Videos and Codes will be released later
\\
  During the performance of sound source localization which uses both visual
and aural information, it presently remains unclear how much either image or
sound modalities contribute to the result, i.e. do we need both image and sound
for sound source localization? To address this question, we develop an
unsupervised learning system that solves sound source localization by
decomposing this task into two steps: (i) "potential sound source
localization", a step that localizes possible sound sources using only visual
information (ii) "object selection", a step that identifies which objects are
actually sounding using aural information. Our overall system achieves
state-of-the-art performance in sound source localization, and more
importantly, we find that despite the constraint on available information, the
results of (i) achieve similar performance. From this observation and further
experiments, we show that visual information is dominant in "sound" source
localization when evaluated with the currently adopted benchmark dataset.
Moreover, we show that the majority of sound-producing objects within the
samples in this dataset can be inherently identified using only visual
information, and thus that the dataset is inadequate to evaluate a system's
capability to leverage aural information. As an alternative, we present an
evaluation protocol that enforces both visual and aural information to be
leveraged, and verify this property through several experiments.
\\ ( https://arxiv.org/abs/2007.05722 ,  9382kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05729
Date: Sat, 11 Jul 2020 09:28:50 GMT   (9673kb,D)

Title: Usefulness of interpretability methods to explain deep learning based
  plant stress phenotyping
Authors: Koushik Nagasubramanian, Asheesh K. Singh, Arti Singh, Soumik Sarkar,
  Baskar Ganapathysubramanian
Categories: cs.CV
Comments: 15 pages, 6 figures
\\
  Deep learning techniques have been successfully deployed for automating plant
stress identification and quantification. In recent years, there is a growing
push towards training models that are interpretable -i.e. that justify their
classification decisions by visually highlighting image features that were
crucial for classification decisions. The expectation is that trained network
models utilize image features that mimic visual cues used by plant
pathologists. In this work, we compare some of the most popular
interpretability methods: Saliency Maps, SmoothGrad, Guided Backpropogation,
Deep Taylor Decomposition, Integrated Gradients, Layer-wise Relevance
Propagation and Gradient times Input, for interpreting the deep learning model.
We train a DenseNet-121 network for the classification of eight different
soybean stresses (biotic and abiotic). Using a dataset consisting of 16,573 RGB
images of healthy and stressed soybean leaflets captured under controlled
conditions, we obtained an overall classification accuracy of 95.05 \%. For a
diverse subset of the test data, we compared the important features with those
identified by a human expert. We observed that most interpretability methods
identify the infected regions of the leaf as important features for some -- but
not all -- of the correctly classified images. For some images, the output of
the interpretability methods indicated that spurious feature correlations may
have been used to correctly classify them. Although the output explanation maps
of these interpretability methods may be different from each other for a given
image, we advocate the use of these interpretability methods as `hypothesis
generation' mechanisms that can drive scientific insight.
\\ ( https://arxiv.org/abs/2007.05729 ,  9673kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05740
Date: Sat, 11 Jul 2020 10:44:41 GMT   (2318kb,D)

Title: Enhanced Behavioral Cloning Based self-driving Car Using Transfer
  Learning
Authors: Uppala Sumanth, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali
  Agarwal
Categories: cs.CV
\\
  With the growing phase of artificial intelligence and autonomous learning,
the self-driving car is one of the promising area of research and emerging as a
center of focus for automobile industries. Behavioral cloning is the process of
replicating human behavior via visuomotor policies by means of machine learning
algorithms. In recent years, several deep learning-based behavioral cloning
approaches have been developed in the context of self-driving cars specifically
based on the concept of transfer learning. Concerning the same, the present
paper proposes a transfer learning approach using VGG16 architecture, which is
fine tuned by retraining the last block while keeping other blocks as
non-trainable. The performance of proposed architecture is further compared
with existing NVIDIA architecture and its pruned variants (pruned by 22.2% and
33.85% using 1x1 filter to decrease the total number of parameters).
Experimental results show that the VGG16 with transfer learning architecture
has outperformed other discussed approaches with faster convergence.
\\ ( https://arxiv.org/abs/2007.05740 ,  2318kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05743
Date: Sat, 11 Jul 2020 11:04:16 GMT   (2238kb,D)

Title: Distangling Biological Noise in Cellular Images with a focus on
  Explainability
Authors: Manik Sharma and Ganapathy Krishnamurthi
Categories: cs.CV
Comments: 13 Pages, 12 figures
\\
  The cost of some drugs and medical treatments has risen in recent years that
many patients are having to go without. A classification project could make
researchers more efficient.
  One of the more surprising reasons behind the cost is how long it takes to
bring new treatments to market. Despite improvements in technology and science,
research and development continues to lag. In fact, finding new treatment
takes, on average, more than 10 years and costs hundreds of millions of
dollars. In turn, greatly decreasing the cost of treatments can make ensure
these treatments get to patients faster. This work aims at solving a part of
this problem by creating a cellular image classification model which can
decipher the genetic perturbations in cell (occurring naturally or
artificially). Another interesting question addressed is what makes the
deep-learning model decide in a particular fashion, which can further help in
demystifying the mechanism of action of certain perturbations and paves a way
towards the explainability of the deep-learning model.
  We show the results of Grad-CAM visualizations and make a case for the
significance of certain features over others. Further we discuss how these
significant features are pivotal in extracting useful diagnostic information
from the deep-learning model.
\\ ( https://arxiv.org/abs/2007.05743 ,  2238kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05751
Date: Sat, 11 Jul 2020 11:34:22 GMT   (1546kb)

Title: Driver Behavior Modelling at the Urban Intersection via Canonical
  Correlation Analysis
Authors: Zirui Li, Chao Lu, Cheng Gong, Cheng Gong, Jinghang Li, Lianzhen Wei
Categories: cs.CV cs.HC
Comments: 2020 3rd IEEE International Conference on Unmanned Systems (ICUS)
\\
  The urban intersection is a typically dynamic and complex scenario for
intelligent vehicles, which exists a variety of driving behaviors and traffic
participants. Accurately modelling the driver behavior at the intersection is
essential for intelligent transportation systems (ITS). Previous researches
mainly focus on using attention mechanism to model the degree of correlation.
In this research, a canonical correlation analysis (CCA)-based framework is
proposed. The value of canonical correlation is used for feature selection.
Gaussian mixture model and Gaussian process regression are applied for driver
behavior modelling. Two experiments using simulated and naturalistic driving
data are designed for verification. Experimental results are consistent with
the driver's judgment. Comparative studies show that the proposed framework can
obtain a better performance.
\\ ( https://arxiv.org/abs/2007.05751 ,  1546kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05756
Date: Sat, 11 Jul 2020 12:11:53 GMT   (4911kb,D)

Title: Generative Graph Perturbations for Scene Graph Prediction
Authors: Boris Knyazev, Harm de Vries, C\u{a}t\u{a}lina Cangea, Graham W.
  Taylor, Aaron Courville, Eugene Belilovsky
Categories: cs.CV cs.LG stat.ML
Comments: https://oolworkshop.github.io/program/ool_21.html, ICML Workshop 2020
  on "Object-Oriented Learning (OOL): Perception, Representation, and
  Reasoning"
\\
  Inferring objects and their relationships from an image is useful in many
applications at the intersection of vision and language. Due to a long tail
data distribution, the task is challenging, with the inevitable appearance of
zero-shot compositions of objects and relationships at test time. Current
models often fail to properly understand a scene in such cases, as during
training they only observe a tiny fraction of the distribution corresponding to
the most frequent compositions. This motivates us to study whether increasing
the diversity of the training distribution, by generating replacement for parts
of real scene graphs, can lead to better generalization? We employ generative
adversarial networks (GANs) conditioned on scene graphs to generate augmented
visual features. To increase their diversity, we propose several strategies to
perturb the conditioning. One of them is to use a language model, such as BERT,
to synthesize plausible yet still unlikely scene graphs. By evaluating our
model on Visual Genome, we obtain both positive and negative results. This
prompts us to make several observations that can potentially lead to further
improvements.
\\ ( https://arxiv.org/abs/2007.05756 ,  4911kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05779
Date: Sat, 11 Jul 2020 14:08:25 GMT   (1644kb)

Title: Pyramid Scale Network for Crowd Counting
Authors: Junhao Cheng, Zhuojun Chen, XinYu Zhang, Yizhou Li, Xiaoyuan Jing
Categories: cs.CV cs.LG
\\
  Crowd counting is a challenging task in computer vision due to serious
occlusions, complex background and large scale variations, etc. Multi-column
architecture is widely adopted to overcome these challenges, yielding
state-of-the-art performance in many public benchmarks. However, there still
are two issues in such design: scale limitation and feature similarity. Further
performance improvements are thus restricted. In this paper, we propose a novel
crowd counting framework called Pyramid Scale Network (PSNet) to explicitly
address these issues. Specifically, for scale limitation, we adopt three
Pyramid Scale Module (PSM) to efficiently capture multi-scale features, which
integrate a message passing mechanism and an attention mechanism into
multi-column architecture. Moreover, for feature similarity, a Differential
loss is introduced to make the features learned by each column in PSM
appropriately different from each other. To the best of our knowledge, PSNet is
the first work to explicitly address scale limitation and feature similarity in
multi-column design. Extensive experiments on five benchmark datasets
demonstrate the effectiveness of the proposed innovations as well as the
superior performance over the state-of-the-art. Our code is publicly available
at: https://github.com/JunhaoCheng/Pyramid_Scale_Network
\\ ( https://arxiv.org/abs/2007.05779 ,  1644kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05786
Date: Sat, 11 Jul 2020 14:37:28 GMT   (3177kb,D)

Title: Generalization of Deep Convolutional Neural Networks -- A Case-study on
  Open-source Chest Radiographs
Authors: Nazanin Mashhaditafreshi, Amara Tariq, Judy Wawira Gichoya, Imon
  Banerjee
Categories: cs.CV cs.LG
\\
  Deep Convolutional Neural Networks (DCNNs) have attracted extensive attention
and been applied in many areas, including medical image analysis and clinical
diagnosis. One major challenge is to conceive a DCNN model with remarkable
performance on both internal and external data. We demonstrate that DCNNs may
not generalize to new data, but increasing the quality and heterogeneity of the
training data helps to improve the generalizibility factor. We use
InceptionResNetV2 and DenseNet121 architectures to predict the risk of 5 common
chest pathologies. The experiments were conducted on three publicly available
databases: CheXpert, ChestX-ray14, and MIMIC Chest Xray JPG. The results show
the internal performance of each of the 5 pathologies outperformed external
performance on both of the models. Moreover, our strategy of exposing the
models to a mix of different datasets during the training phase helps to
improve model performance on the external dataset.
\\ ( https://arxiv.org/abs/2007.05786 ,  3177kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05835
Date: Sat, 11 Jul 2020 19:35:00 GMT   (18364kb,D)

Title: Lightweight Modules for Efficient Deep Learningbased Image Restoration
Authors: Avisek Lahiri, Sourav Bairagya, Sutanu Bera, Siddhant Haldar, Prabir
  Kumar Biswas
Categories: cs.CV eess.IV
Comments: Accepted at: IEEE Transactions on Circuits and Systems for Video
  Technology (Early Access Print) | |Codes Available at:
  https://github.com/avisekiit/TCSVT-LightWeight-CNNs | Supplementary Document
  at:
  https://drive.google.com/file/d/1BQhkh33Sen-d0qOrjq5h8ahw2VCUIVLg/view?usp=sharing
DOI: 10.1109/TCSVT.2020.3007723
\\
  Low level image restoration is an integral component of modern artificial
intelligence (AI) driven camera pipelines. Most of these frameworks are based
on deep neural networks which present a massive computational overhead on
resource constrained platform like a mobile phone. In this paper, we propose
several lightweight low-level modules which can be used to create a
computationally low cost variant of a given baseline model. Recent works for
efficient neural networks design have mainly focused on classification.
However, low-level image processing falls under the image-to-image' translation
genre which requires some additional computational modules not present in
classification. This paper seeks to bridge this gap by designing generic
efficient modules which can replace essential components used in contemporary
deep learning based image restoration networks. We also present and analyse our
results highlighting the drawbacks of applying depthwise separable
convolutional kernel (a popular method for efficient classification network)
for sub-pixel convolution based upsampling (a popular upsampling strategy for
low-level vision applications). This shows that concepts from domain of
classification cannot always be seamlessly integrated into image-to-image
translation tasks. We extensively validate our findings on three popular tasks
of image inpainting, denoising and super-resolution. Our results show that
proposed networks consistently output visually similar reconstructions compared
to full capacity baselines with significant reduction of parameters, memory
footprint and execution speeds on contemporary mobile devices.
\\ ( https://arxiv.org/abs/2007.05835 ,  18364kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05836
Date: Sat, 11 Jul 2020 19:37:44 GMT   (492kb,D)

Title: Meta Soft Label Generation for Noisy Labels
Authors: G\"orkem Algan, Ilkay Ulusoy
Categories: cs.CV
\\
  The existence of noisy labels in the dataset causes significant performance
degradation for deep neural networks (DNNs). To address this problem, we
propose a Meta Soft Label Generation algorithm called MSLG, which can jointly
generate soft labels using meta-learning techniques and learn DNN parameters in
an end-to-end fashion. Our approach adapts the meta-learning paradigm to
estimate optimal label distribution by checking gradient directions on both
noisy training data and noise-free meta-data. In order to iteratively update
soft labels, meta-gradient descent step is performed on estimated labels, which
would minimize the loss of noise-free meta samples. In each iteration, the base
classifier is trained on estimated meta labels. MSLG is model-agnostic and can
be added on top of any existing model at hand with ease. We performed extensive
experiments on CIFAR10, Clothing1M and Food101N datasets. Results show that our
approach outperforms other state-of-the-art methods by a large margin.
\\ ( https://arxiv.org/abs/2007.05836 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05853
Date: Sat, 11 Jul 2020 21:11:46 GMT   (406kb)

Title: Complex Wavelet SSIM based Image Data Augmentation
Authors: Ritin Raveendran, Aviral Singh, Rajesh Kumar M
Categories: cs.CV
\\
  One of the biggest problems in neural learning networks is the lack of
training data available to train the network. Data augmentation techniques over
the past few years, have therefore been developed, aiming to increase the
amount of artificial training data with the limited number of real world
samples. In this paper, we look particularly at the MNIST handwritten dataset
an image dataset used for digit recognition, and the methods of data
augmentation done on this data set. We then take a detailed look into one of
the most popular augmentation techniques used for this data set elastic
deformation; and highlight its demerit of degradation in the quality of data,
which introduces irrelevant data to the training set. To decrease this
irrelevancy, we propose to use a similarity measure called Complex Wavelet
Structural Similarity Index Measure (CWSSIM) to selectively filter out the
irrelevant data before we augment the data set. We compare our observations
with the existing augmentation technique and find our proposed method works
yields better results than the existing technique.
\\ ( https://arxiv.org/abs/2007.05853 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05854
Date: Sat, 11 Jul 2020 21:12:24 GMT   (588kb)

Title: Efficient resource management in UAVs for Visual Assistance
Authors: Bapireddy Karri and Sudip Misra, Senior Member IEEE
Categories: cs.CV cs.AI cs.LG
Comments: 7 pages
\\
  There is an increased interest in the use of Unmanned Aerial Vehicles (UAVs)
for agriculture, military, disaster management and aerial photography around
the world. UAVs are scalable, flexible and are useful in various environments
where direct human intervention is difficult. In general, the use of UAVs with
cameras mounted to them has increased in number due to their wide range of
applications in real life scenarios. With the advent of deep learning models in
computer vision many models have shown great success in visual tasks. But most
of evaluation models are done on high end CPUs and GPUs. One of major
challenges in using UAVs for Visual Assistance tasks in real time is managing
the memory usage and power consumption of the these tasks which are
computationally intensive and are difficult to be performed on low end
processor board of the UAV. This projects describes a novel method to optimize
the general image processing tasks like object tracking and object detection
for UAV hardware in real time scenarios without affecting the flight time and
not tampering the latency and accuracy of these models.
\\ ( https://arxiv.org/abs/2007.05854 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05856
Date: Sat, 11 Jul 2020 21:20:55 GMT   (6113kb,D)

Title: Anomaly Detection-Based Unknown Face Presentation Attack Detection
Authors: Yashasvi Baweja, Poojan Oza, Pramuditha Perera and Vishal M. Patel
Categories: cs.CV
\\
  Anomaly detection-based spoof attack detection is a recent development in
face Presentation Attack Detection (fPAD), where a spoof detector is learned
using only non-attacked images of users. These detectors are of practical
importance as they are shown to generalize well to new attack types. In this
paper, we present a deep-learning solution for anomaly detection-based spoof
attack detection where both classifier and feature representations are learned
together end-to-end. First, we introduce a pseudo-negative class during
training in the absence of attacked images. The pseudo-negative class is
modeled using a Gaussian distribution whose mean is calculated by a weighted
running mean. Secondly, we use pairwise confusion loss to further regularize
the training process. The proposed approach benefits from the representation
learning power of the CNNs and learns better features for fPAD task as shown in
our ablation study. We perform extensive experiments on four publicly available
datasets: Replay-Attack, Rose-Youtu, OULU-NPU and Spoof in Wild to show the
effectiveness of the proposed approach over the previous methods. Code is
available at: \url{https://github.com/yashasvi97/IJCB2020_anomaly}
\\ ( https://arxiv.org/abs/2007.05856 ,  6113kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05887
Date: Sun, 12 Jul 2020 02:17:29 GMT   (729kb)

Title: Train Your Data Processor: Distribution-Aware and Error-Compensation
  Coordinate Decoding for Human Pose Estimation
Authors: Feiyu Yang, Yu Chen, Zhe Pan, Min Zhang, Min Xue, Yaoyang Mo, Yao
  Zhang, Guoxiong Guan, Beibei Qian, Zhenzhong Xiao, Zhan Song
Categories: cs.CV
Comments: Improve the state-of-the-art of COCO keypoint detection challenge by
  1-2 AP. Project page: https://github.com/fyang235/DAEC
\\
  Recently, the leading performance of human pose estimation is dominated by
heatmap based methods. While being a fundamental component of heatmap
processing, heatmap decoding (i.e. transforming heatmaps to coordinates)
receives only limited investigations, to our best knowledge. This work fills
the gap by studying the heatmap decoding processing with a particular focus on
the errors introduced throughout the prediction process. We found that the
errors of heatmap based methods are surprisingly significant, which
nevertheless was universally ignored before. In view of the discovered
importance, we further reveal the intrinsic limitations of the previous widely
used heatmap decoding methods and thereout propose a Distribution-Aware and
Error-Compensation Coordinate Decoding (DAEC). Serving as a model-agnostic
plug-in, DAEC learns its decoding strategy from training data and remarkably
improves the performance of a variety of state-of-the-art human pose estimation
models. Specifically, equipped with DAEC, the SimpleBaseline-ResNet152-256x192
and HRNet-W48-256x192 are significantly improved by 2.6% and 2.9% achieving
72.6% and 75.7% on COCO, respectively. Moreover, the HR-W32-256x256 and
ResNet-152-256x256 frameworks enjoy even more dramatic promotions of 8.4% and
7.8% on MPII. Extensive experiments performed on these two common benchmarks,
demonstrates that DAEC exceeds its competitors by considerable margins, backing
up the rationality and generality of our novel heatmap decoding idea. The
project is available at https://github.com/fyang235/DAEC.
\\ ( https://arxiv.org/abs/2007.05887 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05892
Date: Sun, 12 Jul 2020 03:04:12 GMT   (4250kb,D)

Title: PA-GAN: Progressive Attention Generative Adversarial Network for Facial
  Attribute Editing
Authors: Zhenliang He, Meina Kan, Jichao Zhang, Shiguang Shan
Categories: cs.CV
Comments: Code: https://github.com/LynnHo/PA-GAN-Tensorflow
\\
  Facial attribute editing aims to manipulate attributes on the human face,
e.g., adding a mustache or changing the hair color. Existing approaches suffer
from a serious compromise between correct attribute generation and preservation
of the other information such as identity and background, because they edit the
attributes in the imprecise area. To resolve this dilemma, we propose a
progressive attention GAN (PA-GAN) for facial attribute editing. In our
approach, the editing is progressively conducted from high to low feature level
while being constrained inside a proper attribute area by an attention mask at
each level. This manner prevents undesired modifications to the irrelevant
regions from the beginning, and then the network can focus more on correctly
generating the attributes within a proper boundary at each level. As a result,
our approach achieves correct attribute editing with irrelevant details much
better preserved compared with the state-of-the-arts. Codes are released at
https://github.com/LynnHo/PA-GAN-Tensorflow.
\\ ( https://arxiv.org/abs/2007.05892 ,  4250kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05906
Date: Sun, 12 Jul 2020 04:31:28 GMT   (2665kb)

Title: Framework for Passenger Seat Availability Using Face Detection in
  Passenger Bus
Authors: Khawar Islam, Uzma Afzal
Categories: cs.CV cs.LG
\\
  Advancements in Intelligent Transportation System (IES) improve passenger
traveling by providing information systems for bus arrival time and counting
the number of passengers and buses in cities. Passengers still face bus waiting
and seat unavailability issues which have adverse effects on traffic management
and controlling authority. We propose a Face Detection based Framework (FDF) to
determine passenger seat availability in a camera-equipped bus through face
detection which is based on background subtraction to count empty, filled, and
total seats. FDF has an integrated smartphone Passenger Application (PA) to
identify the nearest bus stop. We evaluate FDF in a live test environment and
results show that it gives 90% accuracy. We believe our results have the
potential to address traffic management concerns and assist passengers to save
their valuable time
\\ ( https://arxiv.org/abs/2007.05906 ,  2665kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05914
Date: Sun, 12 Jul 2020 05:24:08 GMT   (423kb,D)

Title: Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data
  Analysis
Authors: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
Categories: cs.CV cs.LG
Comments: Accepted for Publication at MICCAI 2020
\\
  Automating the analysis of imagery of the Gastrointestinal (GI) tract
captured during endoscopy procedures has substantial potential benefits for
patients, as it can provide diagnostic support to medical practitioners and
reduce mistakes via human error. To further the development of such methods, we
propose a two-stream model for endoscopic image analysis. Our model fuses two
streams of deep feature inputs by mapping their inherent relations through a
novel relational network model, to better model symptoms and classify the
image. In contrast to handcrafted feature-based models, our proposed network is
able to learn features automatically and outperforms existing state-of-the-art
methods on two public datasets: KVASIR and Nerthus. Our extensive evaluations
illustrate the importance of having two streams of inputs instead of a single
stream and also demonstrates the merits of the proposed relational network
architecture to combine those streams.
\\ ( https://arxiv.org/abs/2007.05914 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05932
Date: Sun, 12 Jul 2020 07:58:31 GMT   (646kb,D)

Title: Pose-aware Adversarial Domain Adaptation for Personalized Facial
  Expression Recognition
Authors: Guang Liang, Shangfei Wang, Can Wang
Categories: cs.CV
\\
  Current facial expression recognition methods fail to simultaneously cope
with pose and subject variations.
  In this paper, we propose a novel unsupervised adversarial domain adaptation
method which can alleviate both variations at the same time. Specially, our
method consists of three learning strategies: adversarial domain adaptation
learning, cross adversarial feature learning, and reconstruction learning. The
first aims to learn pose- and expression-related feature representations in the
source domain and adapt both feature distributions to that of the target domain
by imposing adversarial learning. By using personalized adversarial domain
adaptation, this learning strategy can alleviate subject variations and exploit
information from the source domain to help learning in the target domain.
  The second serves to perform feature disentanglement between pose- and
expression-related feature representations by impulsing pose-related feature
representations expression-undistinguished and the expression-related feature
representations pose-undistinguished.
  The last can further boost feature learning by applying face image
reconstructions so that the learned expression-related feature representations
are more pose- and identity-robust.
  Experimental results on four benchmark datasets demonstrate the effectiveness
of the proposed method.
\\ ( https://arxiv.org/abs/2007.05932 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05934
Date: Sun, 12 Jul 2020 08:01:06 GMT   (1596kb,D)

Title: Adversarial Self-Supervised Learning for Semi-Supervised 3D Action
  Recognition
Authors: Chenyang Si, Xuecheng Nie, Wei Wang, Liang Wang, Tieniu Tan, Jiashi
  Feng
Categories: cs.CV
Comments: Accepted by ECCV2020
\\
  We consider the problem of semi-supervised 3D action recognition which has
been rarely explored before. Its major challenge lies in how to effectively
learn motion representations from unlabeled data. Self-supervised learning
(SSL) has been proved very effective at learning representations from unlabeled
data in the image domain. However, few effective self-supervised approaches
exist for 3D action recognition, and directly applying SSL for semi-supervised
learning suffers from misalignment of representations learned from SSL and
supervised learning tasks. To address these issues, we present Adversarial
Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and
the semi-supervised scheme via neighbor relation exploration and adversarial
learning. Specifically, we design an effective SSL scheme to improve the
discrimination capability of learned representations for 3D action recognition,
through exploring the data relations within a neighborhood. We further propose
an adversarial regularization to align the feature distributions of labeled and
unlabeled samples. To demonstrate effectiveness of the proposed ASSL in
semi-supervised 3D action recognition, we conduct extensive experiments on NTU
and N-UCLA datasets. The results confirm its advantageous performance over
state-of-the-art semi-supervised methods in the few label regime for 3D action
recognition.
\\ ( https://arxiv.org/abs/2007.05934 ,  1596kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05942
Date: Sun, 12 Jul 2020 09:01:57 GMT   (1879kb,D)

Title: Fruit classification using deep feature maps in the presence of
  deceptive similar classes
Authors: Mohit Dandekar, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali
  Agarwal
Categories: cs.CV
\\
  Autonomous detection and classification of objects are admired area of
research in many industrial applications. Though, humans can distinguish
objects with high multi-granular similarities very easily; but for the
machines, it is a very challenging task. The convolution neural networks (CNN)
have illustrated efficient performance in multi-level representations of
objects for classification. Conventionally, the existing deep learning models
utilize the transformed features generated by the rearmost layer for training
and testing. However, it is evident that this does not work well with
multi-granular data, especially, in presence of deceptive similar classes
(almost similar but different classes). The objective of the present research
is to address the challenge of classification of deceptively similar
multi-granular objects with an ensemble approach thfat utilizes activations
from multiple layers of CNN (deep features). These multi-layer activations are
further utilized to build multiple deep decision trees (known as Random forest)
for classification of objects with similar appearance. The Fruits-360 dataset
is utilized for evaluation of the proposed approach. With extensive trials it
was observed that the proposed model outperformed over the conventional deep
learning approaches.
\\ ( https://arxiv.org/abs/2007.05942 ,  1879kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05946
Date: Sun, 12 Jul 2020 09:16:06 GMT   (7716kb,D)

Title: Dual Adversarial Network: Toward Real-world Noise Removal and Noise
  Generation
Authors: Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng
Categories: cs.CV eess.IV
Comments: Accepted by ECCV 2020
ACM-class: I.4.4
\\
  Real-world image noise removal is a long-standing yet very challenging task
in computer vision. The success of deep neural network in denoising stimulates
the research of noise generation, aiming at synthesizing more clean-noisy image
pairs to facilitate the training of deep denoisers. In this work, we propose a
novel unified framework to simultaneously deal with the noise removal and noise
generation tasks. Instead of only inferring the posteriori distribution of the
latent clean image conditioned on the observed noisy image in traditional MAP
framework, our proposed method learns the joint distribution of the clean-noisy
image pairs. Specifically, we approximate the joint distribution with two
different factorized forms, which can be formulated as a denoiser mapping the
noisy image to the clean one and a generator mapping the clean image to the
noisy one. The learned joint distribution implicitly contains all the
information between the noisy and clean images, avoiding the necessity of
manually designing the image priors and noise assumptions as traditional.
Besides, the performance of our denoiser can be further improved by augmenting
the original training dataset with the learned generator. Moreover, we propose
two metrics to assess the quality of the generated noisy image, for which, to
the best of our knowledge, such metrics are firstly proposed along this
research line. Extensive experiments have been conducted to demonstrate the
superiority of our method over the state-of-the-arts both in the real noise
removal and generation tasks. The training and testing code is available at
https://github.com/zsyOAOA/DANet.
\\ ( https://arxiv.org/abs/2007.05946 ,  7716kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05950
Date: Sun, 12 Jul 2020 10:12:46 GMT   (1081kb,D)

Title: Self-Supervised Drivable Area and Road Anomaly Segmentation using RGB-D
  Data for Robotic Wheelchairs
Authors: Hengli Wang, Yuxiang Sun, Ming Liu
Categories: cs.CV cs.RO
Comments: Published in IEEE Robotics and Automation Letters (RA-L); 8 pages, 8
  figures and 3 tables
Journal-ref: 10.1109/LRA.2019.2932874
\\
  The segmentation of drivable areas and road anomalies are critical
capabilities to achieve autonomous navigation for robotic wheelchairs. The
recent progress of semantic segmentation using deep learning techniques has
presented effective results. However, the acquisition of large-scale datasets
with hand-labeled ground truth is time-consuming and labor-intensive, making
the deep learning-based methods often hard to implement in practice. We
contribute to the solution of this problem for the task of drivable area and
road anomaly segmentation by proposing a self-supervised learning approach. We
develop a pipeline that can automatically generate segmentation labels for
drivable areas and road anomalies. Then, we train RGB-D data-based semantic
segmentation neural networks and get predicted labels. Experimental results
show that our proposed automatic labeling pipeline achieves an impressive
speed-up compared to manual labeling. In addition, our proposed self-supervised
approach exhibits more robust and accurate results than the state-of-the-art
traditional algorithms as well as the state-of-the-art self-supervised
algorithms.
\\ ( https://arxiv.org/abs/2007.05950 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05981
Date: Sun, 12 Jul 2020 13:11:14 GMT   (4416kb,D)

Title: IllumiNet: Transferring Illumination from Planar Surfaces to Virtual
  Objects in Augmented Reality
Authors: Di Xu, Zhen Li, Yanning Zhang, Qi Cao
Categories: cs.CV
\\
  This paper presents an illumination estimation method for virtual objects in
real environment by learning. While previous works tackled this problem by
reconstructing high dynamic range (HDR) environment maps or the corresponding
spherical harmonics, we do not seek to recover the lighting environment of the
entire scene. Given a single RGB image, our method directly infers the relit
virtual object by transferring the illumination features extracted from planar
surfaces in the scene to the desired geometries. Compared to previous works,
our approach is more robust as it works in both indoor and outdoor environments
with spatially-varying illumination. Experiments and evaluation results show
that our approach outperforms the state-of-the-art quantitatively and
qualitatively, achieving realistic augmented experience.
\\ ( https://arxiv.org/abs/2007.05981 ,  4416kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05996
Date: Sun, 12 Jul 2020 14:16:35 GMT   (3314kb,D)

Title: Differentiable Programming for Hyperspectral Unmixing using a
  Physics-based Dispersion Model
Authors: John Janiczek, Parth Thaker, Gautam Dasarathy, Christopher S. Edwards,
  Philip Christensen, Suren Jayasuriya
Categories: cs.CV eess.IV physics.ao-ph
Comments: 36 pages, 11 figures. Accepted to European Conference on Computer
  Vision (ECCV) 2020
\\
  Hyperspectral unmixing is an important remote sensing task with applications
including material identification and analysis. Characteristic spectral
features make many pure materials identifiable from their visible-to-infrared
spectra, but quantifying their presence within a mixture is a challenging task
due to nonlinearities and factors of variation. In this paper, spectral
variation is considered from a physics-based approach and incorporated into an
end-to-end spectral unmixing algorithm via differentiable programming. The
dispersion model is introduced to simulate realistic spectral variation, and an
efficient method to fit the parameters is presented. Then, this dispersion
model is utilized as a generative model within an analysis-by-synthesis
spectral unmixing algorithm. Further, a technique for inverse rendering using a
convolutional neural network to predict parameters of the generative model is
introduced to enhance performance and speed when training data is available.
Results achieve state-of-the-art on both infrared and visible-to-near-infrared
(VNIR) datasets, and show promise for the synergy between physics-based models
and deep learning in hyperspectral unmixing in the future.
\\ ( https://arxiv.org/abs/2007.05996 ,  3314kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06002
Date: Sun, 12 Jul 2020 14:35:13 GMT   (919kb,D)

Title: Multi-Modality Information Fusion for Radiomics-based Neural
  Architecture Search
Authors: Yige Peng, Lei Bi, Michael Fulham, Dagan Feng, and Jinman Kim
Categories: cs.CV cs.LG eess.IV
Comments: Accepted by MICCAI 2020
\\
  'Radiomics' is a method that extracts mineable quantitative features from
radiographic images. These features can then be used to determine prognosis,
for example, predicting the development of distant metastases (DM). Existing
radiomics methods, however, require complex manual effort including the design
of hand-crafted radiomic features and their extraction and selection. Recent
radiomics methods, based on convolutional neural networks (CNNs), also require
manual input in network architecture design and hyper-parameter tuning.
Radiomic complexity is further compounded when there are multiple imaging
modalities, for example, combined positron emission tomography - computed
tomography (PET-CT) where there is functional information from PET and
complementary anatomical localization information from computed tomography
(CT). Existing multi-modality radiomics methods manually fuse the data that are
extracted separately. Reliance on manual fusion often results in sub-optimal
fusion because they are dependent on an 'expert's' understanding of medical
images. In this study, we propose a multi-modality neural architecture search
method (MM-NAS) to automatically derive optimal multi-modality image features
for radiomics and thus negate the dependence on a manual process. We evaluated
our MM-NAS on the ability to predict DM using a public PET-CT dataset of
patients with soft-tissue sarcomas (STSs). Our results show that our MM-NAS had
a higher prediction accuracy when compared to state-of-the-art radiomics
methods.
\\ ( https://arxiv.org/abs/2007.06002 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06013
Date: Sun, 12 Jul 2020 15:17:00 GMT   (7179kb,D)

Title: MeDaS: An open-source platform as service to help break the walls
  between medicine and informatics
Authors: Liang Zhang, Johann Li, Ping Li, Xiaoyuan Lu, Peiyi Shen, Guangming
  Zhu, Syed Afaq Shah, Mohammed Bennarmoun, Kun Qian, Bj\"orn W. Schuller
Categories: cs.CV eess.IV
\\
  In the past decade, deep learning (DL) has achieved unprecedented success in
numerous fields including computer vision, natural language processing, and
healthcare. In particular, DL is experiencing an increasing development in
applications for advanced medical image analysis in terms of analysis,
segmentation, classification, and furthermore. On the one hand, tremendous
needs that leverage the power of DL for medical image analysis are arising from
the research community of a medical, clinical, and informatics background to
jointly share their expertise, knowledge, skills, and experience. On the other
hand, barriers between disciplines are on the road for them often hampering a
full and efficient collaboration. To this end, we propose our novel open-source
platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the
best of our knowledge, MeDaS is the first open-source platform proving a
collaborative and interactive service for researchers from a medical background
easily using DL related toolkits, and at the same time for scientists or
engineers from information sciences to understand the medical knowledge side.
Based on a series of toolkits and utilities from the idea of RINV (Rapid
Implementation aNd Verification), our proposed MeDaS platform can implement
pre-processing, post-processing, augmentation, visualization, and other phases
needed in medical image analysis. Five tasks including the subjects of lung,
liver, brain, chest, and pathology, are validated and demonstrated to be
efficiently realisable by using MeDaS.
\\ ( https://arxiv.org/abs/2007.06013 ,  7179kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06032
Date: Sun, 12 Jul 2020 16:32:26 GMT   (5082kb,D)

Title: Probabilistic Jacobian-based Saliency Maps Attacks
Authors: Ant\'onio Loison, Th\'eo Combey and Hatem Hajri
Categories: cs.CV cs.CR cs.LG
Comments: 21 pages
\\
  Machine learning models have achieved spectacular performances in various
critical fields including intelligent monitoring, autonomous driving and
malware detection. Therefore, robustness against adversarial attacks represents
a key issue to trust these models. In particular, the Jacobian-based Saliency
Map Attack (JSMA) is widely used to fool neural network classifiers. In this
paper, we introduce Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), simple,
faster and more efficient versions of JSMA. These attacks rely upon new
saliency maps involving the neural network Jacobian, its output probabilities
and the input features. We demonstrate the advantages of WJSMA and TJSMA
through two computer vision applications on 1) LeNet-5, a well-known Neural
Network classifier (NNC), on the MNIST database and on 2) a more challenging
NNC on the CIFAR-10 dataset. We obtain that WJSMA and TJSMA significantly
outperform JSMA in success rate, speed and average number of changed features.
For instance, on LeNet-5 (with $100\%$ and $99.49\%$ accuracies on the training
and test sets), WJSMA and TJSMA respectively exceed $97\%$ and $98.60\%$ in
success rate for a maximum authorised distortion of $14.5\%$, outperforming
JSMA with more than $9.5$ and $11$ percentage points. The new attacks are then
used to defend and create more robust models than those trained against JSMA.
Like JSMA, our attacks are not scalable on large datasets such as IMAGENET but
despite this fact, they remain attractive for relatively small datasets like
MNIST, CIFAR-10 and may be potential tools for future applications.
\\ ( https://arxiv.org/abs/2007.06032 ,  5082kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06041
Date: Sun, 12 Jul 2020 17:08:41 GMT   (3469kb,D)

Title: Learning to associate detections for real-time multiple object tracking
Authors: Michel Meneses, Leonardo Matos, Bruno Prado, Andr\'e de Carvalho and
  Hendrik Macedo
Categories: cs.CV
Comments: 8 pages, 6 figures
\\
  With the recent advances in the object detection research field,
tracking-by-detection has become the leading paradigm adopted by multi-object
tracking algorithms. By extracting different features from detected objects,
those algorithms can estimate the objects' similarities and association
patterns along successive frames. However, since similarity functions applied
by tracking algorithms are handcrafted, it is difficult to employ them in new
contexts. In this study, it is investigated the use of artificial neural
networks to learning a similarity function that can be used among detections.
During training, the networks were introduced to correct and incorrect
association patterns, sampled from a pedestrian tracking data set. For such,
different motion and appearance features combinations have been explored.
Finally, a trained network has been inserted into a multiple-object tracking
framework, which has been assessed on the MOT Challenge benchmark. Throughout
the experiments, the proposed tracker matched the results obtained by
state-of-the-art methods, it has run 58\% faster than a recent and similar
method, used as baseline.
\\ ( https://arxiv.org/abs/2007.06041 ,  3469kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06068
Date: Sun, 12 Jul 2020 18:55:31 GMT   (5015kb,D)

Title: Visualizing Classification Structure in Deep Neural Networks
Authors: Bilal Alsallakh and Zhixin Yan and Shabnam Ghaffarzadegan and Zeng Dai
  and Liu Ren
Categories: cs.CV cs.LG stat.ML
Comments: 2020 ICML Workshop on Human Interpretability in Machine Learning (WHI
  2020)
\\
  We propose a measure to compute class similarity in large-scale
classification based on prediction scores. Such measure has not been formally
pro-posed in the literature. We show how visualizing the class similarity
matrix can reveal hierarchical structures and relationships that govern the
classes. Through examples with various classifiers, we demonstrate how such
structures can help in analyzing the classification behavior and in inferring
potential corner cases. The source code for one example is available as a
notebook at https://github.com/bilalsal/blocks
\\ ( https://arxiv.org/abs/2007.06068 ,  5015kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06071
Date: Sun, 12 Jul 2020 19:16:19 GMT   (1812kb,D)

Title: A Comparative Study on Polyp Classification using Convolutional Neural
  Networks
Authors: Krushi Patel, Kaidong Li, Ke Tao, Quan Wang, Ajay Bansal, Amit
  Rastogi, Guanghui Wang
Categories: cs.CV
\\
  Colorectal cancer is the third most common cancer diagnosed in both men and
women in the United States. Most colorectal cancers start as a growth on the
inner lining of the colon or rectum, called 'polyp'. Not all polyps are
cancerous, but some can develop into cancer. Early detection and recognition of
the type of polyps is critical to prevent cancer and change outcomes. However,
visual classification of polyps is challenging due to varying illumination
conditions of endoscopy, variant texture, appearance, and overlapping
morphology between polyps. More importantly, evaluation of polyp patterns by
gastroenterologists is subjective leading to a poor agreement among observers.
Deep convolutional neural networks have proven very successful in object
classification across various object categories. In this work, we compare the
performance of the state-of-the-art general object classification models for
polyp classification. We trained a total of six CNN models end-to-end using a
dataset of 157 video sequences composed of two types of polyps: hyperplastic
and adenomatous. Our results demonstrate that the state-of-the-art CNN models
can successfully classify polyps with an accuracy comparable or better than
reported among gastroenterologists. The results of this study can guide future
research in polyp classification.
\\ ( https://arxiv.org/abs/2007.06071 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06077
Date: Sun, 12 Jul 2020 19:54:32 GMT   (254kb,D)

Title: Sparse Graph to Sequence Learning for Vision Conditioned Long Textual
  Sequence Generation
Authors: Aditya Mogadala and Marius Mosbach and Dietrich Klakow
Categories: cs.CV cs.CL cs.LG
Comments: International Conference on Machine Learning (ICML) 2020 Workshop
  (https://logicalreasoninggnn.github.io/)
\\
  Generating longer textual sequences when conditioned on the visual
information is an interesting problem to explore. The challenge here
proliferate over the standard vision conditioned sentence-level generation
(e.g., image or video captioning) as it requires to produce a brief and
coherent story describing the visual content. In this paper, we mask this
Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with
the Transformer architecture. To be specific, we introduce Sparse
Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a
sequence. The encoder aims to directly encode graph-level semantics, while the
decoder is used to generate longer sequences. Experiments conducted with the
benchmark image paragraph dataset show that our proposed achieve 13.3%
improvement on the CIDEr evaluation measure when comparing to the previous
state-of-the-art approach.
\\ ( https://arxiv.org/abs/2007.06077 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06102
Date: Sun, 12 Jul 2020 21:44:38 GMT   (8333kb,D)

Title: SkyScapes -- Fine-Grained Semantic Understanding of Aerial Scenes
Authors: Seyed Majid Azimi, Corentin Henry, Lars Sommer, Arne Schumann and
  Eleonora Vig
Categories: cs.CV
Comments: Accepted in IEEE ICCV19
\\
  Understanding the complex urban infrastructure with centimeter-level accuracy
is essential for many applications from autonomous driving to mapping,
infrastructure monitoring, and urban management. Aerial images provide valuable
information over a large area instantaneously; nevertheless, no current dataset
captures the complexity of aerial scenes at the level of granularity required
by real-world applications. To address this, we introduce SkyScapes, an aerial
image dataset with highly-accurate, fine-grained annotations for pixel-level
semantic labeling. SkyScapes provides annotations for 31 semantic categories
ranging from large structures, such as buildings, roads and vegetation, to fine
details, such as 12 (sub-)categories of lane markings. We have defined two main
tasks on this dataset: dense semantic segmentation and multi-class lane-marking
prediction. We carry out extensive experiments to evaluate state-of-the-art
segmentation methods on SkyScapes. Existing methods struggle to deal with the
wide range of classes, object sizes, scales, and fine details present. We
therefore propose a novel multi-task model, which incorporates semantic edge
detection and is better tuned for feature extraction from a wide range of
scales. This model achieves notable improvements over the baselines in region
outlines and level of detail on both tasks.
\\ ( https://arxiv.org/abs/2007.06102 ,  8333kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06103
Date: Sun, 12 Jul 2020 21:47:35 GMT   (803kb,D)

Title: VINNAS: Variational Inference-based Neural Network Architecture Search
Authors: Martin Ferianc, Hongxiang Fan and Miguel Rodrigues
Categories: cs.CV cs.AI
Comments: Submitted to ICPR'20 https://github.com/iiml-ucl/vinnas
\\
  In recent years, neural architecture search (NAS) has received intensive
scientific and industrial interest due to its capability of finding a neural
architecture with high accuracy for various artificial intelligence tasks such
as image classification or object detection. In particular, gradient-based NAS
approaches have become one of the more popular approaches thanks to their
computational efficiency during the search. However, these methods often
experience a mode collapse, where the quality of the found architectures is
poor due to the algorithm resorting to choosing a single operation type for the
entire network, or stagnating at a local minima for various datasets or search
spaces. To address these defects, we present a differentiable variational
inference-based NAS method for searching sparse convolutional neural networks.
Our approach finds the optimal neural architecture by dropping out candidate
operations in an over-parameterised supergraph using variational dropout with
automatic relevance determination prior, which makes the algorithm gradually
remove unnecessary operations and connections without risking mode collapse.
The evaluation is conducted through searching two types of convolutional cells
that shape the neural network for classifying different image datasets. Our
method finds diverse network cells, while showing state-of-the-art accuracy
with up to $3 \times$ fewer parameters.
\\ ( https://arxiv.org/abs/2007.06103 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06124
Date: Sun, 12 Jul 2020 23:00:30 GMT   (9468kb,D)

Title: EAGLE: Large-scale Dataset for Vehicle Detection in Aerial Imagery
Authors: Seyed Majid Azimi, Reza Bahmanyar, Corenin Henry and Franz Kurz
Categories: cs.CV
\\
  Multi-class vehicle detection from airborne imagery with orientation
estimation is an important task in the near and remote vision domains with
applications in traffic monitoring and disaster management. In the last decade,
we have witnessed significant progress in object detection in ground imagery,
but it is still in its infancy in airborne imagery, mostly due to the scarcity
of diverse and large-scale datasets. Despite being a useful tool for different
applications, current airborne datasets only partially reflect the challenges
of real-world scenarios. To address this issue, we introduce EAGLE (oriEnted
object detection using Aerial imaGery in real-worLd scEnarios), a large-scale
dataset for multi-class vehicle detection with object orientation information
in aerial imagery. It features high-resolution aerial images composed of
different real-world situations with a wide variety of camera sensor,
resolution, flight altitude, weather, illumination, haze, shadow, time, city,
country, occlusion, and camera angle. The annotation was done by airborne
imagery experts with small- and large-vehicle classes. EAGLE contains 215,986
instances annotated with oriented bounding boxes defined by four points and
orientation, making it by far the largest dataset to date in this task. It also
supports researches on the haze and shadow removal as well as super-resolution
and in-painting applications. We define three tasks: detection by (1)
horizontal bounding boxes, (2) rotated bounding boxes, and (3) oriented
bounding boxes. We carried out several experiments to evaluate several
state-of-the-art methods in object detection on our dataset to form a baseline.
Experiments show that the EAGLE dataset accurately reflects real-world
situations and correspondingly challenging applications. The dataset will be
made publicly available.
\\ ( https://arxiv.org/abs/2007.06124 ,  9468kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06127
Date: Sun, 12 Jul 2020 23:13:06 GMT   (4974kb,D)

Title: DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D
  Structure Learning from Silhouette Images
Authors: Zhizhong Han and Chao Chen and Yu-Shen Liu and Matthias Zwicker
Categories: cs.CV
Comments: Accepted at ICML2020
\\
  Differentiable renderers have been used successfully for unsupervised 3D
structure learning from 2D images because they can bridge the gap between 3D
and 2D. To optimize 3D shape parameters, current renderers rely on pixel-wise
losses between rendered images of 3D reconstructions and ground truth images
from corresponding viewpoints. Hence they require interpolation of the
recovered 3D structure at each pixel, visibility handling, and optionally
evaluating a shading model. In contrast, here we propose a Differentiable
Renderer Without Rendering (DRWR) that omits these steps. DRWR only relies on a
simple but effective loss that evaluates how well the projections of
reconstructed 3D point clouds cover the ground truth object silhouette.
Specifically, DRWR employs a smooth silhouette loss to pull the projection of
each individual 3D point inside the object silhouette, and a structure-aware
repulsion loss to push each pair of projections that fall inside the silhouette
far away from each other. Although we omit surface interpolation, visibility
handling, and shading, our results demonstrate that DRWR achieves
state-of-the-art accuracies under widely used benchmarks, outperforming
previous methods both qualitatively and quantitatively. In addition, our
training times are significantly lower due to the simplicity of DRWR.
\\ ( https://arxiv.org/abs/2007.06127 ,  4974kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06131
Date: Sun, 12 Jul 2020 23:45:51 GMT   (4245kb,D)

Title: Locality Guided Neural Networks for Explainable Artificial Intelligence
Authors: Randy Tan, Naimul Khan, and Ling Guan
Categories: cs.CV cs.AI cs.LG
Comments: 8 pages, 3 figures, submitted to WCCI2020
\\
  In current deep network architectures, deeper layers in networks tend to
contain hundreds of independent neurons which makes it hard for humans to
understand how they interact with each other. By organizing the neurons by
correlation, humans can observe how clusters of neighbouring neurons interact
with each other. In this paper, we propose a novel algorithm for back
propagation, called Locality Guided Neural Network(LGNN) for training networks
that preserves locality between neighbouring neurons within each layer of a
deep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to
enforce a local topology on each layer of a deep network such that neighbouring
neurons are highly correlated with each other. This method contributes to the
domain of Explainable Artificial Intelligence (XAI), which aims to alleviate
the black-box nature of current AI methods and make them understandable by
humans. Our method aims to achieve XAI in deep learning without changing the
structure of current models nor requiring any post processing. This paper
focuses on Convolutional Neural Networks (CNNs), but can theoretically be
applied to any type of deep learning architecture. In our experiments, we train
various VGG and Wide ResNet (WRN) networks for image classification on
CIFAR100. In depth analyses presenting both qualitative and quantitative
results demonstrate that our method is capable of enforcing a topology on each
layer while achieving a small increase in classification accuracy
\\ ( https://arxiv.org/abs/2007.06131 ,  4245kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06141
Date: Mon, 13 Jul 2020 01:09:06 GMT   (2463kb,D)

Title: Gender Classification and Bias Mitigation in Facial Images
Authors: Wenying Wu, Pavlos Protopapas, Zheng Yang, Panagiotis Michalatos
Categories: cs.CV cs.LG
Comments: 9 pages
Journal-ref: WebSci (2020) 106-114
DOI: 10.1145/3394231
\\
  Gender classification algorithms have important applications in many domains
today such as demographic research, law enforcement, as well as human-computer
interaction. Recent research showed that algorithms trained on biased benchmark
databases could result in algorithmic bias. However, to date, little research
has been carried out on gender classification algorithms' bias towards gender
minorities subgroups, such as the LGBTQ and the non-binary population, who have
distinct characteristics in gender expression. In this paper, we began by
conducting surveys on existing benchmark databases for facial recognition and
gender classification tasks. We discovered that the current benchmark databases
lack representation of gender minority subgroups. We worked on extending the
current binary gender classifier to include a non-binary gender class. We did
that by assembling two new facial image databases: 1) a racially balanced
inclusive database with a subset of LGBTQ population 2) an inclusive-gender
database that consists of people with non-binary gender. We worked to increase
classification accuracy and mitigate algorithmic biases on our baseline model
trained on the augmented benchmark database. Our ensemble model has achieved an
overall accuracy score of 90.39%, which is a 38.72% increase from the baseline
binary gender classifier trained on Adience. While this is an initial attempt
towards mitigating bias in gender classification, more work is needed in
modeling gender as a continuum by assembling more inclusive databases.
\\ ( https://arxiv.org/abs/2007.06141 ,  2463kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06143
Date: Mon, 13 Jul 2020 01:13:23 GMT   (500kb,D)

Title: Embedded Deep Bilinear Interactive Information and Selective Fusion for
  Multi-view Learning
Authors: Jinglin Xu, Wenbin Li, Jiantao Shen, Xinwang Liu, Peicheng Zhou,
  Xiangsen Zhang, Xiwen Yao, and Junwei Han
Categories: cs.CV cs.LG
\\
  As a concrete application of multi-view learning, multi-view classification
improves the traditional classification methods significantly by integrating
various views optimally. Although most of the previous efforts have been
demonstrated the superiority of multi-view learning, it can be further improved
by comprehensively embedding more powerful cross-view interactive information
and a more reliable multi-view fusion strategy in intensive studies. To fulfill
this goal, we propose a novel multi-view learning framework to make the
multi-view classification better aimed at the above-mentioned two aspects. That
is, we seamlessly embed various intra-view information, cross-view
multi-dimension bilinear interactive information, and a new view ensemble
mechanism into a unified framework to make a decision via the optimization. In
particular, we train different deep neural networks to learn various intra-view
representations, and then dynamically learn multi-dimension bilinear
interactive information from different bilinear similarities via the bilinear
function between views. After that, we adaptively fuse the representations of
multiple views by flexibly tuning the parameters of the view-weight, which not
only avoids the trivial solution of weight but also provides a new way to
select a few discriminative views that are beneficial to make a decision for
the multi-view classification. Extensive experiments on six publicly available
datasets demonstrate the effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2007.06143 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06144
Date: Mon, 13 Jul 2020 01:17:25 GMT   (5274kb,D)

Title: Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection
Authors: Cong Chen and Shouyang Dong and Ye Tian abd Kunlin Cao and Li Liu and
  Yuanhao Guo
Categories: cs.CV
Comments: 11 papges, 3 figures, preprint for submission
\\
  This paper focuses on the problem of Semi-Supervised Object Detection (SSOD).
In the field of Semi-Supervised Learning (SSL), the Knowledge Distillation (KD)
framework which consists of a teacher model and a student model is widely used
to make good use of the unlabeled images. Given unlabeled images, the teacher
is supposed to yield meaningful targets (e.g. well-posed logits) to regularize
the training of the student. However, directly applying the KD framework in
SSOD has the following obstacles. (1) Teacher and student predictions may be
very close which limits the upper-bound of the student, and (2) the data
imbalance dilemma caused by dense prediction from object detection hinders an
efficient consistency regularization between the teacher and student. To solve
these problems, we propose the Temporal Self-Ensembling Teacher (TSE-T) model
on top of the KD framework. Differently from the conventional KD methods, we
devise a temporally updated teacher model. First, our teacher model ensembles
its temporal predictions for unlabeled images under varying perturbations.
Second, our teacher model ensembles its temporal model weights by Exponential
Moving Average (EMA) which allows it gradually learn from student. The above
self-ensembling strategies collaboratively yield better teacher predictions for
unblabeled images. Finally, we use focal loss to formulate the consistency
regularization to handle the data imbalance problem. Evaluated on the widely
used VOC and COCO benchmarks, our method has achieved 80.73% and 40.52% (mAP)
on the VOC2007 test set and the COCO2012 test-dev set respectively, which
outperforms the fully-supervised detector by 2.37% and 1.49%. Furthermore, our
method sets the new state state of the art in SSOD on VOC benchmark which
outperforms the baseline SSOD method by 1.44%. The source code of this work is
publicly available at \url{http://github.com/SYangDong/tse-t.
\\ ( https://arxiv.org/abs/2007.06144 ,  5274kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06146
Date: Mon, 13 Jul 2020 01:31:12 GMT   (7728kb,D)

Title: Fine-Grained Crowd Counting
Authors: Jia Wan, Nikil Senthil Kumar, Antoni B. Chan
Categories: cs.CV
\\
  Current crowd counting algorithms are only concerned about the number of
people in an image, which lacks low-level fine-grained information of the
crowd. For many practical applications, the total number of people in an image
is not as useful as the number of people in each sub-category. E.g., knowing
the number of people waiting inline or browsing can help retail stores; knowing
the number of people standing/sitting can help restaurants/cafeterias; knowing
the number of violent/non-violent people can help police in crowd management.
In this paper, we propose fine-grained crowd counting, which differentiates a
crowd into categories based on the low-level behavior attributes of the
individuals (e.g. standing/sitting or violent behavior) and then counts the
number of people in each category. To enable research in this area, we
construct a new dataset of four real-world fine-grained counting tasks:
traveling direction on a sidewalk, standing or sitting, waiting in line or not,
and exhibiting violent behavior or not. Since the appearance features of
different crowd categories are similar, the challenge of fine-grained crowd
counting is to effectively utilize contextual information to distinguish
between categories. We propose a two branch architecture, consisting of a
density map estimation branch and a semantic segmentation branch. We propose
two refinement strategies for improving the predictions of the two branches.
First, to encode contextual information, we propose feature propagation guided
by the density map prediction, which eliminates the effect of background
features during propagation. Second, we propose a complementary attention model
to share information between the two branches. Experiment results confirm the
effectiveness of our method.
\\ ( https://arxiv.org/abs/2007.06146 ,  7728kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06149
Date: Mon, 13 Jul 2020 01:49:07 GMT   (11176kb,D)

Title: Universal-to-Specific Framework for Complex Action Recognition
Authors: Peisen Zhao, Lingxi Xie, Ya Zhang, Qi Tian
Categories: cs.CV
Comments: 13 pages, 8 figures
\\
  Video-based action recognition has recently attracted much attention in the
field of computer vision. To solve more complex recognition tasks, it has
become necessary to distinguish different levels of interclass variations.
Inspired by a common flowchart based on the human decision-making process that
first narrows down the probable classes and then applies a "rethinking" process
for finer-level recognition, we propose an effective universal-to-specific
(U2S) framework for complex action recognition. The U2S framework is composed
of three subnetworks: a universal network, a category-specific network, and a
mask network. The universal network first learns universal feature
representations. The mask network then generates attention masks for confusing
classes through category regularization based on the output of the universal
network. The mask is further used to guide the category-specific network for
class-specific feature representations. The entire framework is optimized in an
end-to-end manner. Experiments on a variety of benchmark datasets, e.g., the
Something-Something, UCF101, and HMDB51 datasets, demonstrate the effectiveness
of the U2S framework; i.e., U2S can focus on discriminative spatiotemporal
regions for confusing categories. We further visualize the relationship between
different classes, showing that U2S indeed improves the discriminability of
learned features. Moreover, the proposed U2S model is a general framework and
may adopt any base recognition network.
\\ ( https://arxiv.org/abs/2007.06149 ,  11176kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06153
Date: Mon, 13 Jul 2020 02:04:39 GMT   (12733kb,D)

Title: AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning
Authors: Mehdi Mousavi, Aashis Khanal, Rolando Estrada
Categories: cs.CV cs.LG
Comments: 14 pages, 7 figures
\\
  Machine learning requires data, but acquiring and labeling real-world data is
challenging, expensive, and time-consuming. More importantly, it is nearly
impossible to alter real data post-acquisition (e.g., change the illumination
of a room), making it very difficult to measure how specific properties of the
data affect performance. In this paper, we present AI Playground (AIP), an
open-source, Unreal Engine-based tool for generating and labeling virtual image
data. With AIP, it is trivial to capture the same image under different
conditions (e.g., fidelity, lighting, etc.) and with different ground truths
(e.g., depth or surface normal values). AIP is easily extendable and can be
used with or without code. To validate our proposed tool, we generated eight
datasets of otherwise identical but varying lighting and fidelity conditions.
We then trained deep neural networks to predict (1) depth values, (2) surface
normals, or (3) object labels and assessed each network's intra- and
cross-dataset performance. Among other insights, we verified that sensitivity
to different settings is problem-dependent. We confirmed the findings of other
studies that segmentation models are very sensitive to fidelity, but we also
found that they are just as sensitive to lighting. In contrast, depth and
normal estimation models seem to be less sensitive to fidelity or lighting and
more sensitive to the structure of the image. Finally, we tested our trained
depth-estimation networks on two real-world datasets and obtained results
comparable to training on real data alone, confirming that our virtual
environments are realistic enough for real-world tasks.
\\ ( https://arxiv.org/abs/2007.06153 ,  12733kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06156
Date: Mon, 13 Jul 2020 02:44:38 GMT   (1570kb,D)

Title: Deep Reinforced Attention Learning for Quality-Aware Visual Recognition
Authors: Duo Li, Qifeng Chen
Categories: cs.CV
Comments: Accepted by ECCV 2020
\\
  In this paper, we build upon the weakly-supervised generation mechanism of
intermediate attention maps in any convolutional neural networks and disclose
the effectiveness of attention modules more straightforwardly to fully exploit
their potential. Given an existing neural network equipped with arbitrary
attention modules, we introduce a meta critic network to evaluate the quality
of attention maps in the main network. Due to the discreteness of our designed
reward, the proposed learning method is arranged in a reinforcement learning
setting, where the attention actors and recurrent critics are alternately
optimized to provide instant critique and revision for the temporary attention
representation, hence coined as Deep REinforced Attention Learning (DREAL). It
could be applied universally to network architectures with different types of
attention modules and promotes their expressive ability by maximizing the
relative gain of the final recognition performance arising from each individual
attention module, as demonstrated by extensive experiments on both category and
instance recognition benchmarks.
\\ ( https://arxiv.org/abs/2007.06156 ,  1570kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06166
Date: Mon, 13 Jul 2020 03:13:48 GMT   (5189kb,D)

Title: Low to High Dimensional Modality Hallucination using Aggregated Fields
  of View
Authors: Kausic Gunasekar, Qiang Qiu and Yezhou Yang
Categories: cs.CV
Journal-ref: IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.
  1983-1990, April 2020
DOI: 10.1109/LRA.2020.2970679
\\
  Real-world robotics systems deal with data from a multitude of modalities,
especially for tasks such as navigation and recognition. The performance of
those systems can drastically degrade when one or more modalities become
inaccessible, due to factors such as sensors' malfunctions or adverse
environments. Here, we argue modality hallucination as one effective way to
ensure consistent modality availability and thereby reduce unfavorable
consequences. While hallucinating data from a modality with richer information,
e.g., RGB to depth, has been researched extensively, we investigate the more
challenging low-to-high modality hallucination with interesting use cases in
robotics and autonomous systems. We present a novel hallucination architecture
that aggregates information from multiple fields of view of the local
neighborhood to recover the lost information from the extant modality. The
process is implemented by capturing a non-linear mapping between the data
modalities and the learned mapping is used to aid the extant modality to
mitigate the risk posed to the system in the adverse scenarios which involve
modality loss. We also conduct extensive classification and segmentation
experiments on UWRGBD and NYUD datasets and demonstrate that hallucination
allays the negative effects of the modality loss. Implementation and models:
https://github.com/kausic94/Hallucination
\\ ( https://arxiv.org/abs/2007.06166 ,  5189kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06178
Date: Mon, 13 Jul 2020 04:06:43 GMT   (5535kb,D)

Title: Bridging Maximum Likelihood and Adversarial Learning via
  $\alpha$-Divergence
Authors: Miaoyun Zhao, Yulai Cong, Shuyang Dai, Lawrence Carin
Categories: cs.CV
Comments: AAAI 2020
\\
  Maximum likelihood (ML) and adversarial learning are two popular approaches
for training generative models, and from many perspectives these techniques are
complementary. ML learning encourages the capture of all data modes, and it is
typically characterized by stable training. However, ML learning tends to
distribute probability mass diffusely over the data space, $e.g.$, yielding
blurry synthetic images. Adversarial learning is well known to synthesize
highly realistic natural images, despite practical challenges like mode
dropping and delicate training. We propose an $\alpha$-Bridge to unify the
advantages of ML and adversarial learning, enabling the smooth transfer from
one to the other via the $\alpha$-divergence. We reveal that generalizations of
the $\alpha$-Bridge are closely related to approaches developed recently to
regularize adversarial learning, providing insights into that prior work, and
further understanding of why the $\alpha$-Bridge performs well in practice.
\\ ( https://arxiv.org/abs/2007.06178 ,  5535kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06181
Date: Mon, 13 Jul 2020 04:27:25 GMT   (832kb,D)

Title: Learning to Learn Parameterized Classification Networks for Scalable
  Input Images
Authors: Duo Li, Anbang Yao and Qifeng Chen
Categories: cs.CV
Comments: Accepted by ECCV 2020. Code and models are available at
  https://github.com/d-li14/SAN
\\
  Convolutional Neural Networks (CNNs) do not have a predictable recognition
behavior with respect to the input resolution change. This prevents the
feasibility of deployment on different input image resolutions for a specific
model. To achieve efficient and flexible image classification at runtime, we
employ meta learners to generate convolutional weights of main networks for
various input scales and maintain privatized Batch Normalization layers per
scale. For improved training performance, we further utilize knowledge
distillation on the fly over model predictions based on different input
resolutions. The learned meta network could dynamically parameterize main
networks to act on input images of arbitrary size with consistently better
accuracy compared to individually trained models. Extensive experiments on the
ImageNet demonstrate that our method achieves an improved accuracy-efficiency
trade-off during the adaptive inference process. By switching executable input
resolutions, our method could satisfy the requirement of fast adaption in
different resource-constrained environments. Code and models are available at
https://github.com/d-li14/SAN.
\\ ( https://arxiv.org/abs/2007.06181 ,  832kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06189
Date: Mon, 13 Jul 2020 05:00:09 GMT   (4737kb,D)

Title: Understanding Adversarial Examples from the Mutual Influence of Images
  and Perturbations
Authors: Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In-So Kweon
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2020
\\
  A wide variety of works have explored the reason for the existence of
adversarial examples, but there is no consensus on the explanation. We propose
to treat the DNN logits as a vector for feature representation, and exploit
them to analyze the mutual influence of two independent inputs based on the
Pearson correlation coefficient (PCC). We utilize this vector representation to
understand adversarial examples by disentangling the clean images and
adversarial perturbations, and analyze their influence on each other. Our
results suggest a new perspective towards the relationship between images and
universal perturbations: Universal perturbations contain dominant features, and
images behave like noise to them. This feature perspective leads to a new
method for generating targeted universal adversarial perturbations using random
source images. We are the first to achieve the challenging task of a targeted
universal attack without utilizing original training data. Our approach using a
proxy dataset achieves comparable performance to the state-of-the-art baselines
which utilize the original training dataset.
\\ ( https://arxiv.org/abs/2007.06189 ,  4737kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06191
Date: Mon, 13 Jul 2020 05:14:11 GMT   (4000kb,D)

Title: PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale
  Convolutional Layer
Authors: Duo Li, Anbang Yao and Qifeng Chen
Categories: cs.CV
Comments: Accepted by ECCV 2020. Code and models are available at
  https://github.com/d-li14/PSConv
\\
  Despite their strong modeling capacities, Convolutional Neural Networks
(CNNs) are often scale-sensitive. For enhancing the robustness of CNNs to scale
variance, multi-scale feature fusion from different layers or filters attracts
great attention among existing solutions, while the more granular kernel space
is overlooked. We bridge this regret by exploiting multi-scale features in a
finer granularity. The proposed convolution operation, named Poly-Scale
Convolution (PSConv), mixes up a spectrum of dilation rates and tactfully
allocate them in the individual convolutional kernels of each filter regarding
a single convolutional layer. Specifically, dilation rates vary cyclically
along the axes of input and output channels of the filters, aggregating
features over a wide range of scales in a neat style. PSConv could be a drop-in
replacement of the vanilla convolution in many prevailing CNN backbones,
allowing better representation learning without introducing additional
parameters and computational complexities. Comprehensive experiments on the
ImageNet and MS COCO benchmarks validate the superior performance of PSConv.
Code and models are available at https://github.com/d-li14/PSConv.
\\ ( https://arxiv.org/abs/2007.06191 ,  4000kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06196
Date: Mon, 13 Jul 2020 05:27:48 GMT   (291kb,D)

Title: Data from Model: Extracting Data from Non-robust and Robust Models
Authors: Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In-So Kweon
Categories: cs.CV cs.LG
Comments: Accepted at the CVPR 2020 Workshop on Adversarial Machine Learning in
  Computer Vision
\\
  The essence of deep learning is to exploit data to train a deep neural
network (DNN) model. This work explores the reverse process of generating data
from a model, attempting to reveal the relationship between the data and the
model. We repeat the process of Data to Model (DtM) and Data from Model (DfM)
in sequence and explore the loss of feature mapping information by measuring
the accuracy drop on the original validation dataset. We perform this
experiment for both a non-robust and robust origin model. Our results show that
the accuracy drop is limited even after multiple sequences of DtM and DfM,
especially for robust models. The success of this cycling transformation can be
attributed to the shared feature mapping existing in data and model. Using the
same data, we observe that different DtM processes result in models having
different features, especially for different network architecture families,
even though they achieve comparable performance.
\\ ( https://arxiv.org/abs/2007.06196 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06198
Date: Mon, 13 Jul 2020 05:36:36 GMT   (3380kb,D)

Title: Reducing Language Biases in Visual Question Answering with
  Visually-Grounded Question Encoder
Authors: Gouthaman KV and Anurag Mittal
Categories: cs.CV
Comments: ECCV 2020
\\
  Recent studies have shown that current VQA models are heavily biased on the
language priors in the train set to answer the question, irrespective of the
image. E.g., overwhelmingly answer "what sport is" as "tennis" or "what color
banana" as "yellow." This behavior restricts them from real-world application
scenarios. In this work, we propose a novel model-agnostic question encoder,
Visually-Grounded Question Encoder (VGQE), for VQA that reduces this effect.
VGQE utilizes both visual and language modalities equally while encoding the
question. Hence the question representation itself gets sufficient
visual-grounding, and thus reduces the dependency of the model on the language
priors. We demonstrate the effect of VGQE on three recent VQA models and
achieve state-of-the-art results on the bias-sensitive split of the VQAv2
dataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on
the standard VQAv2 benchmark, our approach does not drop the accuracy; instead,
it improves the performance.
\\ ( https://arxiv.org/abs/2007.06198 ,  3380kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06227
Date: Mon, 13 Jul 2020 07:59:55 GMT   (2796kb,D)

Title: Hierarchical Dynamic Filtering Network for RGB-D Salient Object
  Detection
Authors: Youwei Pang, Lihe Zhang, Xiaoqi Zhao, Huchuan Lu
Categories: cs.CV
Comments: The work has been accepted by ECCV 2020
\\
  The main purpose of RGB-D salient object detection (SOD) is how to better
integrate and utilize cross-modal fusion information. In this paper, we explore
these issues from a new perspective. We integrate the features of different
modalities through densely connected structures and use their mixed features to
generate dynamic filters with receptive fields of different sizes. In the end,
we implement a kind of more flexible and efficient multi-scale cross-modal
feature processing, i.e. dynamic dilated pyramid module. In order to make the
predictions have sharper edges and consistent saliency regions, we design a
hybrid enhanced loss function to further optimize the results. This loss
function is also validated to be effective in the single-modal RGB SOD task. In
terms of six metrics, the proposed method outperforms the existing twelve
methods on eight challenging benchmark datasets. A large number of experiments
verify the effectiveness of the proposed module and loss function. Our code,
model and results are available at \url{https://github.com/lartpang/HDFNet}.
\\ ( https://arxiv.org/abs/2007.06227 ,  2796kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06233
Date: Mon, 13 Jul 2020 08:24:41 GMT   (6014kb,D)

Title: Location-Aware Box Reasoning for Anchor-Based Single-Shot Object
  Detection
Authors: Wenchi Ma, Kaidong Li, Guanghui Wang
Categories: cs.CV
\\
  In the majority of object detection frameworks, the confidence of instance
classification is used as the quality criterion of predicted bounding boxes,
like the confidence-based ranking in non-maximum suppression (NMS). However,
the quality of bounding boxes, indicating the spatial relations, is not only
correlated with the classification scores. Compared with the region proposal
network (RPN) based detectors, single-shot object detectors suffer the box
quality as there is a lack of pre-selection of box proposals. In this paper, we
aim at single-shot object detectors and propose a location-aware anchor-based
reasoning (LAAR) for the bounding boxes. LAAR takes both the location and
classification confidences into consideration for the quality evaluation of
bounding boxes. We introduce a novel network block to learn the relative
location between the anchors and the ground truths, denoted as a localization
score, which acts as a location reference during the inference stage. The
proposed localization score leads to an independent regression branch and
calibrates the bounding box quality by scoring the predicted localization score
so that the best-qualified bounding boxes can be picked up in NMS. Experiments
on MS COCO and PASCAL VOC benchmarks demonstrate that the proposed
location-aware framework enhances the performances of current anchor-based
single-shot object detection frameworks and yields consistent and robust
detection results.
\\ ( https://arxiv.org/abs/2007.06233 ,  6014kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06240
Date: Mon, 13 Jul 2020 08:49:00 GMT   (1048kb,D)

Title: Expert Training: Task Hardness Aware Meta-Learning for Few-Shot
  Classification
Authors: Yucan Zhou, Yu Wang, Jianfei Cai, Yu Zhou, Qinghua Hu, Weiping Wang
Categories: cs.CV cs.LG stat.ML
Comments: 9 pages, 6 figures
\\
  Deep neural networks are highly effective when a large number of labeled
samples are available but fail with few-shot classification tasks. Recently,
meta-learning methods have received much attention, which train a meta-learner
on massive additional tasks to gain the knowledge to instruct the few-shot
classification. Usually, the training tasks are randomly sampled and performed
indiscriminately, often making the meta-learner stuck into a bad local optimum.
Some works in the optimization of deep neural networks have shown that a better
arrangement of training data can make the classifier converge faster and
perform better. Inspired by this idea, we propose an easy-to-hard expert
meta-training strategy to arrange the training tasks properly, where easy tasks
are preferred in the first phase, then, hard tasks are emphasized in the second
phase. A task hardness aware module is designed and integrated into the
training procedure to estimate the hardness of a task based on the
distinguishability of its categories. In addition, we explore multiple hardness
measurements including the semantic relation, the pairwise Euclidean distance,
the Hausdorff distance, and the Hilbert-Schmidt independence criterion.
Experimental results on the miniImageNet and tieredImageNetSketch datasets show
that the meta-learners can obtain better results with our expert training
strategy.
\\ ( https://arxiv.org/abs/2007.06240 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06271
Date: Mon, 13 Jul 2020 09:52:37 GMT   (5627kb,D)

Title: RATT: Recurrent Attention to Transient Tasks for Continual Image
  Captioning
Authors: Riccardo Del Chiaro, Bart{\l}omiej Twardowski, Andrew D. Bagdanov,
  Joost van de Weijer
Categories: cs.CV cs.LG
Comments: 8 pages, 5 figures, to be published in LifelongML workshop at
  ICML2020
\\
  Research on continual learning has led to a variety of approaches to
mitigating catastrophic forgetting in feed-forward classification networks.
Until now surprisingly little attention has been focused on continual learning
of recurrent models applied to problems like image captioning. In this paper we
take a systematic look at continual learning of LSTM-based models for image
captioning. We propose an attention-based approach that explicitly accommodates
the transient nature of vocabularies in continual image captioning tasks --
i.e. that task vocabularies are not disjoint. We call our method Recurrent
Attention to Transient Tasks (RATT), and also show how to adapt continual
learning approaches based on weight egularization and knowledge distillation to
recurrent continual learning problems. We apply our approaches to incremental
image captioning problem on two new continual learning benchmarks we define
using the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is
able to sequentially learn five captioning tasks while incurring no forgetting
of previously learned ones.
\\ ( https://arxiv.org/abs/2007.06271 ,  5627kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06272
Date: Mon, 13 Jul 2020 09:53:20 GMT   (2782kb,D)

Title: Screen Tracking for Clinical Translation of Live Ultrasound Image
  Analysis Methods
Authors: Simona Treivase, Alberto Gomez, Jacqueline Matthew, Emily Skelton,
  Julia A. Schnabel, Nicolas Toussaint
Categories: cs.CV cs.HC
MSC-class: 68T45
\\
  Ultrasound (US) imaging is one of the most commonly used non-invasive imaging
techniques. However, US image acquisition requires simultaneous guidance of the
transducer and interpretation of images, which is a highly challenging task
that requires years of training. Despite many recent developments in
intra-examination US image analysis, the results are not easy to translate to a
clinical setting. We propose a generic framework to extract the US images and
superimpose the results of an analysis task, without any need for physical
connection or alteration to the US system. The proposed method captures the US
image by tracking the screen with a camera fixed at the sonographer's view
point and reformats the captured image to the right aspect ratio, in 87.66 +-
3.73ms on average.
  It is hypothesized that this would enable to input such retrieved image into
an image processing pipeline to extract information that can help improve the
examination. This information could eventually be projected back to the
sonographer's field of view in real time using, for example, an augmented
reality (AR) headset.
\\ ( https://arxiv.org/abs/2007.06272 ,  2782kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06277
Date: Mon, 13 Jul 2020 09:58:14 GMT   (6542kb,D)

Title: OpenStreetMap: Challenges and Opportunities in Machine Learning and
  Remote Sensing
Authors: John Vargas, Shivangi Srivastava, Devis Tuia, Alexandre Falcao
Categories: cs.CV eess.IV
DOI: 10.1109/MGRS.2020.2994107
\\
  OpenStreetMap (OSM) is a community-based, freely available, editable map
service that was created as an alternative to authoritative ones. Given that it
is edited mainly by volunteers with different mapping skills, the completeness
and quality of its annotations are heterogeneous across different geographical
locations. Despite that, OSM has been widely used in several applications in
{Geosciences}, Earth Observation and environmental sciences. In this work, we
present a review of recent methods based on machine learning to improve and use
OSM data. Such methods aim either 1) at improving the coverage and quality of
OSM layers, typically using GIS and remote sensing technologies, or 2) at using
the existing OSM layers to train models based on image data to serve
applications like navigation or {land use} classification. We believe that OSM
(as well as other sources of open land maps) can change the way we interpret
remote sensing data and that the synergy with machine learning can scale
participatory map making and its quality to the level needed to serve global
and up-to-date land mapping.
\\ ( https://arxiv.org/abs/2007.06277 ,  6542kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06279
Date: Mon, 13 Jul 2020 10:00:44 GMT   (390kb,D)

Title: Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for
  Annotation-efficient Cardiac Segmentation
Authors: Kang Li, Shujun Wang, Lequan Yu, and Pheng-Ann Heng
Categories: cs.CV
Comments: Accepted at MICCAI 2020
\\
  Medical image annotations are prohibitively time-consuming and expensive to
obtain. To alleviate annotation scarcity, many approaches have been developed
to efficiently utilize extra information, e.g.,semi-supervised learning further
exploring plentiful unlabeled data, domain adaptation including multi-modality
learning and unsupervised domain adaptation resorting to the prior knowledge
from additional modality. In this paper, we aim to investigate the feasibility
of simultaneously leveraging abundant unlabeled data and well-established
cross-modality data for annotation-efficient medical image segmentation. To
this end, we propose a novel semi-supervised domain adaptation approach, namely
Dual-Teacher, where the student model not only learns from labeled target data
(e.g., CT), but also explores unlabeled target data and labeled source data
(e.g., MR) by two teacher models. Specifically, the student model learns the
knowledge of unlabeled target data from intra-domain teacher by encouraging
prediction consistency, as well as the shape priors embedded in labeled source
data from inter-domain teacher via knowledge distillation. Consequently, the
student model can effectively exploit the information from all three data
resources and comprehensively integrate them to achieve improved performance.
We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate that
our approach is able to concurrently utilize unlabeled data and cross-modality
data with superior performance, outperforming semi-supervised learning and
domain adaptation methods with a large margin.
\\ ( https://arxiv.org/abs/2007.06279 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06288
Date: Mon, 13 Jul 2020 10:15:44 GMT   (2022kb)

Title: Fusing Motion Patterns and Key Visual Information for Semantic Event
  Recognition in Basketball Videos
Authors: Lifang Wu, Zhou Yang, Qi Wang, Meng Jian, Boxuan Zhao, Junchi Yan,
  Chang Wen Chen
Categories: cs.CV
\\
  Many semantic events in team sport activities e.g. basketball often involve
both group activities and the outcome (score or not). Motion patterns can be an
effective means to identify different activities. Global and local motions have
their respective emphasis on different activities, which are difficult to
capture from the optical flow due to the mixture of global and local motions.
Hence it calls for a more effective way to separate the global and local
motions. When it comes to the specific case for basketball game analysis, the
successful score for each round can be reliably detected by the appearance
variation around the basket. Based on the observations, we propose a scheme to
fuse global and local motion patterns (MPs) and key visual information (KVI)
for semantic event recognition in basketball videos. Firstly, an algorithm is
proposed to estimate the global motions from the mixed motions based on the
intrinsic property of camera adjustments. And the local motions could be
obtained from the mixed and global motions. Secondly, a two-stream 3D CNN
framework is utilized for group activity recognition over the separated global
and local motion patterns. Thirdly, the basket is detected and its appearance
features are extracted through a CNN structure. The features are utilized to
predict the success or failure. Finally, the group activity recognition and
success/failure prediction results are integrated using the kronecker product
for event recognition. Experiments on NCAA dataset demonstrate that the
proposed method obtains state-of-the-art performance.
\\ ( https://arxiv.org/abs/2007.06288 ,  2022kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06292
Date: Mon, 13 Jul 2020 10:20:58 GMT   (2720kb,D)

Title: Knowledge Graph Driven Approach to Represent Video Streams for
  Spatiotemporal Event Pattern Matching in Complex Event Processing
Authors: Piyush Yadav, Dhaval Salwala, Edward Curry
Categories: cs.CV cs.AI cs.DB cs.MM
Comments: 31 pages, 14 Figures, Publication accepted in International Journal
  of Graph Computing
\\
  Complex Event Processing (CEP) is an event processing paradigm to perform
real-time analytics over streaming data and match high-level event patterns.
Presently, CEP is limited to process structured data stream. Video streams are
complicated due to their unstructured data model and limit CEP systems to
perform matching over them. This work introduces a graph-based structure for
continuous evolving video streams, which enables the CEP system to query
complex video event patterns. We propose the Video Event Knowledge Graph
(VEKG), a graph driven representation of video data. VEKG models video objects
as nodes and their relationship interaction as edges over time and space. It
creates a semantic knowledge representation of video data derived from the
detection of high-level semantic concepts from the video using an ensemble of
deep learning models. A CEP-based state optimization - VEKG-Time Aggregated
Graph (VEKG-TAG) is proposed over VEKG representation for faster event
detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides
a summarized view of the VEKG graph over a given time length. We defined a set
of nine event pattern rules for two domains (Activity Recognition and Traffic
Management), which act as a query and applied over VEKG graphs to discover
complex event patterns. To show the efficacy of our approach, we performed
extensive experiments over 801 video clips across 10 datasets. The proposed
VEKG approach was compared with other state-of-the-art methods and was able to
detect complex event patterns over videos with F-Score ranging from 0.44 to
0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%
and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,
achieving sub-second median latency of 4-20 milliseconds.
\\ ( https://arxiv.org/abs/2007.06292 ,  2720kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06294
Date: Mon, 13 Jul 2020 10:26:30 GMT   (829kb)

Title: Seeing Eye-to-Eye? A Comparison of Object Recognition Performance in
  Humans and Deep Convolutional Neural Networks under Image Manipulation
Authors: Leonard E. van Dyck and Walter R. Gruber
Categories: cs.CV cs.LG eess.IV q-bio.NC
Comments: 19 pages, 7 figures, 3 tables
\\
  For a considerable time, deep convolutional neural networks (DCNNs) have
reached human benchmark performance in object recognition. On that account,
computational neuroscience and the field of machine learning have started to
attribute numerous similarities and differences to artificial and biological
vision. This study aims towards a behavioral comparison of visual core object
recognition between humans and feedforward neural networks in a classification
learning paradigm on an ImageNet data set. For this purpose, human participants
(n = 65) competed in an online experiment against different feedforward DCNNs.
The designed approach based on a typical learning process of seven different
monkey categories included a training and validation phase with natural
examples, as well as a testing phase with novel shape and color manipulations.
Analyses of accuracy revealed that humans not only outperform DCNNs on all
conditions, but also display significantly greater robustness towards shape and
most notably color alterations. Furthermore, a precise examination of
behavioral patterns highlights these findings by revealing independent
classification errors between the groups. The obtained results endorse an
implementation of recurrent circuits similar to the primate ventral stream in
artificial vision models as a way to achieve adequate object generalization
abilities across unexperienced manipulations.
\\ ( https://arxiv.org/abs/2007.06294 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06309
Date: Mon, 13 Jul 2020 11:03:09 GMT   (7469kb,D)

Title: Part-aware Prototype Network for Few-shot Semantic Segmentation
Authors: Yongfei Liu, Xiangyi Zhang, Songyang Zhang, Xuming He
Categories: cs.CV
Comments: ECCV-2020
\\
  Few-shot semantic segmentation aims to learn to segment new object classes
with only a few annotated examples, which has a wide range of real-world
applications. Most existing methods either focus on the restrictive setting of
one-way few-shot segmentation or suffer from incomplete coverage of object
regions. In this paper, we propose a novel few-shot semantic segmentation
framework based on the prototype representation. Our key idea is to decompose
the holistic class representation into a set of part-aware prototypes, capable
of capturing diverse and fine-grained object features. In addition, we propose
to leverage unlabeled data to enrich our part-aware prototypes, resulting in
better modeling of intra-class variations of semantic objects. We develop a
novel graph neural network model to generate and enhance the proposed
part-aware prototypes based on labeled and unlabeled images. Extensive
experimental evaluations on two benchmarks show that our method outperforms the
prior art with a sizable margin.
\\ ( https://arxiv.org/abs/2007.06309 ,  7469kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06312
Date: Mon, 13 Jul 2020 11:11:17 GMT   (4326kb,D)

Title: Domain aware medical image classifier interpretation by counterfactual
  impact analysis
Authors: Dimitrios Lenis, David Major, Maria Wimmer, Astrid Berg, Gert Sluiter,
  and Katja B\"uhler
Categories: cs.CV cs.LG
Comments: Accepted for publication at International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI) 2020
ACM-class: I.2.6
\\
  The success of machine learning methods for computer vision tasks has driven
a surge in computer assisted prediction for medicine and biology. Based on a
data-driven relationship between input image and pathological classification,
these predictors deliver unprecedented accuracy. Yet, the numerous approaches
trying to explain the causality of this learned relationship have fallen short:
time constraints, coarse, diffuse and at times misleading results, caused by
the employment of heuristic techniques like Gaussian noise and blurring, have
hindered their clinical adoption.
  In this work, we discuss and overcome these obstacles by introducing a
neural-network based attribution method, applicable to any trained predictor.
Our solution identifies salient regions of an input image in a single
forward-pass by measuring the effect of local image-perturbations on a
predictor's score. We replace heuristic techniques with a strong neighborhood
conditioned inpainting approach, avoiding anatomically implausible, hence
adversarial artifacts. We evaluate on public mammography data and compare
against existing state-of-the-art methods. Furthermore, we exemplify the
approach's generalizability by demonstrating results on chest X-rays. Our
solution shows, both quantitatively and qualitatively, a significant reduction
of localization ambiguity and clearer conveying results, without sacrificing
time efficiency.
\\ ( https://arxiv.org/abs/2007.06312 ,  4326kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06317
Date: Mon, 13 Jul 2020 11:24:48 GMT   (5453kb,D)

Title: IntegralAction: Pose-driven Feature Integration for Robust Human Action
  Recognition in Videos
Authors: Gyeongsik Moon, Heeseung Kwon, Kyoung Mu Lee, Minsu Cho
Categories: cs.CV
\\
  Most current action recognition methods heavily rely on appearance
information by taking an RGB sequence of entire image regions as input. While
being effective in exploiting contextual information around humans, e.g., human
appearance and scene category, they are easily fooled by out-of-context action
videos where the contexts do not exactly match with target actions. In
contrast, pose-based methods, which takes a sequence of human skeletons only as
input, suffer from inaccurate pose estimation or ambiguity of human pose per
se. Integrating these two approaches has turned out to be non-trivial; training
a model with both appearance and pose ends up with a strong bias towards
appearance and does not generalize well to unseen videos. To address this
problem, we propose to learn pose-driven feature integration that dynamically
combines appearance and pose streams by observing pose features on the fly. The
main idea is to let the pose stream decide how much and which appearance
information is used in integration based on whether the given pose information
is reliable or not. We show that the proposed IntegralAction achieves highly
robust performance across in-context and out-of-context action video datasets.
\\ ( https://arxiv.org/abs/2007.06317 ,  5453kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06334
Date: Mon, 13 Jul 2020 12:07:25 GMT   (1789kb,D)

Title: Active Crowd Counting with Limited Supervision
Authors: Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, Li Li
Categories: cs.CV
Comments: ECCV2020 camera ready
\\
  To learn a reliable people counter from crowd images, head center annotations
are normally required. Annotating head centers is however a laborious and
tedious process in dense crowds. In this paper, we present an active learning
framework which enables accurate crowd counting with limited supervision: given
a small labeling budget, instead of randomly selecting images to annotate, we
first introduce an active labeling strategy to annotate the most informative
images in the dataset and learn the counting model upon them. The process is
repeated such that in every cycle we select the samples that are diverse in
crowd density and dissimilar to previous selections. In the last cycle when the
labeling budget is met, the large amount of unlabeled data are also utilized: a
distribution classifier is introduced to align the labeled data with unlabeled
data; furthermore, we propose to mix up the distribution labels and latent
representations of data in the network to particularly improve the distribution
alignment in-between training samples. We follow the popular density estimation
pipeline for crowd counting. Extensive experiments are conducted on standard
benchmarks i.e. ShanghaiTech, UCF CC 50, MAll, TRANCOS, and DCC. By annotating
limited number of images (e.g. 10% of the dataset), our method reaches levels
of performance not far from the state of the art which utilize full annotations
of the dataset.
\\ ( https://arxiv.org/abs/2007.06334 ,  1789kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06344
Date: Mon, 13 Jul 2020 12:30:49 GMT   (3231kb,D)

Title: End-to-End Multi-Object Tracking with Global Response Map
Authors: Xingyu Wan, Jiakai Cao, Sanping Zhou, Jinjun Wang
Categories: cs.CV cs.LG
\\
  Most existing Multi-Object Tracking (MOT) approaches follow the
Tracking-by-Detection paradigm and the data association framework where objects
are firstly detected and then associated. Although deep-learning based method
can noticeably improve the object detection performance and also provide good
appearance features for cross-frame association, the framework is not
completely end-to-end, and therefore the computation is huge while the
performance is limited. To address the problem, we present a completely
end-to-end approach that takes image-sequence/video as input and outputs
directly the located and tracked objects of learned types. Specifically, with
our introduced multi-object representation strategy, a global response map can
be accurately generated over frames, from which the trajectory of each tracked
object can be easily picked up, just like how a detector inputs an image and
outputs the bounding boxes of each detected object. The proposed model is fast
and accurate. Experimental results based on the MOT16 and MOT17 benchmarks show
that our proposed on-line tracker achieved state-of-the-art performance on
several tracking metrics.
\\ ( https://arxiv.org/abs/2007.06344 ,  3231kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06355
Date: Mon, 13 Jul 2020 12:59:40 GMT   (5135kb,D)

Title: Multiple Sound Sources Localization from Coarse to Fine
Authors: Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, Weiyao Lin
Categories: cs.CV
Comments: to appear in ECCV 2020
\\
  How to visually localize multiple sound sources in unconstrained videos is a
formidable problem, especially when lack of the pairwise sound-object
annotations. To solve this problem, we develop a two-stage audiovisual learning
framework that disentangles audio and visual representations of different
categories from complex scenes, then performs cross-modal feature alignment in
a coarse-to-fine manner. Our model achieves state-of-the-art results on public
dataset of localization, as well as considerable performance on multi-source
sound localization in complex scenes. We then employ the localization results
for sound separation and obtain comparable performance to existing methods.
These outcomes demonstrate our model's ability in effectively aligning sounds
with specific visual sources.
\\ ( https://arxiv.org/abs/2007.06355 ,  5135kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06356
Date: Mon, 13 Jul 2020 13:05:45 GMT   (2090kb,D)

Title: Disentanglement of Color and Shape Representations for Continual
  Learning
Authors: David Berga, Marc Masana and Joost Van de Weijer
Categories: cs.CV
Comments: Accepted at CL-ICML 2020
\\
  We hypothesize that disentangled feature representations suffer less from
catastrophic forgetting. As a case study we perform explicit disentanglement of
color and shape, by adjusting the network architecture. We tested
classification accuracy and forgetting in a task-incremental setting with
Oxford-102 Flowers dataset. We combine our method with Elastic Weight
Consolidation, Learning without Forgetting, Synaptic Intelligence and Memory
Aware Synapses, and show that feature disentanglement positively impacts
continual learning performance.
\\ ( https://arxiv.org/abs/2007.06356 ,  2090kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06364
Date: Mon, 13 Jul 2020 13:20:32 GMT   (4698kb,D)

Title: On uncertainty estimation in active learning for image segmentation
Authors: Bo Li, Tommy Sonne Alstr{\o}m
Categories: cs.CV cs.LG
Comments: Presented at ICML 2020 Workshop on Uncertainty & Robustness in Deep
  Learning
\\
  Uncertainty estimation is important for interpreting the trustworthiness of
machine learning models in many applications. This is especially critical in
the data-driven active learning setting where the goal is to achieve a certain
accuracy with minimum labeling effort. In such settings, the model learns to
select the most informative unlabeled samples for annotation based on its
estimated uncertainty. The highly uncertain predictions are assumed to be more
informative for improving model performance. In this paper, we explore
uncertainty calibration within an active learning framework for medical image
segmentation, an area where labels often are scarce. Various uncertainty
estimation methods and acquisition strategies (regions and full images) are
investigated. We observe that selecting regions to annotate instead of full
images leads to more well-calibrated models. Additionally, we experimentally
show that annotating regions can cut 50% of pixels that need to be labeled by
humans compared to annotating full images.
\\ ( https://arxiv.org/abs/2007.06364 ,  4698kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06371
Date: Mon, 13 Jul 2020 13:31:38 GMT   (1908kb,D)

Title: Learning and Exploiting Interclass Visual Correlations for Medical Image
  Classification
Authors: Dong Wei, Shilei Cao, Kai Ma, Yefeng Zheng
Categories: cs.CV
\\
  Deep neural network-based medical image classifications often use "hard"
labels for training, where the probability of the correct category is 1 and
those of others are 0. However, these hard targets can drive the networks
over-confident about their predictions and prone to overfit the training data,
affecting model generalization and adaption. Studies have shown that label
smoothing and softening can improve classification performance. Nevertheless,
existing approaches are either non-data-driven or limited in applicability. In
this paper, we present the Class-Correlation Learning Network (CCL-Net) to
learn interclass visual correlations from given training data, and produce soft
labels to help with classification tasks. Instead of letting the network
directly learn the desired correlations, we propose to learn them implicitly
via distance metric learning of class-specific embeddings with a lightweight
plugin CCL block. An intuitive loss based on a geometrical explanation of
correlation is designed for bolstering learning of the interclass correlations.
We further present end-to-end training of the proposed CCL block as a plugin
head together with the classification backbone while generating soft labels on
the fly. Our experimental results on the International Skin Imaging
Collaboration 2018 dataset demonstrate effective learning of the interclass
correlations from training data, as well as consistent improvements in
performance upon several widely used modern network structures with the CCL
block.
\\ ( https://arxiv.org/abs/2007.06371 ,  1908kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06389
Date: Mon, 13 Jul 2020 14:03:10 GMT   (4512kb,D)

Title: Term Revealing: Furthering Quantization at Run Time on Quantized DNNs
Authors: H. T. Kung, Bradley McDanel, Sai Qian Zhang
Categories: cs.CV
Comments: 13 pages, 19 figures, 4 tables, To appear in Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis (SC), 2020
\\
  We present a novel technique, called Term Revealing (TR), for furthering
quantization at run time for improved performance of Deep Neural Networks
(DNNs) already quantized with conventional quantization methods. TR operates on
power-of-two terms in binary expressions of values. In computing a dot-product
computation, TR dynamically selects a fixed number of largest terms to use from
the values of the two vectors in the dot product. By exploiting normal-like
weight and data distributions typically present in DNNs, TR has a minimal
impact on DNN model performance (i.e., accuracy or perplexity). We use TR to
facilitate tightly synchronized processor arrays, such as systolic arrays, for
efficient parallel processing. We show an FPGA implementation that can use a
small number of control bits to switch between conventional quantization and
TR-enabled quantization with a negligible delay. To enhance TR efficiency
further, we propose HESE encoding (Hybrid Encoding for Signed Expressions) of
values, as opposed to classic binary encoding with nonnegative power-of-two
terms. We evaluate TR with HESE encoded values on an MLP for MNIST, multiple
CNNs for ImageNet, and an LSTM for Wikitext-2, and show significant reductions
in inference computations (between 3-10x) compared to conventional quantization
for the same level of model performance.
\\ ( https://arxiv.org/abs/2007.06389 ,  4512kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06392
Date: Mon, 13 Jul 2020 14:15:37 GMT   (8166kb,D)

Title: DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with
  Restricted Computational Resources
Authors: Amir Sharifi, Ahmadreza Zibaei, Mahdi Rezaei
Categories: cs.CV cs.LG cs.RO cs.SY eess.IV eess.SY
\\
  One of the most challenging and non-trivial tasks in robotics-based rescue
operations is Hazardous Materials or HAZMATs sign detection within the
operation field, in order to prevent other unexpected disasters. Each Hazmat
sign has a specific meaning that the rescue robot should detect and interpret
it to take a safe action, accordingly. Accurate Hazmat detection and real-time
processing are the two most important factors in such robotics applications.
Furthermore, we also have to cope with some secondary challengers such as image
distortion problems and restricted CPU and computational resources which are
embedded in a rescue robot. In this paper, we propose a CNN-Based pipeline
called DeepHAZMAT for detecting and segmenting Hazmats in four steps; 1)
optimising the number of input images that are fed into the CNN network, 2)
using the YOLOv3-tiny structure to collect the required visual information from
the hazardous areas, 3) Hazmat sign segmentation and separation from the
background using GrabCut technique, and 4) post-processing the result with
morphological operators and convex hall algorithm. In spite of the utilisation
of a very limited memory and CPU resources, the experimental results show the
proposed method has successfully maintained a better performance in terms of
detection-speed and detection-accuracy, compared with the state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2007.06392 ,  8166kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06402
Date: Mon, 13 Jul 2020 14:27:14 GMT   (5579kb,D)

Title: Nested Learning For Multi-Granular Tasks
Authors: Rapha\"el Achddou, J.Matias di Martino, Guillermo Sapiro
Categories: cs.CV cs.LG stat.ML
\\
  Standard deep neural networks (DNNs) are commonly trained in an end-to-end
fashion for specific tasks such as object recognition, face identification, or
character recognition, among many examples. This specificity often leads to
overconfident models that generalize poorly to samples that are not from the
original training distribution. Moreover, such standard DNNs do not allow to
leverage information from heterogeneously annotated training data, where for
example, labels may be provided with different levels of granularity.
Furthermore, DNNs do not produce results with simultaneous different levels of
confidence for different levels of detail, they are most commonly an all or
nothing approach. To address these challenges, we introduce the concept of
nested learning: how to obtain a hierarchical representation of the input such
that a coarse label can be extracted first, and sequentially refine this
representation, if the sample permits, to obtain successively refined
predictions, all of them with the corresponding confidence. We explicitly
enforce this behavior by creating a sequence of nested information bottlenecks.
Looking at the problem of nested learning from an information theory
perspective, we design a network topology with two important properties. First,
a sequence of low dimensional (nested) feature embeddings are enforced. Then we
show how the explicit combination of nested outputs can improve both the
robustness and the accuracy of finer predictions. Experimental results on
Cifar-10, Cifar-100, MNIST, Fashion-MNIST, Dbpedia, and Plantvillage
demonstrate that nested learning outperforms the same network trained in the
standard end-to-end fashion.
\\ ( https://arxiv.org/abs/2007.06402 ,  5579kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06404
Date: Mon, 13 Jul 2020 14:28:37 GMT   (5797kb,D)

Title: Fashion-IQ 2020 Challenge 2nd Place Team's Solution
Authors: Minchul Shin, Yoonjae Cho, Seongwuk Hong
Categories: cs.CV
Comments: 4 pages, CVPR 2020 Workshop, Fashion IQ Challenge
\\
  This paper is dedicated to team VAA's approach submitted to the Fashion-IQ
challenge in CVPR 2020. Given a pair of the image and the text, we present a
novel multimodal composition method, RTIC, that can effectively combine the
text and the image modalities into a semantic space. We extract the image and
the text features that are encoded by the CNNs and the sequential models (e.g.,
LSTM or GRU), respectively. To emphasize the meaning of the residual of the
feature between the target and candidate, the RTIC is composed of N-blocks with
channel-wise attention modules. Then, we add the encoded residual to the
feature of the candidate image to obtain a synthesized feature. We also
explored an ensemble strategy with variants of models and achieved a
significant boost in performance comparing to the best single model. Finally,
our approach achieved 2nd place in the Fashion-IQ 2020 Challenge with a test
score of 48.02 on the leaderboard.
\\ ( https://arxiv.org/abs/2007.06404 ,  5797kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06426
Date: Mon, 13 Jul 2020 15:00:19 GMT   (1136kb,D)

Title: Multitask Non-Autoregressive Model for Human Motion Prediction
Authors: Bin Li, Jian Tian, Zhongfei Zhang, Hailin Feng, and Xi Li
Categories: cs.CV
\\
  Human motion prediction, which aims at predicting future human skeletons
given the past ones, is a typical sequence-to-sequence problem. Therefore,
extensive efforts have been continued on exploring different RNN-based
encoder-decoder architectures. However, by generating target poses conditioned
on the previously generated ones, these models are prone to bringing issues
such as error accumulation problem. In this paper, we argue that such issue is
mainly caused by adopting autoregressive manner. Hence, a novel
Non-auToregressive Model (NAT) is proposed with a complete non-autoregressive
decoding scheme, as well as a context encoder and a positional encoding module.
More specifically, the context encoder embeds the given poses from temporal and
spatial perspectives. The frame decoder is responsible for predicting each
future pose independently. The positional encoding module injects positional
signal into the model to indicate temporal order. Moreover, a multitask
training paradigm is presented for both low-level human skeleton prediction and
high-level human action recognition, resulting in the convincing improvement
for the prediction task. Our approach is evaluated on Human3.6M and CMU-Mocap
benchmarks and outperforms state-of-the-art autoregressive methods.
\\ ( https://arxiv.org/abs/2007.06426 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06443
Date: Mon, 13 Jul 2020 15:27:33 GMT   (9614kb,D)

Title: Implicit Euler ODE Networks for Single-Image Dehazing
Authors: Jiawei Shen, Zhuoyan Li, Lei Yu, Gui-Song Xia, Wen Yang
Categories: cs.CV
Comments: 10pages, 10 figures, "for the associate project, see
  https://github.com/Jiawei-Shen?tab=repositories", submitted to CVPR workshop
  "vision for four seasons",
ACM-class: F.2.2; I.4.3
\\
  Deep convolutional neural networks (CNN) have been applied for image dehazing
tasks, where the residual network (ResNet) is often adopted as the basic
component to avoid the vanishing gradient problem. Recently, many works
indicate that the ResNet can be considered as the explicit Euler forward
approximation of an ordinary differential equation (ODE). In this paper, we
extend the explicit forward approximation to the implicit backward counterpart,
which can be realized via a recursive neural network, named IM-block. Given
that, we propose an efficient end-to-end multi-level implicit network (MI-Net)
for the single image dehazing problem. Moreover, multi-level fusing (MLF)
mechanism and residual channel attention block (RCA-block) are adopted to boost
performance of our network. Experiments on several dehazing benchmark datasets
demonstrate that our method outperforms existing methods and achieves the
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2007.06443 ,  9614kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06475
Date: Mon, 13 Jul 2020 16:14:41 GMT   (1404kb,D)

Title: Automatic Pass Annotation from Soccer VideoStreams Based on Object
  Detection and LSTM
Authors: Danilo Sorano, Fabio Carrara, Paolo Cintia, Fabrizio Falchi, Luca
  Pappalardo
Categories: cs.CV cs.LG
\\
  Soccer analytics is attracting increasing interest in academia and industry,
thanks to the availability of data that describe all the spatio-temporal events
that occur in each match. These events (e.g., passes, shots, fouls) are
collected by human operators manually, constituting a considerable cost for
data providers in terms of time and economic resources. In this paper, we
describe PassNet, a method to recognize the most frequent events in soccer,
i.e., passes, from video streams. Our model combines a set of artificial neural
networks that perform feature extraction from video streams, object detection
to identify the positions of the ball and the players, and classification of
frame sequences as passes or not passes. We test PassNet on different
scenarios, depending on the similarity of conditions to the match used for
training. Our results show good classification results and significant
improvement in the accuracy of pass detection with respect to baseline
classifiers, even when the match's video conditions of the test and training
sets are considerably different. PassNet is the first step towards an automated
event annotation system that may break the time and the costs for event
annotation, enabling data collections for minor and non-professional divisions,
youth leagues and, in general, competitions whose matches are not currently
annotated by data providers.
\\ ( https://arxiv.org/abs/2007.06475 ,  1404kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06483
Date: Mon, 13 Jul 2020 16:34:05 GMT   (3620kb,D)

Title: Accelerating Translational Image Registration for HDR Images on GPU
Authors: Kadir Cenk Alpay, Kadir Berkay Aydemir, Alptekin Temizel
Categories: cs.CV cs.DC
Comments: Submitted for Consideration for Publication in High Performance
  Computing Conference 2020
\\
  High Dynamic Range (HDR) images are generated using multiple exposures of a
scene. When a hand-held camera is used to capture a static scene, these images
need to be aligned by globally shifting each image in both dimensions. For a
fast and robust alignment, the shift amount is commonly calculated using Median
Threshold Bitmaps (MTB) and creating an image pyramid. In this study, we
optimize these computations using a parallel processing approach utilizing GPU.
Experimental evaluation shows that the proposed implementation achieves a
speed-up of up to 6.24 times over the baseline multi-threaded CPU
implementation on the alignment of one image pair. The source code is available
at https://github.com/kadircenk/WardMTBCuda
\\ ( https://arxiv.org/abs/2007.06483 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06504
Date: Mon, 13 Jul 2020 16:56:27 GMT   (192kb,D)

Title: Towards practical lipreading with distilled and efficient models
Authors: Pingchuan Ma, Brais Martinez, Stavros Petridis, Maja Pantic
Categories: cs.CV
\\
  Lipreading has witnessed a lot of progress due to the resurgence of neural
networks. Recent work has placed emphasis on aspects such as improving
performance by finding the optimal architecture or improving generalization.
However, there is still a significant gap between the current methodologies and
the requirements for an effective deployment of lipreading in practical
scenarios. In this work, we propose a series of innovations that significantly
bridge that gap: first, we raise the state-of-the-art performance by a wide
margin on LRW and LRW-1000 to 88.6% and 46.6%, respectively, through careful
optimization. Secondly, we propose a series of architectural changes, including
a novel depthwise-separable TCN head, that slashes the computational cost to a
fraction of the (already quite efficient) original model. Thirdly, we show that
knowledge distillation is a very effective tool for recovering performance of
the lightweight models. This results in a range of models with different
accuracy-efficiency trade-offs. However, our most promising lightweight models
are on par with the current state-of-the-art while showing a reduction of 8 and
4x in terms of computational cost and number of parameters, respectively, which
we hope will enable the deployment of lipreading models in practical
applications.
\\ ( https://arxiv.org/abs/2007.06504 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06516
Date: Mon, 13 Jul 2020 17:18:21 GMT   (2612kb,D)

Title: Uncertain-DeepSSM: From Images to Probabilistic Shape Models
Authors: Jadie Adams, Riddhish Bhalodia, Shireen Elhabian
Categories: cs.CV
Comments: 16 pages, 7 figures
\\
  Statistical shape modeling (SSM) has recently taken advantage of advances in
deep learning to alleviate the need for a time-consuming and expert-driven
workflow of anatomy segmentation, shape registration, and the optimization of
population-level shape representations. DeepSSM is an end-to-end deep learning
approach that extracts statistical shape representation directly from
unsegmented images with little manual overhead. It performs comparably with
state-of-the-art shape modeling methods for estimating morphologies that are
viable for subsequent downstream tasks. Nonetheless, DeepSSM produces an
overconfident estimate of shape that cannot be blindly assumed to be accurate.
Hence, conveying what DeepSSM does not know, via quantifying granular estimates
of uncertainty, is critical for its direct clinical application as an on-demand
diagnostic tool to determine how trustworthy the model output is. Here, we
propose Uncertain-DeepSSM as a unified model that quantifies both,
data-dependent aleatoric uncertainty by adapting the network to predict
intrinsic input variance, and model-dependent epistemic uncertainty via a Monte
Carlo dropout sampling to approximate a variational distribution over the
network parameters. Experiments show an accuracy improvement over DeepSSM while
maintaining the same benefits of being end-to-end with little pre-processing.
\\ ( https://arxiv.org/abs/2007.06516 ,  2612kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06542
Date: Fri, 10 Jul 2020 03:40:10 GMT   (1320kb,D)

Title: Loss Function Search for Face Recognition
Authors: Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, Tao Mei
Categories: cs.CV
Comments: Accepted by ICML2020. arXiv admin note: substantial text overlap with
  arXiv:1912.00833; text overlap with arXiv:1905.07375 by other authors
\\
  In face recognition, designing margin-based (e.g., angular, additive,
additive angular margins) softmax loss functions plays an important role in
learning discriminative features. However, these hand-crafted heuristic methods
are sub-optimal because they require much effort to explore the large design
space. Recently, an AutoML for loss function search method AM-LFS has been
derived, which leverages reinforcement learning to search loss functions during
the training process. But its search space is complex and unstable that
hindering its superiority. In this paper, we first analyze that the key to
enhance the feature discrimination is actually \textbf{how to reduce the
softmax probability}. We then design a unified formulation for the current
margin-based softmax losses. Accordingly, we define a novel search space and
develop a reward-guided search method to automatically obtain the best
candidate. Experimental results on a variety of face recognition benchmarks
have demonstrated the effectiveness of our method over the state-of-the-art
alternatives.
\\ ( https://arxiv.org/abs/2007.06542 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05630
Date: Fri, 10 Jul 2020 22:14:04 GMT   (41kb,D)

Title: Computing Dense and Sparse Subgraphs of Weakly Closed Graphs
Authors: Tomohiro Koana, Christian Komusiewicz, Frank Sommer
Categories: cs.DM
\\
  A graph $G$ is weakly $\gamma$-closed if every induced subgraph of $G$
contains one vertex~$v$ such that for each non-neighbor $u$ of $v$ it holds
that $|N(u)\cap N(v)|<\gamma$. The weak closure $\gamma(G)$ of a graph,
recently introduced by Fox et al. [SIAM J. Comp. 2020], is the smallest number
such that $G$ is weakly $\gamma$-closed. This graph parameter is never larger
than the degeneracy (plus one) and can be significantly smaller. Extending the
work of Fox et al. [SIAM J. Comp. 2020] on clique enumeration, we show that
several problems related to finding dense subgraphs, such as the enumeration of
bicliques and $s$-plexes, are fixed-parameter tractable with respect to
$\gamma(G)$. Moreover, we show that the problem of determining whether a
weakly-$\gamma$-closed graph $G$ has a subgraph on at least $k$ vertices that
belongs to a graph class $\mathcal{G}$ which is closed under taking subgraphs
admits a kernel with at most $\gamma k^2$ vertices. Finally, we provide
fixed-parameter algorithms for \textsc{Independent Dominating Set} and
\textsc{Dominating Clique} when parameterized by $\gamma+k$ where $k$ is the
solution size.
\\ ( https://arxiv.org/abs/2007.05630 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05861
Date: Sat, 11 Jul 2020 22:00:05 GMT   (475kb,D)

Title: On the complexity of binary polynomial optimization over acyclic
  hypergraphs
Authors: Alberto Del Pia, Silvia Di Gregorio
Categories: cs.DM math.OC
\\
  In this work we consider binary polynomial optimization, which is the problem
of maximizing a given polynomial function over all binary points. We completely
settle the computational complexity of this problem over acyclic hypergraphs.
Our main result is a strongly polynomial-time algorithm for $\beta$-acyclic
hypergraphs. The idea behind our algorithm is to successively remove nest
points from the hypergraph, until there is only one node left. Once we reach
this condition, optimizing the problem becomes trivial. Then, we show that for
$\alpha$-acyclic hypergraphs the problem is strongly NP-hard, and NP-hard to
approximate within a constant factor larger than 16/17 provided that P $\neq$
NP. Our algorithm can also be applied to any general binary polynomial
optimization problem that contains $\beta$-cycles. For these problems, the
algorithm returns a smaller instance together with a rule to extend any optimal
solution of the smaller instance to an optimal solution of the original
instance. We assess the effectiveness of this technique via computational
experiments on random hypergraphs.
\\ ( https://arxiv.org/abs/2007.05861 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05882
Date: Sun, 12 Jul 2020 01:25:33 GMT   (779kb)

Title: Physics Successfully Implements Lagrange Multiplier Optimization
Authors: Sri Krishna Vadlamani, Tianyao Patrick Xiao, Eli Yablonovitch
Categories: cs.ET
\\
  Optimization is a major part of human effort. While being mathematical,
optimization is also built into physics. For example, physics has the principle
of Least Action; the principle of Minimum Entropy Generation; and the
Variational Principle. Physics also has physical annealing which, of course,
preceded computational Simulated Annealing. Physics has the Adiabatic
Principle, which in its quantum form is called Quantum Annealing. Thus,
physical machines can solve the mathematical problem of optimization, including
constraints. Binary constraints can be built into the physical optimization. In
that case the machines are digital in the same sense that a flip-flop is
digital. A wide variety of machines have had recent success at optimizing the
Ising magnetic energy. We demonstrate in this paper that almost all those
machines perform optimization according to the Principle of Minimum Entropy
Generation as put forth by Onsager. Further, we show that this optimization is
in fact equivalent to Lagrange multiplier optimization for constrained
problems. We find that the physical gain coefficients which drive those systems
actually play the role of the corresponding Lagrange Multipliers.
\\ ( https://arxiv.org/abs/2007.05882 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06238
Date: Mon, 13 Jul 2020 08:40:14 GMT   (1696kb,D)

Title: Embracing the Unreliability of Memory Devices for Neuromorphic Computing
Authors: Marc Bocquet, Tifenn Hirtzlin, Jacques-Olivier Klein, Etienne Nowak,
  Elisa Vianello, Jean-Michel Portal and Damien Querlioz
Categories: cs.ET
\\
  The emergence of resistive non-volatile memories opens the way to highly
energy-efficient computation near- or in-memory. However, this type of
computation is not compatible with conventional ECC, and has to deal with
device unreliability. Inspired by the architecture of animal brains, we present
a manufactured differential hybrid CMOS/RRAM memory architecture suitable for
neural network implementation that functions without formal ECC. We also show
that using low-energy but error-prone programming conditions only slightly
reduces network accuracy.
\\ ( https://arxiv.org/abs/2007.06238 ,  1696kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:1902.01926 (*cross-listing*)
Date: Fri, 18 Jan 2019 03:28:44 GMT   (744kb)

Title: IoT Device Fingerprint using Deep Learning
Authors: Sandhya Aneja, Nagender Aneja, Md Shohidul Islam
Categories: cs.NI cs.CV cs.LG
DOI: 10.1109/IOTAIS.2018.8600824
\\
  Device Fingerprinting (DFP) is the identification of a device without using
its network or other assigned identities including IP address, Medium Access
Control (MAC) address, or International Mobile Equipment Identity (IMEI)
number. DFP identifies a device using information from the packets which the
device uses to communicate over the network. Packets are received at a router
and processed to extract the information. In this paper, we worked on the DFP
using Inter Arrival Time (IAT). IAT is the time interval between the two
consecutive packets received. This has been observed that the IAT is unique for
a device because of different hardware and the software used for the device.
The existing work on the DFP uses the statistical techniques to analyze the IAT
and to further generate the information using which a device can be identified
uniquely. This work presents a novel idea of DFP by plotting graphs of IAT for
packets with each graph plotting 100 IATs and subsequently processing the
resulting graphs for the identification of the device. This approach improves
the efficiency to identify a device DFP due to achieved benchmark of the deep
learning libraries in the image processing. We configured Raspberry Pi to work
as a router and installed our packet sniffer application on the Raspberry Pi .
The packet sniffer application captured the packet information from the
connected devices in a log file. We connected two Apple devices iPad4 and
iPhone 7 Plus to the router and created IAT graphs for these two devices. We
used Convolution Neural Network (CNN) to identify the devices and observed the
accuracy of 86.7%.
\\ ( https://arxiv.org/abs/1902.01926 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05592 (*cross-listing*)
Date: Sun, 5 Jul 2020 08:25:37 GMT   (4394kb,D)

Title: Experiments of Federated Learning for COVID-19 Chest X-ray Images
Authors: Boyi Liu, Bingjie Yan, Yize Zhou, Yifan Yang, Yixian Zhang
Categories: eess.IV cs.CV cs.LG
\\
  AI plays an important role in COVID-19 identification. Computer vision and
deep learning techniques can assist in determining COVID-19 infection with
Chest X-ray Images. However, for the protection and respect of the privacy of
patients, the hospital's specific medical-related data did not allow leakage
and sharing without permission. Collecting such training data was a major
challenge. To a certain extent, this has caused a lack of sufficient data
samples when performing deep learning approaches to detect COVID-19. Federated
Learning is an available way to address this issue. It can effectively address
the issue of data silos and get a shared model without obtaining local data. In
the work, we propose the use of federated learning for COVID-19 data training
and deploy experiments to verify the effectiveness. And we also compare
performances of four popular models (MobileNet, ResNet18, MoblieNet, and
COVID-Net) with the federated learning framework and without the framework.
This work aims to inspire more researches on federated learning about COVID-19.
\\ ( https://arxiv.org/abs/2007.05592 ,  4394kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05597 (*cross-listing*)
Date: Fri, 10 Jul 2020 20:19:01 GMT   (5844kb,D)

Title: EMIXER: End-to-end Multimodal X-ray Generation via Self-supervision
Authors: Siddharth Biswal, Peiye Zhuang, Ayis Pyrros, Nasir Siddiqui, Sanmi
  Koyejo, Jimeng Sun
Categories: eess.IV cs.CV cs.LG
\\
  Deep generative models have enabled the automated synthesis of high-quality
data for diverse applications. However, the most effective generative models
are specialized to data from a single domain (e.g., images or text). Real-world
applications such as healthcare require multi-modal data from multiple domains
(e.g., both images and corresponding text), which are difficult to acquire due
to limited availability and privacy concerns and are much harder to synthesize.
To tackle this joint synthesis challenge, we propose an End-to-end MultImodal
X-ray genERative model (EMIXER) for jointly synthesizing x-ray images and
corresponding free-text reports, all conditional on diagnosis labels. EMIXER is
an conditional generative adversarial model by 1) generating an image based on
a label, 2) encoding the image to a hidden embedding, 3) producing the
corresponding text via a hierarchical decoder from the image embedding, and 4)
a joint discriminator for assessing both the image and the corresponding text.
EMIXER also enables self-supervision to leverage vast amount of unlabeled data.
Extensive experiments with real X-ray reports data illustrate how data
augmentation using synthesized multimodal samples can improve the performance
of a variety of supervised tasks including COVID-19 X-ray classification with
very limited samples. The quality of generated images and reports are also
confirmed by radiologists. We quantitatively show that EMIXER generated
synthetic datasets can augment X-ray image classification, report generation
models to achieve 5.94% and 6.9% improvement on models trained only on real
data samples. Taken together, our results highlight the promise of state of
generative models to advance clinical machine learning.
\\ ( https://arxiv.org/abs/2007.05597 ,  5844kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05606 (*cross-listing*)
Date: Fri, 10 Jul 2020 20:54:42 GMT   (721kb)

Title: Neuromorphic Processing and Sensing: Evolutionary Progression of AI to
  Spiking
Authors: Philippe Reiter, Geet Rose Jose, Spyridon Bizmpikis, Ionela-Ancu\c{t}a
  C\^irjil\u{a}
Categories: cs.NE cs.AI cs.CV cs.LG
Comments: 15 pages, 13 figures
\\
  The increasing rise in machine learning and deep learning applications is
requiring ever more computational resources to successfully meet the growing
demands of an always-connected, automated world. Neuromorphic technologies
based on Spiking Neural Network algorithms hold the promise to implement
advanced artificial intelligence using a fraction of the computations and power
requirements by modeling the functioning, and spiking, of the human brain. With
the proliferation of tools and platforms aiding data scientists and machine
learning engineers to develop the latest innovations in artificial and deep
neural networks, a transition to a new paradigm will require building from the
current well-established foundations. This paper explains the theoretical
workings of neuromorphic technologies based on spikes, and overviews the
state-of-art in hardware processors, software platforms and neuromorphic
sensing devices. A progression path is paved for current machine learning
specialists to update their skillset, as well as classification or predictive
models from the current generation of deep neural networks to SNNs. This can be
achieved by leveraging existing, specialized hardware in the form of SpiNNaker
and the Nengo migration toolkit. First-hand, experimental results of converting
a VGG-16 neural network to an SNN are shared. A forward gaze into industrial,
medical and commercial applications that can readily benefit from SNNs wraps up
this investigation into the neuromorphic computing future.
\\ ( https://arxiv.org/abs/2007.05606 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05610 (*cross-listing*)
Date: Fri, 10 Jul 2020 21:07:51 GMT   (2800kb,D)

Title: Batch-Incremental Triplet Sampling for Training Triplet Networks Using
  Bayesian Updating Theorem
Authors: Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H.R.
  Tizhoosh
Categories: stat.ML cs.CV cs.LG eess.IV
Comments: The first two authors contributed equally to this work
\\
  Variants of Triplet networks are robust entities for learning a
discriminative embedding subspace. There exist different triplet mining
approaches for selecting the most suitable training triplets. Some of these
mining methods rely on the extreme distances between instances, and some others
make use of sampling. However, sampling from stochastic distributions of data
rather than sampling merely from the existing embedding instances can provide
more discriminative information. In this work, we sample triplets from
distributions of data rather than from existing instances. We consider a
multivariate normal distribution for the embedding of each class. Using
Bayesian updating and conjugate priors, we update the distributions of classes
dynamically by receiving the new mini-batches of training data. The proposed
triplet mining with Bayesian updating can be used with any triplet-based loss
function, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss.
Accordingly, Our triplet mining approaches are called Bayesian Updating Triplet
(BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is
being used. Experimental results on two public datasets, namely MNIST and
histopathology colorectal cancer (CRC), substantiate the effectiveness of the
proposed triplet mining method.
\\ ( https://arxiv.org/abs/2007.05610 ,  2800kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05742 (*cross-listing*)
Date: Sat, 11 Jul 2020 10:57:45 GMT   (2622kb,D)

Title: Relation-Guided Representation Learning
Authors: Zhao Kang and Xiao Lu and Jian Liang and Kun Bai and Zenglin Xu
Categories: cs.LG cs.CV stat.ML
Comments: Appear in Neural Networks
\\
  Deep auto-encoders (DAEs) have achieved great success in learning data
representations via the powerful representability of neural networks. But most
DAEs only focus on the most dominant structures which are able to reconstruct
the data from a latent space and neglect rich latent structural information. In
this work, we propose a new representation learning method that explicitly
models and leverages sample relations, which in turn is used as supervision to
guide the representation learning. Different from previous work, our framework
well preserves the relations between samples. Since the prediction of pairwise
relations themselves is a fundamental problem, our model adaptively learns them
from data. This provides much flexibility to encode real data manifold. The
important role of relation and representation learning is evaluated on the
clustering task. Extensive experiments on benchmark data sets demonstrate the
superiority of our approach. By seeking to embed samples into subspace, we
further show that our method can address the large-scale and out-of-sample
problem.
\\ ( https://arxiv.org/abs/2007.05742 ,  2622kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05785 (*cross-listing*)
Date: Sat, 11 Jul 2020 14:35:42 GMT   (12kb)

Title: Leaky Integrate-and-Fire Spiking Neuron with Learnable Membrane Time
  Parameter
Authors: Wei Fang
Categories: cs.NE cs.CV cs.LG
\\
  The Spiking Neural Networks (SNNs) have attracted research interest due to
its temporal information processing capability, low power consumption, and high
biological plausibility. The Leaky Integrate-and-Fire (LIF) neuron model is one
of the most popular spiking neuron models used in SNNs for it achieves a
balance between computing cost and biological plausibility. The most important
parameter of a LIF neuron is the membrane time constant $\tau$, which
determines the decay rate of membrane potential. The value of $\tau$ plays a
crucial role in SNNs containing LIF neurons. However, $\tau$ is usually treated
as a hyper-parameter, which is preset before training SNNs and adjusted
manually. In this article, we propose a novel spiking neuron, namely parametric
Leaky Integrate-and-Fire (PLIF) neuron, whose $\tau$ is a learnable parameter
rather than an empirical hyper-parameter. We evaluate the performance of SNNs
with PLIF neurons for image classification tasks on both traditional static
MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS
datasets. The experiment results show that SNNs augmented by PLIF neurons
outperform those with conventional spiking neurons.
\\ ( https://arxiv.org/abs/2007.05785 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05791 (*cross-listing*)
Date: Sat, 11 Jul 2020 15:04:28 GMT   (5005kb,D)

Title: Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast
  Cancer Risk Models
Authors: Yue Liu, Hossein Azizpour, Fredrik Strand, Kevin Smith
Categories: eess.IV cs.CV cs.LG
Comments: Medical Image Computing and Computer Assisted Interventions 2020
\\
  The ability to accurately estimate risk of developing breast cancer would be
invaluable for clinical decision-making. One promising new approach is to
integrate image-based risk models based on deep neural networks. However, one
must take care when using such models, as selection of training data influences
the patterns the network will learn to identify. With this in mind, we trained
networks using three different criteria to select the positive training data
(i.e. images from patients that will develop cancer): an inherent risk model
trained on images with no visible signs of cancer, a cancer signs model trained
on images containing cancer or early signs of cancer, and a conflated model
trained on all images from patients with a cancer diagnosis. We find that these
three models learn distinctive features that focus on different patterns, which
translates to contrasts in performance. Short-term risk is best estimated by
the cancer signs model, whilst long-term risk is best estimated by the inherent
risk model. Carelessly training with all images conflates inherent risk with
early cancer signs, and yields sub-optimal estimates in both regimes. As a
consequence, conflated models may lead physicians to recommend preventative
action when early cancer signs are already visible.
\\ ( https://arxiv.org/abs/2007.05791 ,  5005kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05828 (*cross-listing*)
Date: Sat, 11 Jul 2020 18:41:47 GMT   (9744kb,D)

Title: Understanding Object Detection Through An Adversarial Lens
Authors: Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei,
  Yanzhao Wu
Categories: cs.CR cs.CV cs.LG
\\
  Deep neural networks based object detection models have revolutionized
computer vision and fueled the development of a wide range of visual
recognition applications. However, recent studies have revealed that deep
object detectors can be compromised under adversarial attacks, causing a victim
detector to detect no object, fake objects, or mislabeled objects. With object
detection being used pervasively in many security-critical applications, such
as autonomous vehicles and smart cities, we argue that a holistic approach for
an in-depth understanding of adversarial attacks and vulnerabilities of deep
object detection systems is of utmost importance for the research community to
develop robust defense mechanisms. This paper presents a framework for
analyzing and evaluating vulnerabilities of the state-of-the-art object
detectors under an adversarial lens, aiming to analyze and demystify the attack
strategies, adverse effects, and costs, as well as the cross-model and
cross-resolution transferability of attacks. Using a set of quantitative
metrics, extensive experiments are performed on six representative deep object
detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two
benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed
framework can serve as a methodical benchmark for analyzing adversarial
behaviors and risks in real-time object detection systems. We conjecture that
this framework can also serve as a tool to assess the security risks and the
adversarial robustness of deep object detectors to be deployed in real-world
applications.
\\ ( https://arxiv.org/abs/2007.05828 ,  9744kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05830 (*cross-listing*)
Date: Sat, 11 Jul 2020 19:00:45 GMT   (2081kb,D)

Title: AutoEmbedder: A semi-supervised DNN embedding system for clustering
Authors: Abu Quwsar Ohi, M. F. Mridha, Farisa Benta Safir, Md. Abdul Hamid,
  Muhammad Mostafa Monowar
Categories: cs.LG cs.CV stat.ML
Comments: The manuscript is accepted and published in Knowledge-Based System
Journal-ref: Knowledge-Based Systems, p.106190 (2020)
DOI: 10.1016/j.knosys.2020.106190
\\
  Clustering is widely used in unsupervised learning method that deals with
unlabeled data. Deep clustering has become a popular study area that relates
clustering with Deep Neural Network (DNN) architecture. Deep clustering method
downsamples high dimensional data, which may also relate clustering loss. Deep
clustering is also introduced in semi-supervised learning (SSL). Most SSL
methods depend on pairwise constraint information, which is a matrix containing
knowledge if data pairs can be in the same cluster or not. This paper
introduces a novel embedding system named AutoEmbedder, that downsamples higher
dimensional data to clusterable embedding points. To the best of our knowledge,
this is the first research endeavor that relates to traditional classifier DNN
architecture with a pairwise loss reduction technique. The training process is
semi-supervised and uses Siamese network architecture to compute pairwise
constraint loss in the feature learning phase. The AutoEmbedder outperforms
most of the existing DNN based semi-supervised methods tested on famous
datasets.
\\ ( https://arxiv.org/abs/2007.05830 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05840 (*cross-listing*)
Date: Sat, 11 Jul 2020 19:46:18 GMT   (1206kb,D)

Title: Representation Learning via Adversarially-Contrastive Optimal Transport
Authors: Anoop Cherian, Shuchin Aeron
Categories: cs.LG cs.CV stat.ML
Comments: Accepted at ICML 2020
\\
  In this paper, we study the problem of learning compact (low-dimensional)
representations for sequential data that captures its implicit spatio-temporal
cues. To maximize extraction of such informative cues from the data, we set the
problem within the context of contrastive representation learning and to that
end propose a novel objective via optimal transport. Specifically, our
formulation seeks a low-dimensional subspace representation of the data that
jointly (i) maximizes the distance of the data (embedded in this subspace) from
an adversarial data distribution under the optimal transport, a.k.a. the
Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the
data distortion. To generate the adversarial distribution, we propose a novel
framework connecting Wasserstein GANs with a classifier, allowing a principled
mechanism for producing good negative distributions for contrastive learning,
which is currently a challenging problem. Our full objective is cast as a
subspace learning problem on the Grassmann manifold and solved via Riemannian
optimization. To empirically study our formulation, we provide experiments on
the task of human action recognition in video sequences. Our results
demonstrate competitive performance against challenging baselines.
\\ ( https://arxiv.org/abs/2007.05840 ,  1206kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05993 (*cross-listing*)
Date: Sun, 12 Jul 2020 13:58:07 GMT   (1688kb,D)

Title: Deep Network Interpolation for Accelerated Parallel MR Image
  Reconstruction
Authors: Chen Qin, Jo Schlemper, Kerstin Hammernik, Jinming Duan, Ronald M
  Summers, and Daniel Rueckert
Categories: eess.IV cs.CV
Comments: Presented at 2020 ISMRM Conference & Exhibition (Abstract #4958)
\\
  We present a deep network interpolation strategy for accelerated parallel MR
image reconstruction. In particular, we examine the network interpolation in
parameter space between a source model that is formulated in an unrolled scheme
with L1 and SSIM losses and its counterpart that is trained with an adversarial
loss. We show that by interpolating between the two different models of the
same network structure, the new interpolated network can model a trade-off
between perceptual quality and fidelity.
\\ ( https://arxiv.org/abs/2007.05993 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06059 (*cross-listing*)
Date: Sun, 12 Jul 2020 18:25:17 GMT   (2909kb,D)

Title: It Is Likely That Your Loss Should be a Likelihood
Authors: Mark Hamilton, Evan Shelhamer, William T. Freeman
Categories: cs.LG cs.CV stat.ML
\\
  We recall that certain common losses are simplified likelihoods and instead
argue for optimizing full likelihoods that include their parameters, such as
the variance of the normal distribution and the temperature of the softmax
distribution. Joint optimization of likelihood and model parameters can
adaptively tune the scales and shapes of losses and the weights of
regularizers. We survey and systematically evaluate how to parameterize and
apply likelihood parameters for robust modeling and re-calibration.
Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting
the scale parameters of normal and Laplace priors and introduce more flexible
element-wise regularizers.
\\ ( https://arxiv.org/abs/2007.06059 ,  2909kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06063 (*cross-listing*)
Date: Sun, 12 Jul 2020 18:33:09 GMT   (5357kb,D)

Title: Exploiting Uncertainties from Ensemble Learners to Improve
  Decision-Making in Healthcare AI
Authors: Yingshui Tan, Baihong Jin, Xiangyu Yue, Yuxin Chen, Alberto
  Sangiovanni Vincentelli
Categories: cs.LG cs.CV stat.ML
Comments: Preprint of submission to NeurIPS 2020
\\
  Ensemble learning is widely applied in Machine Learning (ML) to improve model
performance and to mitigate decision risks. In this approach, predictions from
a diverse set of learners are combined to obtain a joint decision. Recently,
various methods have been explored in literature for estimating decision
uncertainties using ensemble learning; however, determining which metrics are a
better fit for certain decision-making applications remains a challenging task.
In this paper, we study the following key research question in the selection of
uncertainty metrics: when does an uncertainty metric outperforms another? We
answer this question via a rigorous analysis of two commonly used uncertainty
metrics in ensemble learning, namely ensemble mean and ensemble variance. We
show that, under mild assumptions on the ensemble learners, ensemble mean is
preferable with respect to ensemble variance as an uncertainty metric for
decision making. We empirically validate our assumptions and theoretical
results via an extensive case study: the diagnosis of referable diabetic
retinopathy.
\\ ( https://arxiv.org/abs/2007.06063 ,  5357kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06151 (*cross-listing*)
Date: Mon, 13 Jul 2020 02:02:00 GMT   (428kb,D)

Title: MS-NAS: Multi-Scale Neural Architecture Search for Medical Image
  Segmentation
Authors: Xingang Yan, Weiwen Jiang, Yiyu Shi, and Cheng Zhuo
Categories: eess.IV cs.CV
\\
  The recent breakthroughs of Neural Architecture Search (NAS) have motivated
various applications in medical image segmentation. However, most existing work
either simply rely on hyper-parameter tuning or stick to a fixed network
backbone, thereby limiting the underlying search space to identify more
efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS)
framework that is featured with multi-scale search space from network backbone
to cell operation, and multi-scale fusion capability to fuse features with
different sizes. To mitigate the computational overhead due to the larger
search space, a partial channel connection scheme and a two-step decoding
method are utilized to reduce computational overhead while maintaining
optimization quality. Experimental results show that on various datasets for
segmentation, MS-NAS outperforms the state-of-the-art methods and achieves
0.6-5.4% mIOU and 0.4-3.5% DSC improvements, while the computational resource
consumption is reduced by 18.0-24.9%.
\\ ( https://arxiv.org/abs/2007.06151 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06199 (*cross-listing*)
Date: Mon, 13 Jul 2020 05:37:00 GMT   (4539kb,D)

Title: CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic
  Transformations of Chest X-rays for Benchmarking Deep Learning Robustness
Authors: Nick A. Phillips, Pranav Rajpurkar, Mark Sabini, Rayan Krishnan,
  Sharon Zhou, Anuj Pareek, Nguyet Minh Phu, Chris Wang, Andrew Y. Ng, Matthew
  P. Lungren
Categories: eess.IV cs.CV cs.LG
\\
  Clinical deployment of deep learning algorithms for chest x-ray
interpretation requires a solution that can integrate into the vast spectrum of
clinical workflows across the world. An appealing solution to scaled deployment
is to leverage the existing ubiquity of smartphones: in several parts of the
world, clinicians and radiologists capture photos of chest x-rays to share with
other experts or clinicians via smartphone using messaging services like
WhatsApp. However, the application of chest x-ray algorithms to photos of chest
x-rays requires reliable classification in the presence of smartphone photo
artifacts such as screen glare and poor viewing angle not typically encountered
on digital x-rays used to train machine learning models. We introduce
CheXphoto, a dataset of smartphone photos and synthetic photographic
transformations of chest x-rays sampled from the CheXpert dataset. To generate
CheXphoto we (1) automatically and manually captured photos of digital x-rays
under different settings, including various lighting conditions and locations,
and, (2) generated synthetic transformations of digital x-rays targeted to make
them look like photos of digital x-rays and x-ray films. We release this
dataset as a resource for testing and improving the robustness of deep learning
algorithms for automated chest x-ray interpretation on smartphone photos of
chest x-rays.
\\ ( https://arxiv.org/abs/2007.06199 ,  4539kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06289 (*cross-listing*)
Date: Mon, 13 Jul 2020 10:16:54 GMT   (1205kb,D)

Title: Accelerated FBP for computed tomography image reconstruction
Authors: Anastasiya Dolmatova, Marina Chukalina and Dmitry Nikolaev
Categories: eess.IV cs.CV
\\
  Filtered back projection (FBP) is a commonly used technique in tomographic
image reconstruction demonstrating acceptable quality. The classical direct
implementations of this algorithm require the execution of $\Theta(N^3)$
operations, where $N$ is the linear size of the 2D slice. Recent approaches
including reconstruction via the Fourier slice theorem require $\Theta(N^2\log
N)$ multiplication operations. In this paper, we propose a novel approach that
reduces the computational complexity of the algorithm to $\Theta(N^2\log N)$
addition operations avoiding Fourier space. For speeding up the convolution,
ramp filter is approximated by a pair of causal and anticausal recursive
filters, also known as Infinite Impulse Response filters. The back projection
is performed with the fast discrete Hough transform. Experimental results on
simulated data demonstrate the efficiency of the proposed approach.
\\ ( https://arxiv.org/abs/2007.06289 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06341 (*cross-listing*)
Date: Mon, 13 Jul 2020 12:19:03 GMT   (1744kb,D)

Title: DeU-Net: Deformable U-Net for 3D Cardiac MRI Video Segmentation
Authors: Shunjie Dong, Jinlong Zhao, Maojun Zhang, Zhengxue Shi, Jianing Deng,
  Yiyu Shi, Mei Tian, Cheng Zhuo
Categories: eess.IV cs.CV
\\
  Automatic segmentation of cardiac magnetic resonance imaging (MRI)
facilitates efficient and accurate volume measurement in clinical applications.
However, due to anisotropic resolution and ambiguous border (e.g., right
ventricular endocardium), existing methods suffer from the degradation of
accuracy and robustness in 3D cardiac MRI video segmentation. In this paper, we
propose a novel Deformable U-Net (DeU-Net) to fully exploit spatio-temporal
information from 3D cardiac MRI video, including a Temporal Deformable
Aggregation Module (TDAM) and a Deformable Global Position Attention (DGPA)
network. First, the TDAM takes a cardiac MRI video clip as input with temporal
information extracted by an offset prediction network. Then we fuse extracted
temporal information via a temporal aggregation deformable convolution to
produce fused feature maps. Furthermore, to aggregate meaningful features, we
devise the DGPA network by employing deformable attention U-Net, which can
encode a wider range of multi-dimensional contextual information into global
and local features. Experimental results show that our DeU-Net achieves the
state-of-the-art performance on commonly used evaluation metrics, especially
for cardiac marginal information (ASSD and HD).
\\ ( https://arxiv.org/abs/2007.06341 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06346 (*cross-listing*)
Date: Mon, 13 Jul 2020 12:33:25 GMT   (1022kb,D)

Title: Whitening for Self-Supervised Representation Learning
Authors: Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe
Categories: cs.LG cs.CV stat.ML
\\
  Recent literature on self-supervised learning is based on the contrastive
loss, where image instances which share the same semantic content ("positives")
are contrasted with instances extracted from other images ("negatives").
However, in order for the learning to be effective, a lot of negatives should
be compared with a positive pair. This is not only computationally demanding,
but it also requires that the positive and the negative representations are
kept consistent with each other over a long training period. In this paper we
propose a different direction and a new loss function for self-supervised
learning which is based on the whitening of the latent-space features. The
whitening operation has a "scattering" effect on the batch samples, which
compensates the lack of a large number of negatives, avoiding degenerate
solutions where all the sample representations collapse to a single point. We
empirically show that our loss accelerates self-supervised training and the
learned representations are much more effective for downstream tasks than
previously published work.
\\ ( https://arxiv.org/abs/2007.06346 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06492 (*cross-listing*)
Date: Fri, 10 Jul 2020 06:03:55 GMT   (638kb)

Title: Single Image Dehazing Algorithm Based on Sky Region Segmentation
Authors: Weixiang Li, Wei Jie, Somaiyeh MahmoudZadeh
Categories: eess.IV cs.CV
Journal-ref: International Conference on Advanced Data Mining and Applications,
  2019
DOI: 10.1007/978-3-030-35231-8_35
\\
  In this paper a hybrid image defogging approach based on region segmentation
is proposed to address the dark channel priori algorithm's shortcomings in
de-fogging the sky regions. The preliminary stage of the proposed approach
focuses on the segmentation of sky and non-sky regions in a foggy image taking
the advantageous of Meanshift and edge detection with embedded confidence. In
the second stage, an improved dark channel priori algorithm is employed to
defog the non-sky region. Ultimately, the sky area is processed by DehazeNet
algorithm, which relies on deep learning Convolutional Neural Networks. The
simulation results show that the proposed hybrid approach in this research
addresses the problem of color distortion associated with sky regions in foggy
images. The approach greatly improves the image quality indices including
entropy information, visibility ratio of the edges, average gradient, and the
saturation percentage with a very fast computation time, which is a good
indication of the excellent performance of this model.
\\ ( https://arxiv.org/abs/2007.06492 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06537 (*cross-listing*)
Date: Fri, 10 Jul 2020 11:23:14 GMT   (1331kb,D)

Title: Blockchain-Federated-Learning and Deep Learning Models for COVID-19
  detection using CT Imaging
Authors: Rajesh Kumar, Abdullah Aman Khan, Sinmin Zhang, WenYong Wang, Yousif
  Abuidris, Waqas Amin, and Jay Kumar
Categories: eess.IV cs.CV cs.LG
Comments: arXiv admin note: text overlap with arXiv:2003.10849 by other authors
\\
  With the increase of COVID-19 cases worldwide, an effective way is required
to diagnose COVID-19 patients. The primary problem in diagnosing COVID-19
patients is the shortage of testing kits, due to the quick spread of the virus,
medical practitioners are facing difficulty identifying the positive cases. The
second real-world problem is to share the data among the hospitals globally
while keeping in view the privacy concern of the organizations. To address the
problem of building a collaborative network model without leakage privacy of
data are major concerns for training the deep learning model, this paper
proposes a framework that collects a huge amount of data from different sources
(various hospitals) and to train the deep learning model over a decentralized
network for the newest information about COVID-19 patients. The main goal of
this paper is to improve the recognition of a global deep learning model using,
novel and up-to-date data, and learn itself from such data to improve
recognition of COVID-19 patients based on computed tomography (CT) slices.
Moreover, the integration of blockchain and federated-learning technology
collects the data from different hospitals without leakage the privacy of the
data. Firstly, we collect real-life COVID-19 patients data open to the research
community. Secondly, we use various deep learning models (VGG, DenseNet,
AlexNet, MobileNet, ResNet, and Capsule Network) to recognize the patterns via
COVID-19 patients' lung screening. Thirdly, securely share the data among
various hospitals with the integration of federated learning and blockchain.
Finally, our results demonstrate a better performance to detect COVID-19
patients.
\\ ( https://arxiv.org/abs/2007.06537 ,  1331kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06544 (*cross-listing*)
Date: Mon, 13 Jul 2020 17:49:30 GMT   (2430kb)

Title: Free-running SIMilarity-Based Angiography (SIMBA) for simplified
  anatomical MR imaging of the heart
Authors: John Heerfordt, Kevin K. Whitehead, Jessica A.M. Bastiaansen, Lorenzo
  Di Sopra, Christopher W. Roy, J\'er\^ome Yerly, Bastien Milani, Mark A.
  Fogel, Matthias Stuber, Davide Piccini
Categories: eess.IV cs.CV physics.med-ph
Comments: 8 figures, 2 tables
\\
  Purpose: Whole-heart MRA techniques typically target pre-determined motion
states and address cardiac and respiratory dynamics independently. We propose a
novel fast reconstruction algorithm, applicable to ungated free-running
sequences, that leverages inherent similarities in the acquired data to avoid
such physiological constraints.
  Theory and Methods: The proposed SIMilarity-Based Angiography (SIMBA) method
clusters the continuously acquired k-space data in order to find a
motion-consistent subset that can be reconstructed into a motion-suppressed
whole-heart MRA. Free-running 3D radial datasets from six ferumoxytol-enhanced
scans of pediatric cardiac patients and twelve non-contrast scans of healthy
volunteers were reconstructed with a non-motion-suppressed regridding of all
the acquired data (All Data), our proposed SIMBA method, and a previously
published free-running framework (FRF) that uses cardiac and respiratory
self-gating and compressed sensing. Images were compared for blood-myocardium
interface sharpness, contrast ratio, and visibility of coronary artery ostia.
  Results: Both the fast SIMBA reconstruction (~20s) and the FRF provided
significantly higher blood-myocardium sharpness than All Data (P<0.001). No
significant difference was observed among the former two. Significantly higher
blood-myocardium contrast ratio was obtained with SIMBA compared to All Data
and FRF (P<0.01). More coronary ostia could be visualized with both SIMBA and
FRF than with All Data (All Data: 4/36, SIMBA: 30/36, FRF: 33/36, both P<0.001)
but no significant difference was found between the first two.
  Conclusion: The combination of free-running sequences and the fast SIMBA
reconstruction, which operates without a priori assumptions related to
physiological motion, forms a simple workflow for obtaining whole-heart MRA
with sharp anatomical structures.
\\ ( https://arxiv.org/abs/2007.06544 ,  2430kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06559 (*cross-listing*)
Date: Mon, 13 Jul 2020 17:59:31 GMT   (16165kb,D)

Title: Graph Structure of Neural Networks
Authors: Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie
Categories: cs.LG cs.CV cs.SI stat.ML
Comments: ICML 2020
\\
  Neural networks are often represented as graphs of connections between
neurons. However, despite their wide use, there is currently little
understanding of the relationship between the graph structure of the neural
network and its predictive performance. Here we systematically investigate how
does the graph structure of neural networks affect their predictive
performance. To this end, we develop a novel graph-based representation of
neural networks called relational graph, where layers of neural network
computation correspond to rounds of message exchange along the graph structure.
Using this representation we show that: (1) a "sweet spot" of relational graphs
leads to neural networks with significantly improved predictive performance;
(2) neural network's performance is approximately a smooth function of the
clustering coefficient and average path length of its relational graph; (3) our
findings are consistent across many different tasks and datasets; (4) the sweet
spot can be identified efficiently; (5) top-performing neural networks have
graph structure surprisingly similar to those of real biological neural
networks. Our work opens new directions for the design of neural architectures
and the understanding on neural networks in general.
\\ ( https://arxiv.org/abs/2007.06559 ,  16165kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05634 (*cross-listing*)
Date: Fri, 10 Jul 2020 22:25:06 GMT   (387kb)

Title: Vector Balancing in Lebesgue Spaces
Authors: Victor Reis, Thomas Rothvoss
Categories: cs.DS cs.DM
Comments: 19 pages
\\
  A tantalizing conjecture in discrete mathematics is the one of Koml\'os,
suggesting that for any vectors $\mathbf{a}_1,\ldots,\mathbf{a}_n \in B_2^m$
there exist signs $x_1, \dots, x_n \in \{ -1,1\}$ so that $\|\sum_{i=1}^n
x_i\mathbf{a}_i\|_\infty \le O(1)$. It is a natural extension to ask what
$\ell_q$-norm bound to expect for $\mathbf{a}_1,\ldots,\mathbf{a}_n \in B_p^m$.
We prove that, for $2 \le p \le q \le \infty$, such vectors admit fractional
colorings $x_1, \dots, x_n \in [-1,1]$ with a linear number of $\pm 1$
coordinates so that $\|\sum_{i=1}^n x_i\mathbf{a}_i\|_q \leq
O(\sqrt{\min(p,\log(2m/n))}) \cdot n^{1/2-1/p+ 1/q}$, and that one can obtain a
full coloring at the expense of another factor of $\frac{1}{1/2 - 1/p + 1/q}$.
In particular, for $p \in (2,3]$ we can indeed find signs $\mathbf{x} \in \{
-1,1\}^n$ with $\|\sum_{i=1}^n x_i\mathbf{a}_i\|_\infty \le O(n^{1/2-1/p} \cdot
\frac{1}{p-2})$. Our result generalizes Spencer's theorem, for which $p = q =
\infty$, and is tight for $m = n$.
  Additionally, we prove that for any fixed constant $\delta>0$, in a centrally
symmetric body $K \subseteq \mathbb{R}^n$ with measure at least $e^{-\delta n}$
one can find such a fractional coloring in polynomial time. Previously this was
known only for a small enough constant -- indeed in this regime classical
nonconstructive arguments do not apply and partial colorings of the form
$\mathbf{x} \in \{ -1,0,1\}^n$ do not necessarily exist.
\\ ( https://arxiv.org/abs/2007.05634 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05656 (*cross-listing*)
Date: Sat, 11 Jul 2020 00:48:35 GMT   (480kb)

Title: Convex Hulls for Graphs of Quadratic Functions With Unit Coefficients:
  Even Wheels and Complete Split Graphs
Authors: Mitchell Harris and Thomas Kalinowski
Categories: math.OC cs.DM math.CO
Comments: 27 pages
MSC-class: 90C57, 90C26, 52B12
\\
  We study the convex hull of the graph of a quadratic function
$f(\mathbf{x})=\sum_{ij\in E}x_ix_j$, where the sum is over the edge set of a
graph $G$ with vertex set $\{1,\dots,n\}$. Using an approach proposed by Gupte
et al. (Discrete Optimization $\textbf{36}$, 2020, 100569), we investigate
minimal extended formulations using additional variables $y_{ij}$, $1\leq
i<j\leq n$, representing the products $x_ix_j$. The basic idea is to identify a
set of facets of the Boolean Quadric Polytope which is sufficient for
characterizing the convex hull for the given graph. Our main results are
extended formulations for the cases that the underlying graph $G$ is either an
even wheel or a complete split graph.
\\ ( https://arxiv.org/abs/2007.05656 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05841 (*cross-listing*)
Date: Sat, 11 Jul 2020 19:46:29 GMT   (32kb)

Title: Tighter Bounds on the Independence Number of the Birkhoff Graph
Authors: Leonardo Nagami Coregliano, Fernando Granha Jeronimo
Categories: math.CO cs.DM
Comments: 34 pages, 4 figures, 1 table
MSC-class: Primary: 05C69. Secondary: 20C30, 90C05, 05C15
\\
  The Birkhoff graph $\mathcal{B}_n$ is the Cayley graph of the symmetric group
$S_n$, where two permutations are adjacent if they differ by a single cycle.
Our main result is a tighter upper bound on the independence number
$\alpha(\mathcal{B}_n)$ of $\mathcal{B}_n$, namely, we show that
$\alpha(\mathcal{B}_n) \le O(n!/1.97^n)$ improving on the previous known bound
of $\alpha(\mathcal{B}_n) \le O(n!/\sqrt{2}^{n})$ by [Kane-Lovett-Rao, FOCS
2017]. Our approach combines a higher-order version of their representation
theoretic techniques with linear programming. With an explicit construction, we
also improve their lower bound on $\alpha(\mathcal{B}_n)$ by a factor of $n/2$.
This construction is based on a proper coloring of $\mathcal{B}_n$, which also
gives an upper bound on the chromatic number $\chi(\mathcal{B}_n)$ of
$\mathcal{B}_n$. Via known connections, the upper bound on
$\alpha(\mathcal{B}_n)$ implies alphabet size lower bounds for a family of
maximally recoverable codes on grid-like topologies.
\\ ( https://arxiv.org/abs/2007.05841 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06066 (*cross-listing*)
Date: Sun, 12 Jul 2020 18:48:15 GMT   (23kb)

Title: The linear arboricity conjecture for 3-degenerate graphs
Authors: Manu Basavaraju and Arijit Bishnu and Mathew Francis and Drimit
  Pattanayak
Categories: math.CO cs.DM
Comments: 15 pages, 1 figure, extended abstract to appear in the proceedings of
  WG 2020
MSC-class: 05C70 (Primary) 05C85 (Secondary)
ACM-class: G.2.2; F.2.2
\\
  A $k$-linear coloring of a graph $G$ is an edge coloring of $G$ with $k$
colors so that each color class forms a linear forest---a forest whose each
connected component is a path. The linear arboricity $\chi_l'(G)$ of $G$ is the
minimum integer $k$ such that there exists a $k$-linear coloring of $G$.
Akiyama, Exoo and Harary conjectured in 1980 that for every graph $G$,
$\chi_l'(G)\leq \left \lceil \frac{\Delta(G)+1}{2}\right\rceil$ where
$\Delta(G)$ is the maximum degree of $G$. We prove the conjecture for
3-degenerate graphs. This establishes the conjecture for graphs of treewidth at
most 3 and provides an alternative proof for the conjecture for triangle-free
planar graphs. Our proof also yields an $O(n)$-time algorithm that partitions
the edge set of any 3-degenerate graph $G$ on $n$ vertices into at most
$\left\lceil\frac{\Delta(G)+1}{2}\right\rceil$ linear forests. Since
$\chi'_l(G)\geq\left\lceil\frac{\Delta(G)}{2}\right\rceil$ for any graph $G$,
the partition produced by the algorithm differs in size from the optimum by at
most an additive factor of 1.
\\ ( https://arxiv.org/abs/2007.06066 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06098 (*cross-listing*)
Date: Sun, 12 Jul 2020 21:12:19 GMT   (69kb)

Title: Graph Connectivity and Single Element Recovery via Linear and OR Queries
Authors: Sepehr Assadi, Deeparnab Chakrabarty, Sanjeev Khanna
Categories: cs.DS cs.DM
\\
  We study the problem of finding a spanning forest in an undirected,
$n$-vertex multi-graph under two basic query models. One is the Linear query
model which are linear measurements on the incidence vector induced by the
edges; the other is the weaker OR query model which only reveals whether a
given subset of plausible edges is empty or not. At the heart of our study lies
a fundamental problem which we call the {\em single element recovery} problem:
given a non-negative real vector $x$ in $N$ dimension, return a single element
$x_j > 0$ from the support. Queries can be made in rounds, and our goals is to
understand the trade-offs between the query complexity and the rounds of
adaptivity needed to solve these problems, for both deterministic and
randomized algorithms. These questions have connections and ramifications to
multiple areas such as sketching, streaming, graph reconstruction, and
compressed sensing. Our main results are:
  * For the single element recovery problem, it is easy to obtain a
deterministic, $r$-round algorithm which makes $(N^{1/r}-1)$-queries per-round.
We prove that this is tight: any $r$-round deterministic algorithm must make
$\geq (N^{1/r} - 1)$ linear queries in some round. In contrast, a $1$-round
$O(\log^2 N)$-query randomized algorithm which succeeds 99% of the time is
known to exist.
  * We design a deterministic $O(r)$-round, $\tilde{O}(n^{1+1/r})$-OR query
algorithm for graph connectivity. We complement this with an
$\tilde{\Omega}(n^{1 + 1/r})$-lower bound for any $r$-round deterministic
algorithm in the OR-model.
  * We design a randomized, $2$-round algorithm for the graph connectivity
problem which makes $\tilde{O}(n)$-OR queries. In contrast, we prove that any
$1$-round algorithm (possibly randomized) requires $\tilde{\Omega}(n^2)$-OR
queries.
\\ ( https://arxiv.org/abs/2007.06098 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05565 (*cross-listing*)
Date: Fri, 10 Jul 2020 18:40:02 GMT   (65kb,D)

Title: Reverse Annealing for Nonnegative/Binary Matrix Factorization
Authors: John Golden, Daniel O'Malley
Categories: cs.LG cs.ET quant-ph stat.ML
Comments: 9 pages, 5 figures
\\
  It was recently shown that quantum annealing can be used as an effective,
fast subroutine in certain types of matrix factorization algorithms. The
quantum annealing algorithm performed best for quick, approximate answers, but
performance rapidly plateaued. In this paper, we utilize reverse annealing
instead of forward annealing in the quantum annealing subroutine for
nonnegative/binary matrix factorization problems. After an initial global
search with forward annealing, reverse annealing performs a series of local
searches that refine existing solutions. The combination of forward and reverse
annealing significantly improves performance compared to forward annealing
alone for all but the shortest run times.
\\ ( https://arxiv.org/abs/2007.05565 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06176 (*cross-listing*)
Date: Mon, 13 Jul 2020 04:02:35 GMT   (148kb,D)

Title: Coarse scale representation of spiking neural networks: backpropagation
  through spikes and application to neuromorphic hardware
Authors: Angel Yanguas-Gil
Categories: cs.NE cs.ET cs.LG
Comments: Paper accepted in ICONS 2020
\\
  In this work we explore recurrent representations of leaky integrate and fire
neurons operating at a timescale equal to their absolute refractory period. Our
coarse time scale approximation is obtained using a probability distribution
function for spike arrivals that is homogeneously distributed over this time
interval. This leads to a discrete representation that exhibits the same
dynamics as the continuous model, enabling efficient large scale simulations
and backpropagation through the recurrent implementation. We use this approach
to explore the training of deep spiking neural networks including
convolutional, all-to-all connectivity, and maxpool layers directly in Pytorch.
We found that the recurrent model leads to high classification accuracy using
just 4-long spike trains during training. We also observed a good transfer back
to continuous implementations of leaky integrate and fire neurons. Finally, we
applied this approach to some of the standard control problems as a first step
to explore reinforcement learning using neuromorphic chips.
\\ ( https://arxiv.org/abs/2007.06176 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06237 (*cross-listing*)
Date: Mon, 13 Jul 2020 08:39:09 GMT   (28740kb,D)

Title: LSQT: Low-Stretch Quasi-Trees for Bundling and Layout
Authors: Rebecca Vandenberg, Madison Elliott, Nicholas Harvey, and Tamara
  Munzner
Categories: cs.HC cs.GR
Comments: 10 pages, 8 figures
\\
  We introduce low-stretch trees to the visualization community with LSQT, our
novel technique that uses quasi-trees for both layout and edge bundling. Our
method offers strong computational speed and complexity guarantees by
leveraging the convenient properties of low-stretch trees, which accurately
reflect the topological structure of arbitrary graphs with superior fidelity
compared to arbitrary spanning trees. Low-stretch quasi-trees also have
provable sparseness guarantees, providing algorithmic support for aggressive
de-cluttering of hairball graphs. LSQT does not rely on previously computed
vertex positions and computes bundles based on topological structure before any
geometric layout occurs. Edge bundles are computed efficiently and stored in an
explicit data structure that supports sophisticated visual encoding and
interaction techniques, including dynamic layout adjustment and interactive
bundle querying. Our unoptimized implementation handles graphs of over 100,000
edges in eight seconds, providing substantially higher performance than
previous approaches.
\\ ( https://arxiv.org/abs/2007.06237 ,  28740kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1908.07342
replaced with revised version Sat, 11 Jul 2020 19:09:23 GMT   (446kb,D)

Title: New efficient flat-back 3D gadgets in origami extrusions compatible with
  the conventional pyramid-supported 3D gadgets
Authors: Mamoru Doi
Categories: cs.CG math.MG
Comments: 42 pages, 24 figures
\\ ( https://arxiv.org/abs/1908.07342 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:1905.11672
replaced with revised version Sun, 12 Jul 2020 22:27:29 GMT   (7858kb,D)

Title: Invertible generative models for inverse problems: mitigating
  representation error and dataset bias
Authors: Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, Paul Hand
Categories: cs.CV
Comments: Camera ready version for ICML 2020, paper 2655
\\ ( https://arxiv.org/abs/1905.11672 ,  7858kb)
------------------------------------------------------------------------------
\\
arXiv:1907.02282
replaced with revised version Sat, 11 Jul 2020 07:21:26 GMT   (7298kb,D)

Title: Edge-Aware Deep Image Deblurring
Authors: Zhichao Fu, Tianlong Ma, Yingbin Zheng, Hao Ye, Jing Yang, and Liang
  He
Categories: cs.CV
\\ ( https://arxiv.org/abs/1907.02282 ,  7298kb)
------------------------------------------------------------------------------
\\
arXiv:1907.06781
replaced with revised version Sat, 11 Jul 2020 17:32:35 GMT   (5865kb,D)

Title: Rethinking RGB-D Salient Object Detection: Models, Data Sets, and
  Large-Scale Benchmarks
Authors: Deng-Ping Fan, Zheng Lin, Jia-Xing Zhao, Yun Liu, Zhao Zhang, Qibin
  Hou, Menglong Zhu, Ming-Ming Cheng
Categories: cs.CV
Comments: Accepted in TNNLS20. 15 pages, 12 figures. Code:
  https://github.com/DengPingFan/D3NetBenchmark
DOI: 10.1109/TNNLS.2020.2996406
\\ ( https://arxiv.org/abs/1907.06781 ,  5865kb)
------------------------------------------------------------------------------
\\
arXiv:1908.11314
replaced with revised version Sat, 11 Jul 2020 08:48:57 GMT   (2107kb,D)

Title: Variational Denoising Network: Toward Blind Noise Modeling and Removal
Authors: Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang and Deyu Meng
Categories: cs.CV
Comments: 11 pages, 4 figures
MSC-class: 68T45
\\ ( https://arxiv.org/abs/1908.11314 ,  2107kb)
------------------------------------------------------------------------------
\\
arXiv:1909.07636
replaced with revised version Mon, 13 Jul 2020 13:07:28 GMT   (1606kb,D)

Title: Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight
  Convolutional Neural Networks
Authors: Gil Shomron, Ron Banner, Moran Shkolnik, Uri Weiser
Categories: cs.CV
\\ ( https://arxiv.org/abs/1909.07636 ,  1606kb)
------------------------------------------------------------------------------
\\
arXiv:1909.10137
replaced with revised version Mon, 13 Jul 2020 14:13:50 GMT   (1085kb)

Title: Validation of image-guided cochlear implant programming techniques
Authors: Yiyuan Zhao, Jianing Wang, Rui Li, Robert F. Labadie, Benoit M.
  Dawant, Jack H. Noble
Categories: cs.CV cs.GL eess.IV q-bio.QM
Comments: 37 pages, 12 figures, 7 tables
\\ ( https://arxiv.org/abs/1909.10137 ,  1085kb)
------------------------------------------------------------------------------
\\
arXiv:1911.08616
replaced with revised version Sat, 11 Jul 2020 02:13:01 GMT   (7965kb,D)

Title: Attention Guided Anomaly Localization in Images
Authors: Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, Abhijit
  Mahalanobis
Categories: cs.CV eess.IV
Comments: Accepted to ECCV 2020
\\ ( https://arxiv.org/abs/1911.08616 ,  7965kb)
------------------------------------------------------------------------------
\\
arXiv:1911.10713
replaced with revised version Mon, 13 Jul 2020 06:59:51 GMT   (301kb,D)

Title: Prototype Rectification for Few-Shot Learning
Authors: Jinlu Liu and Liang Song and Yongqiang Qin
Categories: cs.CV
Comments: ECCV 2020 Oral
\\ ( https://arxiv.org/abs/1911.10713 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:1912.08562
replaced with revised version Sun, 12 Jul 2020 10:20:02 GMT   (3563kb,D)

Title: CPGAN: Full-Spectrum Content-Parsing Generative Adversarial Networks for
  Text-to-Image Synthesis
Authors: Jiadong Liang and Wenjie Pei and Feng Lu
Categories: cs.CV
Comments: 18 pages,8 figures
\\ ( https://arxiv.org/abs/1912.08562 ,  3563kb)
------------------------------------------------------------------------------
\\
arXiv:1912.13471
replaced with revised version Sun, 12 Jul 2020 12:29:00 GMT   (7034kb,D)

Title: OneGAN: Simultaneous Unsupervised Learning of Conditional Image
  Generation, Foreground Segmentation, and Fine-Grained Clustering
Authors: Yaniv Benny and Lior Wolf
Categories: cs.CV
Comments: To be published in the European Conference on Computer Vision (ECCV)
  2020
\\ ( https://arxiv.org/abs/1912.13471 ,  7034kb)
------------------------------------------------------------------------------
\\
arXiv:2001.02314
replaced with revised version Mon, 13 Jul 2020 01:21:09 GMT   (2860kb,D)

Title: Bridging Knowledge Graphs to Generate Scene Graphs
Authors: Alireza Zareian, Svebor Karaman, Shih-Fu Chang
Categories: cs.CV
Comments: To be presented at ECCV 2020
\\ ( https://arxiv.org/abs/2001.02314 ,  2860kb)
------------------------------------------------------------------------------
\\
arXiv:2001.09136
replaced with revised version Mon, 13 Jul 2020 15:55:18 GMT   (522kb,D)

Title: A Branching and Merging Convolutional Network with Homogeneous Filter
  Capsules
Authors: Adam Byerly, Tatiana Kalganova, Ian Dear
Categories: cs.CV cs.LG
Comments: 12 pages, 8 figures, 7 tables
\\ ( https://arxiv.org/abs/2001.09136 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2003.03026
replaced with revised version Mon, 13 Jul 2020 17:31:33 GMT   (7783kb,D)

Title: DA4AD: End-to-End Deep Attention-based Visual Localization for
  Autonomous Driving
Authors: Yao Zhou, Guowei Wan, Shenhua Hou, Li Yu, Gang Wang, Xiaofei Rui,
  Shiyu Song
Categories: cs.CV cs.RO
Comments: 19 pages, 4 figures, Accepted by ECCV 2020
\\ ( https://arxiv.org/abs/2003.03026 ,  7783kb)
------------------------------------------------------------------------------
\\
arXiv:2003.03241
replaced with revised version Mon, 13 Jul 2020 16:06:36 GMT   (1378kb,D)

Title: Automated detection of corrosion in used nuclear fuel dry storage
  canisters using residual neural networks
Authors: Theodore Papamarkou, Hayley Guy, Bryce Kroencke, Jordan Miller,
  Preston Robinette, Daniel Schultz, Jacob Hinkle, Laura Pullum, Catherine
  Schuman, Jeremy Renshaw, Stylianos Chatzidakis
Categories: cs.CV stat.AP stat.ML
\\ ( https://arxiv.org/abs/2003.03241 ,  1378kb)
------------------------------------------------------------------------------
\\
arXiv:2003.03488
replaced with revised version Mon, 13 Jul 2020 03:05:51 GMT   (2725kb,D)

Title: ReActNet: Towards Precise Binary Neural Network with Generalized
  Activation Functions
Authors: Zechun Liu and Zhiqiang Shen and Marios Savvides and Kwang-Ting Cheng
Categories: cs.CV cs.LG eess.IV
Comments: Accepted to ECCV 2020. Code is available at:
  https://github.com/liuzechun/ReActNet
\\ ( https://arxiv.org/abs/2003.03488 ,  2725kb)
------------------------------------------------------------------------------
\\
arXiv:2003.04151
replaced with revised version Mon, 13 Jul 2020 15:14:03 GMT   (1671kb,D)

Title: Embedding Propagation: Smoother Manifold for Few-Shot Classification
Authors: Pau Rodr\'iguez, Issam Laradji, Alexandre Drouin, Alexandre Lacoste
Categories: cs.CV cs.LG
Comments: Published at ECCV2020
\\ ( https://arxiv.org/abs/2003.04151 ,  1671kb)
------------------------------------------------------------------------------
\\
arXiv:2003.05597
replaced with revised version Sun, 12 Jul 2020 12:59:20 GMT   (6587kb,D)

Title: Arbitrary-Oriented Object Detection with Circular Smooth Label
Authors: Xue Yang and Junchi Yan
Categories: cs.CV cs.AI
Comments: 18 pages, 7 figures, 6 tables, accepted by ECCV2020
\\ ( https://arxiv.org/abs/2003.05597 ,  6587kb)
------------------------------------------------------------------------------
\\
arXiv:2003.05653
replaced with revised version Mon, 13 Jul 2020 08:41:09 GMT   (2967kb,D)

Title: Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images
  Using Graph Convolutional Networks
Authors: Jiangke Lin, Yi Yuan, Tianjia Shao, Kun Zhou
Categories: cs.CV
Comments: Accepted to CVPR 2020. The source code is available at
  https://github.com/FuxiCV/3D-Face-GCNs
\\ ( https://arxiv.org/abs/2003.05653 ,  2967kb)
------------------------------------------------------------------------------
\\
arXiv:2003.11142
replaced with revised version Mon, 13 Jul 2020 03:48:01 GMT   (826kb,D)

Title: BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage
  Models
Authors: Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan
  Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, Quoc Le
Categories: cs.CV
Comments: Accepted in ECCV 2020
\\ ( https://arxiv.org/abs/2003.11142 ,  826kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13549
replaced with revised version Mon, 13 Jul 2020 14:57:16 GMT   (524kb,D)

Title: Rethinking Depthwise Separable Convolutions: How Intra-Kernel
  Correlations Lead to Improved MobileNets
Authors: Daniel Haase and Manuel Amthor
Categories: cs.CV
Comments: Published at CVPR 2020. Code and models are available under
  https://github.com/zeiss-microscopy/BSConv
Journal-ref: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 14600-14609
\\ ( https://arxiv.org/abs/2003.13549 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13948
replaced with revised version Sun, 12 Jul 2020 06:19:36 GMT   (7255kb,D)

Title: Segmenting Transparent Objects in the Wild
Authors: Enze Xie, Wenjia Wang, Wenhai Wang, Mingyu Ding, Chunhua Shen, Ping
  Luo
Categories: cs.CV
Comments: ECCV2020 Accept
\\ ( https://arxiv.org/abs/2003.13948 ,  7255kb)
------------------------------------------------------------------------------
\\
arXiv:2004.08190
replaced with revised version Fri, 10 Jul 2020 19:32:01 GMT   (7718kb,D)

Title: Structured Landmark Detection via Topology-Adapting Deep Graph Learning
Authors: Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo,
  Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, and Shun Miao
Categories: cs.CV
Comments: Accepted to ECCV-20. Camera-ready with supplementary material
\\ ( https://arxiv.org/abs/2004.08190 ,  7718kb)
------------------------------------------------------------------------------
\\
arXiv:2004.10129
replaced with revised version Sun, 12 Jul 2020 12:50:14 GMT   (819kb,D)

Title: Have you forgotten? A method to assess if machine learning models have
  forgotten data
Authors: Xiao Liu, Sotirios A Tsaftaris
Categories: cs.CV
Comments: Accepted by MICCAI 2020
\\ ( https://arxiv.org/abs/2004.10129 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2004.12535
replaced with revised version Fri, 10 Jul 2020 23:37:20 GMT   (8873kb,D)

Title: Difficulty Translation in Histopathology Images
Authors: Jerry Wei and Arief Suriawinata and Xiaoying Liu and Bing Ren and
  Mustafa Nasir-Moin and Naofumi Tomita and Jason Wei and Saeed Hassanpour
Categories: cs.CV
Comments: Accepted to 2020 Artificial Intelligence in Medicine (AIME)
  conference. Invited for long oral presentation
\\ ( https://arxiv.org/abs/2004.12535 ,  8873kb)
------------------------------------------------------------------------------
\\
arXiv:2004.13073
replaced with revised version Mon, 13 Jul 2020 12:22:38 GMT   (1385kb,D)

Title: A Novel Attention-based Aggregation Function to Combine Vision and
  Language
Authors: Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
Categories: cs.CV cs.CL cs.LG
Comments: ICPR 2020
\\ ( https://arxiv.org/abs/2004.13073 ,  1385kb)
------------------------------------------------------------------------------
\\
arXiv:2005.09120
replaced with revised version Sat, 11 Jul 2020 20:23:19 GMT   (1167kb,D)

Title: Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation
Authors: Shuhao Fu, Yongyi Lu, Yan Wang, Yuyin Zhou, Wei Shen, Elliot Fishman,
  Alan Yuille
Categories: cs.CV
Comments: Accepted at MICCAL 2020
\\ ( https://arxiv.org/abs/2005.09120 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2006.03184
replaced with revised version Mon, 13 Jul 2020 07:27:02 GMT   (9405kb,D)

Title: Pick-Object-Attack: Type-Specific Adversarial Attack for Object
  Detection
Authors: Omid Mohamad Nezami, Akshay Chaturvedi, Mark Dras, Utpal Garain
Categories: cs.CV cs.LG
Comments: The paper is under consideration at Computer Vision and Image
  Understanding
\\ ( https://arxiv.org/abs/2006.03184 ,  9405kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04255
replaced with revised version Sat, 11 Jul 2020 07:07:28 GMT   (8219kb,D)

Title: How useful is Active Learning for Image-based Plant Phenotyping?
Authors: Koushik Nagasubramanian, Talukder Z. Jubery, Fateme Fotouhi Ardakani,
  Seyed Vahid Mirnezami, Asheesh K. Singh, Arti Singh, Soumik Sarkar, and
  Baskar Ganapathysubramanian
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.04255 ,  8219kb)
------------------------------------------------------------------------------
\\
arXiv:2006.07114
replaced with revised version Mon, 13 Jul 2020 09:14:27 GMT   (1456kb,D)

Title: Knowledge Distillation Meets Self-Supervision
Authors: Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy
Categories: cs.CV
Comments: To appear in ECCV 2020. Code is available at:
  https://github.com/xuguodong03/SSKD
\\ ( https://arxiv.org/abs/2006.07114 ,  1456kb)
------------------------------------------------------------------------------
\\
arXiv:2006.07828
replaced with revised version Mon, 13 Jul 2020 15:02:59 GMT   (4952kb,D)

Title: On Saliency Maps and Adversarial Robustness
Authors: Puneet Mangla, Vedant Singh, Vineeth N Balasubramanian
Categories: cs.CV cs.LG
Comments: Accepted at ECML-PKDD 2020, Acknowledgements added
\\ ( https://arxiv.org/abs/2006.07828 ,  4952kb)
------------------------------------------------------------------------------
\\
arXiv:2006.12906
replaced with revised version Sun, 12 Jul 2020 23:33:28 GMT   (4752kb,D)

Title: Probabilistic Crowd GAN: Multimodal Pedestrian Trajectory Prediction
  using a Graph Vehicle-Pedestrian Attention Network
Authors: Stuart Eiffert, Kunming Li, Mao Shan, Stewart Worrall, Salah Sukkarieh
  and Eduardo Nebot
Categories: cs.CV cs.RO
Comments: Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L) Copyright may be transferred without notice, after which this version
  may no longer be accessible
DOI: 10.1109/LRA.2020.3004324
\\ ( https://arxiv.org/abs/2006.12906 ,  4752kb)
------------------------------------------------------------------------------
\\
arXiv:2006.13806
replaced with revised version Sat, 11 Jul 2020 18:26:47 GMT   (5019kb,D)

Title: X-ModalNet: A Semi-Supervised Deep Cross-Modal Network for
  Classification of Remote Sensing Data
Authors: Danfeng Hong, Naoto Yokoya, Gui-Song Xia, Jocelyn Chanussot, Xiao
  Xiang Zhu
Categories: cs.CV
Journal-ref: ISPRS Journal of Photogrammetry and Remote Sensing,2020,167:12-23
DOI: 10.1016/j.isprsjprs.2020.06.014
\\ ( https://arxiv.org/abs/2006.13806 ,  5019kb)
------------------------------------------------------------------------------
\\
arXiv:2006.14090
replaced with revised version Sun, 12 Jul 2020 05:11:21 GMT   (1752kb,D)

Title: Neural Architecture Design for GPU-Efficient Networks
Authors: Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, Rong Jin
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.14090 ,  1752kb)
------------------------------------------------------------------------------
\\
arXiv:2006.15327
replaced with revised version Mon, 13 Jul 2020 11:31:18 GMT   (9491kb,D)

Title: Compositional Video Synthesis with Action Graphs
Authors: Amir Bar, Roei Herzig, Xiaolong Wang, Gal Chechik, Trevor Darrell,
  Amir Globerson
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.15327 ,  9491kb)
------------------------------------------------------------------------------
\\
arXiv:2006.16067
replaced with revised version Mon, 13 Jul 2020 10:26:14 GMT   (8516kb,D)

Title: Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
Authors: Jihun Yi and Sungroh Yoon
Categories: cs.CV
\\ ( https://arxiv.org/abs/2006.16067 ,  8516kb)
------------------------------------------------------------------------------
\\
arXiv:2006.16471
replaced with revised version Fri, 10 Jul 2020 19:51:52 GMT   (9247kb,D)

Title: Object Detection under Rainy Conditions for Autonomous Vehicles
Authors: Mazin Hnewa and Hayder Radha
Categories: cs.CV
Comments: Accepted in IEEE Signal Processing Magazine / Special Issue on
  Autonomous Driving
\\ ( https://arxiv.org/abs/2006.16471 ,  9247kb)
------------------------------------------------------------------------------
\\
arXiv:2007.00328
replaced with revised version Sat, 11 Jul 2020 06:31:34 GMT   (3887kb,D)

Title: NestFuse: An Infrared and Visible Image Fusion Architecture based on
  Nest Connection and Spatial/Channel Attention Models
Authors: Hui Li, Xiao-Jun Wu, Tariq Durrani
Categories: cs.CV
Comments: 12 pages, 13 figures, 6 tables. IEEE Transactions on Instrumentation
  and Measurement
DOI: 10.1109/TIM.2020.3005230
\\ ( https://arxiv.org/abs/2007.00328 ,  3887kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01464
replaced with revised version Sun, 12 Jul 2020 04:04:28 GMT   (7385kb,D)

Title: Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for
  Accurate Pelvic Fracture Detection in X-ray Images
Authors: Haomin Chen, Yirui Wang, Kang Zheng, Weijian Li, Chi-Tung Cheng, Adam
  P. Harrison, Jing Xiao, Gregory D. Hager, Le Lu, Chien-Hung Liao, Shun Miao
Categories: cs.CV
Comments: ECCV 2020 (camera-ready)
\\ ( https://arxiv.org/abs/2007.01464 ,  7385kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01524
replaced with revised version Sat, 11 Jul 2020 02:00:51 GMT   (2800kb,D)

Title: Domain Adaptation without Source Data
Authors: Youngeun Kim, Sungeun Hong, Donghyeon Cho, Hyoungseob Park,
  Priyadarshini Panda
Categories: cs.CV cs.LG eess.IV
Comments: 13 pages
\\ ( https://arxiv.org/abs/2007.01524 ,  2800kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01546
replaced with revised version Mon, 13 Jul 2020 13:11:44 GMT   (1288kb,D)

Title: Multiple Expert Brainstorming for Domain Adaptive Person
  Re-identification
Authors: Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji and
  Yonghong Tian
Categories: cs.CV
Comments: Accepted by ECCV'20
\\ ( https://arxiv.org/abs/2007.01546 ,  1288kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01992
replaced with revised version Sun, 12 Jul 2020 04:51:07 GMT   (1029kb)

Title: A Survey on Sensor Technologies for Unmanned Ground Vehicles
Authors: Qi Liu, Shihua Yuan, Zirui Li
Categories: cs.CV cs.RO
Comments: 9 pages,6 tables
\\ ( https://arxiv.org/abs/2007.01992 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02343
replaced with revised version Mon, 13 Jul 2020 13:46:10 GMT   (1515kb,D)

Title: Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks
Authors: Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu
Categories: cs.CV
Comments: Accepted by ECCV-2020
\\ ( https://arxiv.org/abs/2007.02343 ,  1515kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02846
replaced with revised version Mon, 13 Jul 2020 14:11:49 GMT   (774kb,D)

Title: Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation
Authors: Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, Stephen Lin
Categories: cs.CV
Comments: To appear in ECCV 2020
\\ ( https://arxiv.org/abs/2007.02846 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03227
replaced with revised version Mon, 13 Jul 2020 13:40:41 GMT   (1062kb,D)

Title: Extracting the fundamental diagram from aerial footage
Authors: Rafael Makrigiorgis, Panayiotis Kolios, Stelios Timotheou, Theocharis
  Theocharides, Christos G. Panayiotou
Categories: cs.CV eess.IV
Comments: 5 pages, 7 figures, 2020 IEEE 91st Vehicular Technology Conference
  (VTC2020-Spring)
DOI: 10.1109/VTC2020-Spring48590.2020.9128534
\\ ( https://arxiv.org/abs/2007.03227 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03380
replaced with revised version Sat, 11 Jul 2020 02:08:22 GMT   (6679kb,D)

Title: Re-thinking Co-Salient Object Detection
Authors: Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang,
  Ming-Ming Cheng, Huazhu Fu, Jianbing Shen
Categories: cs.CV
Comments: 22pages, 18 figures. CVPR2020-CoSOD3K extension. Code:
  https://dpfan.net/CoSOD3K
\\ ( https://arxiv.org/abs/2007.03380 ,  6679kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03777
replaced with revised version Sun, 12 Jul 2020 16:38:50 GMT   (4098kb,D)

Title: Placepedia: Comprehensive Place Understanding with Multi-Faceted
  Annotations
Authors: Huaiyi Huang, Yuqi Zhang, Qingqiu Huang, Zhengkui Guo, Ziwei Liu, and
  Dahua Lin
Categories: cs.CV cs.CL cs.IR
Comments: To appear in ECCV 2020. Dataset is available at:
  https://hahehi.github.io/placepedia.html
\\ ( https://arxiv.org/abs/2007.03777 ,  4098kb)
------------------------------------------------------------------------------
\\
arXiv:2007.04687
replaced with revised version Mon, 13 Jul 2020 04:16:22 GMT   (7899kb,D)

Title: Not only Look, but also Listen: Learning Multimodal Violence Detection
  under Weak Supervision
Authors: Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu,
  Zhiwei Yang
Categories: cs.CV
Comments: To appear in ECCV 2020
\\ ( https://arxiv.org/abs/2007.04687 ,  7899kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05299
replaced with revised version Mon, 13 Jul 2020 10:51:04 GMT   (159kb,D)

Title: Data-Efficient Ranking Distillation for Image Retrieval
Authors: Zakaria Laskar, Juho Kannala
Categories: cs.CV
Comments: 10 pages, 2 figures. Edited figure 7
\\ ( https://arxiv.org/abs/2007.05299 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2002.07153
replaced with revised version Sun, 12 Jul 2020 22:05:34 GMT   (2584kb,D)

Title: Cover Combinatorial Filters and their Minimization Problem (Extended
  Version)
Authors: Yulin Zhang and Dylan A. Shell
Categories: cs.DM cs.RO
Comments: 20 pages, 9 figures, WAFR 2020
\\ ( https://arxiv.org/abs/2002.07153 ,  2584kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01830
replaced with revised version Mon, 13 Jul 2020 12:49:01 GMT   (31kb,D)

Title: Fractional Covers of Hypergraphs with Bounded Multi-Intersection
Authors: Georg Gottlob, Matthias Lanzinger, Reinhard Pichler, Igor Razgon
Categories: cs.DM
Comments: This is the full version of the paper with the same title at MFCS
  2020
ACM-class: G.2.1
\\ ( https://arxiv.org/abs/2007.01830 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2007.00815
replaced with revised version Fri, 10 Jul 2020 20:13:17 GMT   (376kb)

Title: Threshold Logic with Current-Driven Magnetic Domain Walls
Authors: Xuan Hu, Brighton A. Hill, Felipe Garcia-Sanchez, and Joseph S.
  Friedman
Categories: cs.ET cond-mat.mes-hall
\\ ( https://arxiv.org/abs/2007.00815 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:1910.08954
replaced with revised version Mon, 13 Jul 2020 02:08:50 GMT   (9355kb,D)

Title: Top-Down Shape Abstraction Based on Greedy Pole Selection
Authors: Zhiyang Dou, Shiqing Xin, Rui Xu, Jian Xu, Yuanfeng Zhou, Shuangmin
  Chen, Wenping Wang, Xiuyang Zhao, Changhe Tu
Categories: cs.GR
Comments: 13 pages, 14 figures
\\ ( https://arxiv.org/abs/1910.08954 ,  9355kb)
------------------------------------------------------------------------------
\\
arXiv:1910.07898 (*cross-listing*)
replaced with revised version Sun, 12 Jul 2020 16:11:19 GMT   (6317kb,D)

Title: An improved particle packing algorithm for complex geometries
Authors: Pawan Negi and Prabhu Ramachandran
Categories: physics.comp-ph cs.CG
\\ ( https://arxiv.org/abs/1910.07898 ,  6317kb)
------------------------------------------------------------------------------
\\
arXiv:2003.04523 (*cross-listing*)
replaced with revised version Sun, 12 Jul 2020 21:01:55 GMT   (461kb,D)

Title: Elder-Rule-Staircodes for Augmented Metric Spaces
Authors: Chen Cai, Woojin Kim, Facundo Memoli, Yusu Wang
Categories: math.AT cs.CG
Comments: A few important questions considered in the previous version have
  been settled; see Example 4.12 and Section 4.3 in particular. The paper has
  been reorganized. This is the full version of the paper in the Proceedings of
  the 36th International Symposium on Computational Geometry (SoCG 2020); 41
  pages, 17 figures
\\ ( https://arxiv.org/abs/2003.04523 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:1811.00401
replaced with revised version Sun, 12 Jul 2020 07:26:06 GMT   (4548kb,D)

Title: Excessive Invariance Causes Adversarial Vulnerability
Authors: J\"orn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, Matthias Bethge
Categories: cs.LG cs.AI cs.CV stat.ML
Journal-ref: Proceedings of the 7th International Conference on Learning
  Representations (ICLR), 2019
\\ ( https://arxiv.org/abs/1811.00401 ,  4548kb)
------------------------------------------------------------------------------
\\
arXiv:1902.09135
replaced with revised version Sat, 11 Jul 2020 11:33:28 GMT   (1294kb)

Title: A Dual Symmetric Gauss-Seidel Alternating Direction Method of
  Multipliers for Hyperspectral Sparse Unmixing
Authors: Longfei Ren, Chengjing Wang, Peipei Tang, and Zheng Ma
Categories: cs.NA cs.CV eess.IV math.NA
Comments: 30 pages, 6 figures
\\ ( https://arxiv.org/abs/1902.09135 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:1910.02509
replaced with revised version Mon, 13 Jul 2020 17:10:44 GMT   (2356kb,D)

Title: REMIND Your Neural Network to Prevent Catastrophic Forgetting
Authors: Tyler L. Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya,
  Christopher Kanan
Categories: cs.LG cs.CV cs.NE
Comments: To appear in the European Conference on Computer Vision (ECCV-2020)
\\ ( https://arxiv.org/abs/1910.02509 ,  2356kb)
------------------------------------------------------------------------------
\\
arXiv:1911.11028 (*cross-listing*)
replaced with revised version Sun, 12 Jul 2020 01:36:23 GMT   (5004kb,D)

Title: Deep Decomposition Learning for Inverse Imaging Problems
Authors: Dongdong Chen, Mike E. Davies
Categories: eess.IV cs.CV
Comments: To appear in ECCV 2020
\\ ( https://arxiv.org/abs/1911.11028 ,  5004kb)
------------------------------------------------------------------------------
\\
arXiv:1911.11237
replaced with revised version Sun, 12 Jul 2020 21:19:49 GMT   (9029kb,D)

Title: Learning to Learn Words from Visual Scenes
Authors: D\'idac Sur\'is, Dave Epstein, Heng Ji, Shih-Fu Chang, Carl Vondrick
Categories: cs.CL cs.CV cs.LG
Comments: 26 pages, 12 figures
Journal-ref: European Conference on Computer Vision (ECCV), 2020
\\ ( https://arxiv.org/abs/1911.11237 ,  9029kb)
------------------------------------------------------------------------------
\\
arXiv:1912.02279
replaced with revised version Fri, 10 Jul 2020 20:58:01 GMT   (5455kb,D)

Title: Angular Visual Hardness
Authors: Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava,
  Animesh Garg, Anima Anandkumar
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/1912.02279 ,  5455kb)
------------------------------------------------------------------------------
\\
arXiv:1912.08521
replaced with revised version Sun, 12 Jul 2020 13:16:24 GMT   (3124kb,D)

Title: Semantically Plausible and Diverse 3D Human Motion Prediction
Authors: Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars
  Petersson, Stephen Gould
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/1912.08521 ,  3124kb)
------------------------------------------------------------------------------
\\
arXiv:2002.03328
replaced with revised version Sun, 12 Jul 2020 11:56:54 GMT   (8791kb,D)

Title: Out-of-Distribution Detection with Distance Guarantee in Deep Generative
  Models
Authors: Yufeng Zhang, Wanwei Liu, Zhenbang Chen, Ji Wang, Zhiming Liu, Kenli
  Li, Hongmei Wei
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/2002.03328 ,  8791kb)
------------------------------------------------------------------------------
\\
arXiv:2002.07772
replaced with revised version Sat, 11 Jul 2020 00:40:16 GMT   (1553kb,D)

Title: The Tree Ensemble Layer: Differentiability meets Conditional Computation
Authors: Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul
  Mazumder
Categories: cs.LG cs.CV stat.ML
Comments: ICML 2020
\\ ( https://arxiv.org/abs/2002.07772 ,  1553kb)
------------------------------------------------------------------------------
\\
arXiv:2003.10849 (*cross-listing*)
replaced with revised version Sun, 12 Jul 2020 11:35:35 GMT   (458kb)

Title: Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images
  and Deep Convolutional Neural Networks
Authors: Ali Narin, Ceren Kaya, Ziynet Pamuk
Categories: eess.IV cs.CV cs.LG
Comments: The manuscript has 17 pages, 7 figures and 1 table
\\ ( https://arxiv.org/abs/2003.10849 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13659 (*cross-listing*)
replaced with revised version Mon, 13 Jul 2020 06:41:17 GMT   (6093kb,D)

Title: Exploiting Deep Generative Prior for Versatile Image Restoration and
  Manipulation
Authors: Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping
  Luo
Categories: eess.IV cs.CV
Comments: Accepted to ECCV2020 as oral. 1) Precise GAN-inversion by
  discriminator-guided generator finetuning. 2) A versatile way for
  high-quality image restoration and manipulation. Code:
  https://github.com/XingangPan/deep-generative-prior
\\ ( https://arxiv.org/abs/2003.13659 ,  6093kb)
------------------------------------------------------------------------------
\\
arXiv:2004.00218 (*cross-listing*)
replaced with revised version Sat, 11 Jul 2020 04:28:29 GMT   (1133kb)

Title: 3D Deep Learning on Medical Images: A Review
Authors: Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman
  Padmanabhan and Bal\'azs Guly\'as
Categories: q-bio.QM cs.CV cs.LG eess.IV
Comments: 13 pages, 4 figures, 2 tables
\\ ( https://arxiv.org/abs/2004.00218 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2005.02706 (*cross-listing*)
replaced with revised version Fri, 10 Jul 2020 20:42:58 GMT   (2243kb,D)

Title: Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet)
Authors: Chen-Han Tsai, Nahum Kiryati, Eli Konen, Iris Eshed, Arnaldo Mayer
Categories: eess.IV cs.CV
Comments: 11 pages, 4 figures, Accepted to the Medical Imaging and Deep
  Learning (MIDL) Conference 2020
\\ ( https://arxiv.org/abs/2005.02706 ,  2243kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01491
replaced with revised version Sun, 12 Jul 2020 16:43:57 GMT   (8505kb,D)

Title: Self-Supervised GAN Compression
Authors: Chong Yu, Jeff Pool
Categories: cs.LG cs.CV eess.IV
Comments: The appendix for this paper is in the following repository
  https://gitlab.com/dxxz/Self-Supervised-GAN-Compression-Appendix
\\ ( https://arxiv.org/abs/2007.01491 ,  8505kb)
------------------------------------------------------------------------------
\\
arXiv:2007.01867
replaced with revised version Fri, 10 Jul 2020 23:15:52 GMT   (3827kb,D)

Title: TLIO: Tight Learned Inertial Odometry
Authors: Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I. Mourikis,
  Kostas Daniilidis, Vijay Kumar, Jakob Engel
Categories: cs.RO cs.CV cs.LG eess.SP
Comments: Correcting graph and bibliography. Adding journal reference
  information and DOI, in IEEE Robotics and Automation Letters
DOI: 10.1109/LRA.2020.3007421
\\ ( https://arxiv.org/abs/2007.01867 ,  3827kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02096 (*cross-listing*)
replaced with revised version Sat, 11 Jul 2020 13:24:15 GMT   (5639kb)

Title: Multi-Site Infant Brain Segmentation Algorithms: The iSeg-2019 Challenge
Authors: Yue Sun, Kun Gao, Zhengwang Wu, Zhihao Lei, Ying Wei, Jun Ma, Xiaoping
  Yang, Xue Feng, Li Zhao, Trung Le Phan, Jitae Shin, Tao Zhong, Yu Zhang,
  Lequan Yu, Caizi Li, Ramesh Basnet, M. Omair Ahmad, M.N.S. Swamy, Wenao Ma,
  Qi Dou, Toan Duc Bui, Camilo Bermudez Noguera, Bennett Landman (Senior
  Member, IEEE), Ian H. Gotlib, Kathryn L. Humphreys, Sarah Shultz, Longchuan
  Li, Sijie Niu, Weili Lin, Valerie Jewells, Gang Li (Senior Member, IEEE),
  Dinggang Shen (Fellow, IEEE), Li Wang (Senior Member, IEEE)
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2007.02096 ,  5639kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02250
replaced with revised version Mon, 13 Jul 2020 09:07:02 GMT   (1490kb,D)

Title: Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback
  Loops
Authors: Shengyang Chen, Chih-Yung Wen, Yajing Zou and Wu Chen
Categories: cs.RO cs.CV
Comments: 14 pages, 14 figures, 2 tables
\\ ( https://arxiv.org/abs/2007.02250 ,  1490kb)
------------------------------------------------------------------------------
\\
arXiv:2007.02606 (*cross-listing*)
replaced with revised version Mon, 13 Jul 2020 13:10:27 GMT   (8034kb,D)

Title: A Convolutional Approach to Vertebrae Detection and Labelling in Whole
  Spine MRI
Authors: Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman
Categories: eess.IV cs.CV
Comments: Accepted full paper to Medical Image Computing and Computer Assisted
  Intervention 2020. 11 pages plus appendix
\\ ( https://arxiv.org/abs/2007.02606 ,  8034kb)
------------------------------------------------------------------------------
\\
arXiv:2007.03817 (*cross-listing*)
replaced with revised version Sat, 11 Jul 2020 00:33:14 GMT   (1232kb,D)

Title: Self-supervised Skull Reconstruction in Brain CT Images with
  Decompressive Craniectomy
Authors: Franco Matzkin, Virginia Newcombe, Susan Stevenson, Aneesh Khetani,
  Tom Newman, Richard Digby, Andrew Stevens, Ben Glocker, Enzo Ferrante
Categories: eess.IV cs.CV
Comments: Accepted for publication in MICCAI 2020. Update: Figure 1 corrected
  to match description
\\ ( https://arxiv.org/abs/2007.03817 ,  1232kb)
------------------------------------------------------------------------------
\\
arXiv:2007.04344 (*cross-listing*)
replaced with revised version Sat, 11 Jul 2020 03:03:38 GMT   (566kb,D)

Title: Lightweight Image Super-Resolution with Enhanced CNN
Authors: Chunwei Tian, Ruibin Zhuge, Zhihao Wu, Yong Xu, Wangmeng Zuo, Chen
  Chen, Chia-Wen Lin
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2007.04344 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05454 (*cross-listing*)
replaced with revised version Mon, 13 Jul 2020 16:30:44 GMT   (8300kb,D)

Title: SIMBA: Specific Identity Markers for Bone Age Assessment
Authors: Cristina Gonz\'alez and Mar\'ia Escobar and Laura Daza and Felipe
  Torres and Gustavo Triana and Pablo Arbel\'aez
Categories: eess.IV cs.CV
Comments: Accepted at MICCAI 2020
\\ ( https://arxiv.org/abs/2007.05454 ,  8300kb)
------------------------------------------------------------------------------
\\
arXiv:1903.06898
replaced with revised version Sun, 12 Jul 2020 12:59:00 GMT   (16kb)

Title: On-Line Balancing of Random Inputs
Authors: Nikhil Bansal, Joel H. Spencer
Categories: cs.DS cs.DM math.PR
Comments: 13 pages
\\ ( https://arxiv.org/abs/1903.06898 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:1907.06688
replaced with revised version Sat, 11 Jul 2020 15:45:35 GMT   (43kb)

Title: Matrices of optimal tree-depth and a row-invariant parameterized
  algorithm for integer programming
Authors: Timothy F. N. Chan, Jacob W. Cooper, Martin Koutecky, Daniel Kral,
  Kristyna Pekarkova
Categories: cs.DS cs.DM math.OC
Comments: Full version. 45 pages, 4 figures
\\ ( https://arxiv.org/abs/1907.06688 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2003.05755
replaced with revised version Mon, 13 Jul 2020 16:01:18 GMT   (28kb)

Title: Optimization of Generalized Jacobian Chain Products without Memory
  Constraints
Authors: Uwe Naumann
Categories: math.NA cs.DM cs.NA
Comments: 13 pages
\\ ( https://arxiv.org/abs/2003.05755 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2003.06250 (*cross-listing*)
replaced with revised version Sun, 12 Jul 2020 15:17:00 GMT   (357kb)

Title: Harary polynomials
Authors: Orli Herscovici, Johann A. Makowsky, Vsevolod Rakita
Categories: math.CO cs.DM
Comments: 17 pages
MSC-class: 05C30, 05C31, 05C15
\\ ( https://arxiv.org/abs/2003.06250 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2003.13128
replaced with revised version Mon, 13 Jul 2020 16:19:23 GMT   (8035kb,D)

Title: Separable games
Authors: Laura Arditti and Giacomo Como and Fabio Fagnani
Categories: cs.GT cs.DM cs.MA cs.SI
Comments: 21 pages, 6 figures
MSC-class: 91A43, 91A70, 91A06, 91A10, 91A40
ACM-class: G.2
\\ ( https://arxiv.org/abs/2003.13128 ,  8035kb)
------------------------------------------------------------------------------
\\
arXiv:2006.10894 (*cross-listing*)
replaced with revised version Fri, 10 Jul 2020 19:58:19 GMT   (1189kb,D)

Title: The damage throttling number of a graph
Authors: Joshua Carlson, Robin Eagleton, Jesse Geneson, John Petrucci, Carolyn
  Reinhart, Preetul Sen
Categories: math.CO cs.DM
Comments: 23 pages, 8 figures
MSC-class: 05C57, 05C15, 05C50
\\ ( https://arxiv.org/abs/2006.10894 ,  1189kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
